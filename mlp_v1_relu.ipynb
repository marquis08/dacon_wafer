{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426",
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.optim import Adam, SGD, Optimizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Semi_dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.X_dataset = []\n",
    "        self.Y_dataset = []\n",
    "        for x in X:\n",
    "            self.X_dataset.append(torch.FloatTensor(x))\n",
    "        try:\n",
    "            for y in Y.values:\n",
    "                self.Y_dataset.append(torch.tensor(y))\n",
    "        except:\n",
    "            print(\"no label\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X_dataset[index]\n",
    "        try:\n",
    "            target = self.Y_dataset[index]\n",
    "            return data, target\n",
    "        except:\n",
    "            return data\n",
    "\n",
    "\n",
    "def build_dataloader(X, Y, batch_size, shuffle=False):\n",
    "    \n",
    "    dataset = Semi_dataset(X, Y)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=8\n",
    "                            )\n",
    "    return dataloader\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred,\n",
    "                        sample_weight=None,\n",
    "                        multioutput='uniform_average'):\n",
    "    \n",
    "    output_errors = np.average(np.abs(y_pred - y_true),\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class MLP_only_flatfeatures(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(MLP_only_flatfeatures, self).__init__()\n",
    "        self.num_classes = num_classes         \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            #nn.Linear(226, 1000),\n",
    "            nn.Linear(226, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            ####### Block 1 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            ####### Block 2 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            ####### Block 3 #######\n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            ####### Block 4 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            #######################\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SELU(),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, self.num_classes)\n",
    "            )             \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_layers(x)\n",
    "        return out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def build_model(device, model_name='mlp', weight_path=None):\n",
    "\n",
    "    if model_name == 'mlp':\n",
    "        model = MLP_only_flatfeatures(4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def validation(model, criterion, valid_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), 4))\n",
    "    valid_targets = np.zeros((len(valid_loader.dataset), 4))\n",
    "    val_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(valid_loader):\n",
    "            \n",
    "            valid_targets[i * batch_size: (i+1) * batch_size] = target.float().numpy().copy()\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "                \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_preds[i * batch_size: (i+1) * batch_size] = output.detach().cpu().numpy()\n",
    "            \n",
    "            val_loss += loss.item() / len(valid_loader)\n",
    "    \n",
    "    val_score = mean_absolute_error(valid_preds, valid_targets)\n",
    "\n",
    "    return val_loss, val_score\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "if cuda.is_available:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../wafer'\n",
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(DATASET_PATH, 'sample_submission.csv'))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df.iloc[:, 4:], train_df.iloc[:, :4], test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "batch_size = 2048\n",
    "train_loader = build_dataloader(X_train, y_train, batch_size, shuffle=True)\n",
    "valid_loader = build_dataloader(X_val, y_val, batch_size, shuffle=False)\n",
    "\n",
    "test_df.iloc[:, 1:] = scaler.transform(test_df.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "  elapsed: 660m 20s\nEpoch 6130 / 9999  train Loss: 1.8920  val_loss: 0.5875  val_score: 0.5873  lr: 0.00100  elapsed: 660m 27s\nEpoch 6131 / 9999  train Loss: 1.8942  val_loss: 0.5176  val_score: 0.5174  lr: 0.00100  elapsed: 660m 33s\nEpoch 6132 / 9999  train Loss: 1.8879  val_loss: 0.5439  val_score: 0.5437  lr: 0.00100  elapsed: 660m 40s\nEpoch 6133 / 9999  train Loss: 1.8900  val_loss: 0.5424  val_score: 0.5424  lr: 0.00100  elapsed: 660m 46s\nEpoch 6134 / 9999  train Loss: 1.8949  val_loss: 0.5672  val_score: 0.5670  lr: 0.00100  elapsed: 660m 53s\nEpoch 6135 / 9999  train Loss: 1.8896  val_loss: 0.5860  val_score: 0.5857  lr: 0.00100  elapsed: 660m 59s\nEpoch 6136 / 9999  train Loss: 1.8907  val_loss: 0.5816  val_score: 0.5813  lr: 0.00100  elapsed: 661m 6s\nEpoch 6137 / 9999  train Loss: 1.8920  val_loss: 0.5922  val_score: 0.5920  lr: 0.00100  elapsed: 661m 12s\nEpoch 6138 / 9999  train Loss: 1.8895  val_loss: 0.5439  val_score: 0.5437  lr: 0.00100  elapsed: 661m 19s\nEpoch 6139 / 9999  train Loss: 1.8941  val_loss: 0.5873  val_score: 0.5869  lr: 0.00100  elapsed: 661m 25s\nEpoch 6140 / 9999  train Loss: 1.8911  val_loss: 0.5791  val_score: 0.5787  lr: 0.00100  elapsed: 661m 32s\nEpoch 6141 / 9999  train Loss: 1.8978  val_loss: 0.6258  val_score: 0.6249  lr: 0.00100  elapsed: 661m 38s\nEpoch 6142 / 9999  train Loss: 1.8975  val_loss: 0.5275  val_score: 0.5273  lr: 0.00100  elapsed: 661m 45s\nEpoch 6143 / 9999  train Loss: 1.8910  val_loss: 0.5947  val_score: 0.5944  lr: 0.00100  elapsed: 661m 51s\nEpoch 6144 / 9999  train Loss: 1.8899  val_loss: 0.5619  val_score: 0.5616  lr: 0.00100  elapsed: 661m 58s\nEpoch 6145 / 9999  train Loss: 1.8908  val_loss: 0.5143  val_score: 0.5142  lr: 0.00100  elapsed: 662m 4s\nEpoch 6146 / 9999  train Loss: 1.8925  val_loss: 0.5628  val_score: 0.5625  lr: 0.00100  elapsed: 662m 11s\nEpoch 6147 / 9999  train Loss: 1.8918  val_loss: 0.5000  val_score: 0.4998  lr: 0.00100  elapsed: 662m 17s\nEpoch 6148 / 9999  train Loss: 1.8917  val_loss: 0.5048  val_score: 0.5045  lr: 0.00100  elapsed: 662m 24s\nEpoch 6149 / 9999  train Loss: 1.8938  val_loss: 0.5245  val_score: 0.5242  lr: 0.00100  elapsed: 662m 30s\nEpoch 6150 / 9999  train Loss: 1.8915  val_loss: 0.5199  val_score: 0.5197  lr: 0.00100  elapsed: 662m 37s\nEpoch 6151 / 9999  train Loss: 1.8906  val_loss: 0.5887  val_score: 0.5884  lr: 0.00100  elapsed: 662m 43s\nEpoch 6152 / 9999  train Loss: 1.8919  val_loss: 0.5913  val_score: 0.5909  lr: 0.00100  elapsed: 662m 50s\nEpoch 6153 / 9999  train Loss: 1.8890  val_loss: 0.6113  val_score: 0.6111  lr: 0.00100  elapsed: 662m 56s\nEpoch 6154 / 9999  train Loss: 1.8927  val_loss: 0.5261  val_score: 0.5257  lr: 0.00100  elapsed: 663m 3s\nEpoch 6155 / 9999  train Loss: 1.8909  val_loss: 0.5589  val_score: 0.5586  lr: 0.00100  elapsed: 663m 9s\nEpoch 6156 / 9999  train Loss: 1.8911  val_loss: 0.5563  val_score: 0.5560  lr: 0.00100  elapsed: 663m 16s\nEpoch 6157 / 9999  train Loss: 1.8897  val_loss: 0.6127  val_score: 0.6124  lr: 0.00100  elapsed: 663m 22s\nEpoch 6158 / 9999  train Loss: 1.8872  val_loss: 0.5526  val_score: 0.5524  lr: 0.00100  elapsed: 663m 29s\nEpoch 6159 / 9999  train Loss: 1.8928  val_loss: 0.5650  val_score: 0.5646  lr: 0.00100  elapsed: 663m 35s\nEpoch 6160 / 9999  train Loss: 1.8966  val_loss: 0.5429  val_score: 0.5424  lr: 0.00100  elapsed: 663m 42s\nEpoch 6161 / 9999  train Loss: 1.8904  val_loss: 0.5496  val_score: 0.5495  lr: 0.00100  elapsed: 663m 48s\nEpoch 6162 / 9999  train Loss: 1.8905  val_loss: 0.6054  val_score: 0.6051  lr: 0.00100  elapsed: 663m 55s\nEpoch 6163 / 9999  train Loss: 1.8892  val_loss: 0.5509  val_score: 0.5507  lr: 0.00100  elapsed: 664m 1s\nEpoch 6164 / 9999  train Loss: 1.8906  val_loss: 0.5841  val_score: 0.5839  lr: 0.00100  elapsed: 664m 8s\nEpoch 6165 / 9999  train Loss: 1.8923  val_loss: 0.5489  val_score: 0.5486  lr: 0.00100  elapsed: 664m 15s\nEpoch 6166 / 9999  train Loss: 1.8896  val_loss: 0.5844  val_score: 0.5841  lr: 0.00100  elapsed: 664m 21s\nEpoch 6167 / 9999  train Loss: 1.8926  val_loss: 0.5264  val_score: 0.5259  lr: 0.00100  elapsed: 664m 28s\nEpoch 6168 / 9999  train Loss: 1.8915  val_loss: 0.5286  val_score: 0.5283  lr: 0.00100  elapsed: 664m 34s\nEpoch 6169 / 9999  train Loss: 1.8901  val_loss: 0.5440  val_score: 0.5436  lr: 0.00100  elapsed: 664m 41s\nEpoch 6170 / 9999  train Loss: 1.8931  val_loss: 0.5491  val_score: 0.5488  lr: 0.00100  elapsed: 664m 47s\nEpoch 6171 / 9999  train Loss: 1.9120  val_loss: 0.5479  val_score: 0.5477  lr: 0.00100  elapsed: 664m 54s\nEpoch 6172 / 9999  train Loss: 1.8943  val_loss: 0.6125  val_score: 0.6123  lr: 0.00100  elapsed: 665m 0s\nEpoch 6173 / 9999  train Loss: 1.8882  val_loss: 0.5700  val_score: 0.5696  lr: 0.00100  elapsed: 665m 7s\nEpoch 6174 / 9999  train Loss: 1.8875  val_loss: 0.5730  val_score: 0.5727  lr: 0.00100  elapsed: 665m 14s\nEpoch 6175 / 9999  train Loss: 1.8922  val_loss: 0.5409  val_score: 0.5407  lr: 0.00100  elapsed: 665m 20s\nEpoch 6176 / 9999  train Loss: 1.8907  val_loss: 0.5796  val_score: 0.5797  lr: 0.00100  elapsed: 665m 27s\nEpoch 6177 / 9999  train Loss: 1.8910  val_loss: 0.5342  val_score: 0.5339  lr: 0.00100  elapsed: 665m 33s\nEpoch 6178 / 9999  train Loss: 1.8883  val_loss: 0.5635  val_score: 0.5631  lr: 0.00100  elapsed: 665m 40s\nEpoch 6179 / 9999  train Loss: 1.8880  val_loss: 0.5468  val_score: 0.5466  lr: 0.00100  elapsed: 665m 46s\nEpoch 6180 / 9999  train Loss: 1.8909  val_loss: 0.5875  val_score: 0.5873  lr: 0.00100  elapsed: 665m 53s\nEpoch 6181 / 9999  train Loss: 1.8904  val_loss: 0.5648  val_score: 0.5646  lr: 0.00100  elapsed: 665m 59s\nEpoch 6182 / 9999  train Loss: 1.8904  val_loss: 0.5486  val_score: 0.5484  lr: 0.00100  elapsed: 666m 6s\nEpoch 6183 / 9999  train Loss: 1.8923  val_loss: 0.5768  val_score: 0.5765  lr: 0.00100  elapsed: 666m 12s\nEpoch 6184 / 9999  train Loss: 1.8886  val_loss: 0.6029  val_score: 0.6027  lr: 0.00100  elapsed: 666m 18s\nEpoch 6185 / 9999  train Loss: 1.8892  val_loss: 0.5430  val_score: 0.5426  lr: 0.00100  elapsed: 666m 25s\nEpoch 6186 / 9999  train Loss: 1.8916  val_loss: 0.5457  val_score: 0.5454  lr: 0.00100  elapsed: 666m 31s\nEpoch 6187 / 9999  train Loss: 1.8887  val_loss: 0.5686  val_score: 0.5687  lr: 0.00100  elapsed: 666m 38s\nEpoch 6188 / 9999  train Loss: 1.8879  val_loss: 0.5603  val_score: 0.5602  lr: 0.00100  elapsed: 666m 44s\nEpoch 6189 / 9999  train Loss: 1.9040  val_loss: 0.5522  val_score: 0.5519  lr: 0.00100  elapsed: 666m 51s\nEpoch 6190 / 9999  train Loss: 1.8945  val_loss: 0.5896  val_score: 0.5893  lr: 0.00100  elapsed: 666m 57s\nEpoch 6191 / 9999  train Loss: 1.8943  val_loss: 0.5270  val_score: 0.5267  lr: 0.00100  elapsed: 667m 4s\nEpoch 6192 / 9999  train Loss: 1.8882  val_loss: 0.5181  val_score: 0.5178  lr: 0.00100  elapsed: 667m 10s\nEpoch 6193 / 9999  train Loss: 1.8883  val_loss: 0.5362  val_score: 0.5358  lr: 0.00100  elapsed: 667m 17s\nEpoch 6194 / 9999  train Loss: 1.8883  val_loss: 0.5645  val_score: 0.5642  lr: 0.00100  elapsed: 667m 23s\nEpoch 6195 / 9999  train Loss: 1.8903  val_loss: 0.5692  val_score: 0.5690  lr: 0.00100  elapsed: 667m 30s\nEpoch 6196 / 9999  train Loss: 1.8908  val_loss: 0.5637  val_score: 0.5634  lr: 0.00100  elapsed: 667m 36s\nEpoch 6197 / 9999  train Loss: 1.8883  val_loss: 0.5171  val_score: 0.5168  lr: 0.00100  elapsed: 667m 43s\nEpoch 6198 / 9999  train Loss: 1.8902  val_loss: 0.5480  val_score: 0.5476  lr: 0.00100  elapsed: 667m 49s\nEpoch 6199 / 9999  train Loss: 1.8919  val_loss: 0.5783  val_score: 0.5782  lr: 0.00100  elapsed: 667m 56s\nEpoch 6200 / 9999  train Loss: 1.8916  val_loss: 0.5953  val_score: 0.5952  lr: 0.00100  elapsed: 668m 2s\nEpoch 6201 / 9999  train Loss: 1.8911  val_loss: 0.5812  val_score: 0.5809  lr: 0.00100  elapsed: 668m 9s\nEpoch 6202 / 9999  train Loss: 1.8909  val_loss: 0.5733  val_score: 0.5731  lr: 0.00100  elapsed: 668m 15s\nEpoch 6203 / 9999  train Loss: 1.8873  val_loss: 0.5964  val_score: 0.5960  lr: 0.00100  elapsed: 668m 22s\nEpoch 6204 / 9999  train Loss: 1.8921  val_loss: 0.5401  val_score: 0.5398  lr: 0.00100  elapsed: 668m 28s\nEpoch 6205 / 9999  train Loss: 1.8893  val_loss: 0.5902  val_score: 0.5898  lr: 0.00100  elapsed: 668m 35s\nEpoch 6206 / 9999  train Loss: 1.8913  val_loss: 0.5234  val_score: 0.5231  lr: 0.00100  elapsed: 668m 42s\nEpoch 6207 / 9999  train Loss: 1.8891  val_loss: 0.5565  val_score: 0.5566  lr: 0.00100  elapsed: 668m 48s\nEpoch 6208 / 9999  train Loss: 1.8898  val_loss: 0.5489  val_score: 0.5486  lr: 0.00100  elapsed: 668m 54s\nEpoch 6209 / 9999  train Loss: 1.8882  val_loss: 0.5906  val_score: 0.5902  lr: 0.00100  elapsed: 669m 1s\nEpoch 6210 / 9999  train Loss: 1.8925  val_loss: 0.6041  val_score: 0.6034  lr: 0.00100  elapsed: 669m 7s\nEpoch 6211 / 9999  train Loss: 1.8946  val_loss: 0.5410  val_score: 0.5402  lr: 0.00100  elapsed: 669m 14s\nEpoch 6212 / 9999  train Loss: 1.8886  val_loss: 0.5083  val_score: 0.5080  lr: 0.00100  elapsed: 669m 20s\nEpoch 6213 / 9999  train Loss: 1.8890  val_loss: 0.5514  val_score: 0.5512  lr: 0.00100  elapsed: 669m 27s\nEpoch 6214 / 9999  train Loss: 1.9127  val_loss: 0.6191  val_score: 0.6193  lr: 0.00100  elapsed: 669m 33s\nEpoch 6215 / 9999  train Loss: 1.8907  val_loss: 0.5355  val_score: 0.5350  lr: 0.00100  elapsed: 669m 40s\nEpoch 6216 / 9999  train Loss: 1.8914  val_loss: 0.5655  val_score: 0.5653  lr: 0.00100  elapsed: 669m 47s\nEpoch 6217 / 9999  train Loss: 1.8877  val_loss: 0.5248  val_score: 0.5245  lr: 0.00100  elapsed: 669m 53s\nEpoch 6218 / 9999  train Loss: 1.8843  val_loss: 0.5253  val_score: 0.5249  lr: 0.00100  elapsed: 669m 60s\nEpoch 6219 / 9999  train Loss: 1.8878  val_loss: 0.5395  val_score: 0.5391  lr: 0.00100  elapsed: 670m 6s\nEpoch 6220 / 9999  train Loss: 1.8864  val_loss: 0.5515  val_score: 0.5513  lr: 0.00100  elapsed: 670m 13s\nEpoch 6221 / 9999  train Loss: 1.8896  val_loss: 0.6130  val_score: 0.6126  lr: 0.00100  elapsed: 670m 19s\nEpoch 6222 / 9999  train Loss: 1.8934  val_loss: 0.5317  val_score: 0.5315  lr: 0.00100  elapsed: 670m 26s\nEpoch 6223 / 9999  train Loss: 1.8903  val_loss: 0.5389  val_score: 0.5386  lr: 0.00100  elapsed: 670m 32s\nEpoch 6224 / 9999  train Loss: 1.8910  val_loss: 0.5281  val_score: 0.5279  lr: 0.00100  elapsed: 670m 39s\nEpoch 6225 / 9999  train Loss: 1.8887  val_loss: 0.5095  val_score: 0.5092  lr: 0.00100  elapsed: 670m 45s\nEpoch 6226 / 9999  train Loss: 1.8877  val_loss: 0.5224  val_score: 0.5221  lr: 0.00100  elapsed: 670m 52s\nEpoch 6227 / 9999  train Loss: 1.8948  val_loss: 0.6010  val_score: 0.6013  lr: 0.00100  elapsed: 670m 58s\nEpoch 6228 / 9999  train Loss: 1.8915  val_loss: 0.5368  val_score: 0.5363  lr: 0.00100  elapsed: 671m 4s\nEpoch 6229 / 9999  train Loss: 1.8914  val_loss: 0.5593  val_score: 0.5595  lr: 0.00100  elapsed: 671m 11s\nEpoch 6230 / 9999  train Loss: 1.8879  val_loss: 0.5270  val_score: 0.5269  lr: 0.00100  elapsed: 671m 17s\nEpoch 6231 / 9999  train Loss: 1.8899  val_loss: 0.6028  val_score: 0.6027  lr: 0.00100  elapsed: 671m 24s\nEpoch 6232 / 9999  train Loss: 1.8869  val_loss: 0.5363  val_score: 0.5361  lr: 0.00100  elapsed: 671m 31s\nEpoch 6233 / 9999  train Loss: 1.8944  val_loss: 0.5602  val_score: 0.5599  lr: 0.00100  elapsed: 671m 37s\nEpoch 6234 / 9999  train Loss: 1.8879  val_loss: 0.5543  val_score: 0.5542  lr: 0.00100  elapsed: 671m 44s\nEpoch 6235 / 9999  train Loss: 1.8966  val_loss: 0.5817  val_score: 0.5816  lr: 0.00100  elapsed: 671m 50s\nEpoch 6236 / 9999  train Loss: 1.8882  val_loss: 0.5677  val_score: 0.5678  lr: 0.00100  elapsed: 671m 57s\nEpoch 6237 / 9999  train Loss: 1.8888  val_loss: 0.5680  val_score: 0.5678  lr: 0.00100  elapsed: 672m 3s\nEpoch 6238 / 9999  train Loss: 1.8876  val_loss: 0.5743  val_score: 0.5740  lr: 0.00100  elapsed: 672m 10s\nEpoch 6239 / 9999  train Loss: 1.8903  val_loss: 0.5448  val_score: 0.5446  lr: 0.00100  elapsed: 672m 16s\nEpoch 6240 / 9999  train Loss: 1.8887  val_loss: 0.5986  val_score: 0.5983  lr: 0.00100  elapsed: 672m 23s\nEpoch 6241 / 9999  train Loss: 1.8876  val_loss: 0.5695  val_score: 0.5693  lr: 0.00100  elapsed: 672m 30s\nEpoch 6242 / 9999  train Loss: 1.8883  val_loss: 0.5477  val_score: 0.5475  lr: 0.00100  elapsed: 672m 36s\nEpoch 6243 / 9999  train Loss: 1.8890  val_loss: 0.5730  val_score: 0.5727  lr: 0.00100  elapsed: 672m 42s\nEpoch 6244 / 9999  train Loss: 1.8894  val_loss: 0.5354  val_score: 0.5351  lr: 0.00100  elapsed: 672m 49s\nEpoch 6245 / 9999  train Loss: 1.8888  val_loss: 0.5582  val_score: 0.5583  lr: 0.00100  elapsed: 672m 55s\nEpoch 6246 / 9999  train Loss: 1.8891  val_loss: 0.5588  val_score: 0.5582  lr: 0.00100  elapsed: 673m 2s\nEpoch 6247 / 9999  train Loss: 1.8950  val_loss: 0.5461  val_score: 0.5460  lr: 0.00100  elapsed: 673m 8s\nEpoch 6248 / 9999  train Loss: 1.8908  val_loss: 0.5122  val_score: 0.5120  lr: 0.00100  elapsed: 673m 15s\nEpoch 6249 / 9999  train Loss: 1.9035  val_loss: 0.5833  val_score: 0.5831  lr: 0.00100  elapsed: 673m 22s\nEpoch 6250 / 9999  train Loss: 1.8873  val_loss: 0.5904  val_score: 0.5902  lr: 0.00100  elapsed: 673m 28s\nEpoch 6251 / 9999  train Loss: 1.8860  val_loss: 0.5272  val_score: 0.5269  lr: 0.00100  elapsed: 673m 34s\nEpoch 6252 / 9999  train Loss: 1.8894  val_loss: 0.5518  val_score: 0.5515  lr: 0.00100  elapsed: 673m 41s\nEpoch 6253 / 9999  train Loss: 1.8874  val_loss: 0.5461  val_score: 0.5459  lr: 0.00100  elapsed: 673m 47s\nEpoch 6254 / 9999  train Loss: 1.8902  val_loss: 0.5658  val_score: 0.5655  lr: 0.00100  elapsed: 673m 54s\nEpoch 6255 / 9999  train Loss: 1.8865  val_loss: 0.5462  val_score: 0.5459  lr: 0.00100  elapsed: 674m 0s\nEpoch 6256 / 9999  train Loss: 1.8928  val_loss: 0.6205  val_score: 0.6204  lr: 0.00100  elapsed: 674m 7s\nEpoch 6257 / 9999  train Loss: 1.8898  val_loss: 0.5692  val_score: 0.5690  lr: 0.00100  elapsed: 674m 14s\nEpoch 6258 / 9999  train Loss: 1.8840  val_loss: 0.5763  val_score: 0.5759  lr: 0.00100  elapsed: 674m 20s\nEpoch 6259 / 9999  train Loss: 1.8926  val_loss: 0.5216  val_score: 0.5214  lr: 0.00100  elapsed: 674m 26s\nEpoch 6260 / 9999  train Loss: 1.8857  val_loss: 0.5680  val_score: 0.5676  lr: 0.00100  elapsed: 674m 33s\nEpoch 6261 / 9999  train Loss: 1.8844  val_loss: 0.5267  val_score: 0.5263  lr: 0.00100  elapsed: 674m 39s\nEpoch 6262 / 9999  train Loss: 1.8855  val_loss: 0.5479  val_score: 0.5475  lr: 0.00100  elapsed: 674m 46s\nEpoch 6263 / 9999  train Loss: 1.8897  val_loss: 0.5509  val_score: 0.5505  lr: 0.00100  elapsed: 674m 52s\nEpoch 6264 / 9999  train Loss: 1.8885  val_loss: 0.5504  val_score: 0.5502  lr: 0.00100  elapsed: 674m 59s\nEpoch 6265 / 9999  train Loss: 1.8901  val_loss: 0.5565  val_score: 0.5564  lr: 0.00100  elapsed: 675m 5s\nEpoch 6266 / 9999  train Loss: 1.8946  val_loss: 0.5589  val_score: 0.5587  lr: 0.00100  elapsed: 675m 12s\nEpoch 6267 / 9999  train Loss: 1.8894  val_loss: 0.5450  val_score: 0.5447  lr: 0.00100  elapsed: 675m 19s\nEpoch 6268 / 9999  train Loss: 1.8852  val_loss: 0.5389  val_score: 0.5387  lr: 0.00100  elapsed: 675m 25s\nEpoch 6269 / 9999  train Loss: 1.8891  val_loss: 0.5445  val_score: 0.5443  lr: 0.00100  elapsed: 675m 32s\nEpoch 6270 / 9999  train Loss: 1.8885  val_loss: 0.5914  val_score: 0.5916  lr: 0.00100  elapsed: 675m 38s\nEpoch 6271 / 9999  train Loss: 1.8893  val_loss: 0.5384  val_score: 0.5381  lr: 0.00100  elapsed: 675m 45s\nEpoch 6272 / 9999  train Loss: 1.8862  val_loss: 0.5597  val_score: 0.5594  lr: 0.00100  elapsed: 675m 51s\nEpoch 6273 / 9999  train Loss: 1.8857  val_loss: 0.5519  val_score: 0.5515  lr: 0.00100  elapsed: 675m 58s\nEpoch 6274 / 9999  train Loss: 1.8835  val_loss: 0.5585  val_score: 0.5582  lr: 0.00100  elapsed: 676m 4s\nEpoch 6275 / 9999  train Loss: 1.8879  val_loss: 0.5714  val_score: 0.5711  lr: 0.00100  elapsed: 676m 11s\nEpoch 6276 / 9999  train Loss: 1.8876  val_loss: 0.5926  val_score: 0.5923  lr: 0.00100  elapsed: 676m 17s\nEpoch 6277 / 9999  train Loss: 1.8858  val_loss: 0.5556  val_score: 0.5553  lr: 0.00100  elapsed: 676m 24s\nEpoch 6278 / 9999  train Loss: 1.8872  val_loss: 0.5738  val_score: 0.5736  lr: 0.00100  elapsed: 676m 30s\nEpoch 6279 / 9999  train Loss: 1.8883  val_loss: 0.5716  val_score: 0.5712  lr: 0.00100  elapsed: 676m 37s\nEpoch 6280 / 9999  train Loss: 1.8993  val_loss: 0.5829  val_score: 0.5825  lr: 0.00100  elapsed: 676m 44s\nEpoch 6281 / 9999  train Loss: 1.8908  val_loss: 0.5604  val_score: 0.5598  lr: 0.00100  elapsed: 676m 50s\nEpoch 6282 / 9999  train Loss: 1.8880  val_loss: 0.5906  val_score: 0.5906  lr: 0.00100  elapsed: 676m 57s\nEpoch 6283 / 9999  train Loss: 1.8830  val_loss: 0.5149  val_score: 0.5145  lr: 0.00100  elapsed: 677m 3s\nEpoch 6284 / 9999  train Loss: 1.8855  val_loss: 0.5428  val_score: 0.5424  lr: 0.00100  elapsed: 677m 10s\nEpoch 6285 / 9999  train Loss: 1.8858  val_loss: 0.5678  val_score: 0.5676  lr: 0.00100  elapsed: 677m 16s\nEpoch 6286 / 9999  train Loss: 1.8892  val_loss: 0.5432  val_score: 0.5428  lr: 0.00100  elapsed: 677m 23s\nEpoch 6287 / 9999  train Loss: 1.8858  val_loss: 0.5636  val_score: 0.5635  lr: 0.00100  elapsed: 677m 29s\nEpoch 6288 / 9999  train Loss: 1.8878  val_loss: 0.5310  val_score: 0.5309  lr: 0.00100  elapsed: 677m 36s\nEpoch 6289 / 9999  train Loss: 1.8992  val_loss: 0.5797  val_score: 0.5796  lr: 0.00100  elapsed: 677m 42s\nEpoch 6290 / 9999  train Loss: 1.8880  val_loss: 0.5970  val_score: 0.5971  lr: 0.00100  elapsed: 677m 48s\nEpoch 6291 / 9999  train Loss: 1.8845  val_loss: 0.5358  val_score: 0.5355  lr: 0.00100  elapsed: 677m 55s\nEpoch 6292 / 9999  train Loss: 1.8843  val_loss: 0.5882  val_score: 0.5880  lr: 0.00100  elapsed: 678m 2s\nEpoch 6293 / 9999  train Loss: 1.8854  val_loss: 0.6167  val_score: 0.6168  lr: 0.00100  elapsed: 678m 8s\nEpoch 6294 / 9999  train Loss: 1.8869  val_loss: 0.5110  val_score: 0.5108  lr: 0.00100  elapsed: 678m 15s\nEpoch 6295 / 9999  train Loss: 1.8866  val_loss: 0.5473  val_score: 0.5470  lr: 0.00100  elapsed: 678m 21s\nEpoch 6296 / 9999  train Loss: 1.8925  val_loss: 0.5604  val_score: 0.5602  lr: 0.00100  elapsed: 678m 28s\nEpoch 6297 / 9999  train Loss: 1.8892  val_loss: 0.5700  val_score: 0.5697  lr: 0.00100  elapsed: 678m 34s\nEpoch 6298 / 9999  train Loss: 1.8859  val_loss: 0.5276  val_score: 0.5273  lr: 0.00100  elapsed: 678m 41s\nEpoch 6299 / 9999  train Loss: 1.8876  val_loss: 0.6054  val_score: 0.6051  lr: 0.00100  elapsed: 678m 47s\nEpoch 6300 / 9999  train Loss: 1.8886  val_loss: 0.5679  val_score: 0.5680  lr: 0.00100  elapsed: 678m 54s\nEpoch 6301 / 9999  train Loss: 1.8880  val_loss: 0.5251  val_score: 0.5247  lr: 0.00100  elapsed: 679m 0s\nEpoch 6302 / 9999  train Loss: 1.8843  val_loss: 0.5587  val_score: 0.5583  lr: 0.00100  elapsed: 679m 7s\nEpoch 6303 / 9999  train Loss: 1.8861  val_loss: 0.5713  val_score: 0.5711  lr: 0.00100  elapsed: 679m 13s\nEpoch 6304 / 9999  train Loss: 1.8919  val_loss: 0.6069  val_score: 0.6066  lr: 0.00100  elapsed: 679m 20s\nEpoch 6305 / 9999  train Loss: 1.8878  val_loss: 0.5424  val_score: 0.5421  lr: 0.00100  elapsed: 679m 26s\nEpoch 6306 / 9999  train Loss: 1.8873  val_loss: 0.5404  val_score: 0.5401  lr: 0.00100  elapsed: 679m 33s\nEpoch 6307 / 9999  train Loss: 1.8871  val_loss: 0.5490  val_score: 0.5485  lr: 0.00100  elapsed: 679m 39s\nEpoch 6308 / 9999  train Loss: 1.8882  val_loss: 0.6152  val_score: 0.6150  lr: 0.00100  elapsed: 679m 45s\nEpoch 6309 / 9999  train Loss: 1.8883  val_loss: 0.5804  val_score: 0.5800  lr: 0.00100  elapsed: 679m 52s\nEpoch 6310 / 9999  train Loss: 1.8876  val_loss: 0.5923  val_score: 0.5920  lr: 0.00100  elapsed: 679m 58s\nEpoch 6311 / 9999  train Loss: 1.8858  val_loss: 0.5563  val_score: 0.5559  lr: 0.00100  elapsed: 680m 5s\nEpoch 6312 / 9999  train Loss: 1.8873  val_loss: 0.5712  val_score: 0.5709  lr: 0.00100  elapsed: 680m 11s\nEpoch 6313 / 9999  train Loss: 1.8938  val_loss: 0.6120  val_score: 0.6117  lr: 0.00100  elapsed: 680m 18s\nEpoch 6314 / 9999  train Loss: 1.8899  val_loss: 0.5464  val_score: 0.5465  lr: 0.00100  elapsed: 680m 25s\nEpoch 6315 / 9999  train Loss: 1.8896  val_loss: 0.5803  val_score: 0.5800  lr: 0.00100  elapsed: 680m 31s\nEpoch 6316 / 9999  train Loss: 1.8946  val_loss: 0.5978  val_score: 0.5979  lr: 0.00100  elapsed: 680m 38s\n"
    }
   ],
   "source": [
    "# output path\n",
    "#output_dir = Path('./', 'output')\n",
    "#output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "num_epochs = 10000\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "best_epoch_list = []\n",
    "best_valid_score_list = []\n",
    "\n",
    "# build model\n",
    "model = build_model(device, model_name='mlp')\n",
    "model.to(device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = AdamW(model.parameters(), lr)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "best_epoch = 0\n",
    "best_train_loss = 1000\n",
    "best_valid_score = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "        else:\n",
    "            target = target.float()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    val_loss, val_score = validation(model, criterion, valid_loader, device)\n",
    "\n",
    "    elapsed = time() - start_time\n",
    "\n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "\n",
    "    print('Epoch {} / {}  train Loss: {:.4f}  val_loss: {:.4f}  val_score: {:.4f}  lr: {:.5f}  elapsed: {:.0f}m {:.0f}s' \\\n",
    "            .format(epoch,  num_epochs - 1, train_loss, val_loss, val_score, lr[0], elapsed // 60, elapsed % 60))\n",
    "        \n",
    "    #model_path = output_dir / 'best_model.pt'\n",
    "    model_path = '../wafer/mlp_weights/mlp_v1.pt'\n",
    "\n",
    "    if val_score < best_valid_score:\n",
    "        best_valid_score = val_score\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> loss improved to {}'.format(best_valid_score))\n",
    "\n",
    "\n",
    "    best_epoch_list.append(best_epoch)\n",
    "    best_valid_score_list.append(best_valid_score)\n",
    "print(\"==================== mlp - Best val_loss - {:.5f} =================\".format(best_valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "9361"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.8831654755863143"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_to = round(best_valid_score, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n8336 2.8953987583113308\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\n9361 2.8831654755863143\nno label\n"
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 2048\n",
    "test_loader = build_dataloader(test_df.iloc[:, 1:].values, Y=None, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = build_model(device, model_name='mlp')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_preds = np.zeros((len(test_loader.dataset), 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "        outputs = model(data)\n",
    "        test_preds[batch_idx * batch_size:(batch_idx+1) * batch_size] = outputs.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': submission_df['id'],\n",
    "                           'layer_1':test_preds.transpose()[0],\n",
    "                           'layer_2':test_preds.transpose()[1],\n",
    "                           'layer_3':test_preds.transpose()[2],\n",
    "                           'layer_4':test_preds.transpose()[3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>layer_1</th>\n      <th>layer_2</th>\n      <th>layer_3</th>\n      <th>layer_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>253.896347</td>\n      <td>229.427444</td>\n      <td>131.345062</td>\n      <td>87.206612</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>157.500031</td>\n      <td>125.458023</td>\n      <td>239.678467</td>\n      <td>100.030914</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>145.345810</td>\n      <td>174.459229</td>\n      <td>271.544556</td>\n      <td>160.048889</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>92.477386</td>\n      <td>237.762543</td>\n      <td>184.654312</td>\n      <td>82.487114</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>273.964264</td>\n      <td>295.956055</td>\n      <td>245.735535</td>\n      <td>274.909302</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   id     layer_1     layer_2     layer_3     layer_4\n0   0  253.896347  229.427444  131.345062   87.206612\n1   1  157.500031  125.458023  239.678467  100.030914\n2   2  145.345810  174.459229  271.544556  160.048889\n3   3   92.477386  237.762543  184.654312   82.487114\n4   4  273.964264  295.956055  245.735535  274.909302"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../wafer/mlp_submission/mlp_v1_{}_submission.csv'.format(score_to), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}