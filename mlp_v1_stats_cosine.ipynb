{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.optim import Adam, SGD, Optimizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import CosineAnnealingWithRestartsLR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Semi_dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.X_dataset = []\n",
    "        self.Y_dataset = []\n",
    "        for x in X:\n",
    "            self.X_dataset.append(torch.FloatTensor(x))\n",
    "        try:\n",
    "            for y in Y.values:\n",
    "                self.Y_dataset.append(torch.tensor(y))\n",
    "        except:\n",
    "            print(\"no label\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X_dataset[index]\n",
    "        try:\n",
    "            target = self.Y_dataset[index]\n",
    "            return data, target\n",
    "        except:\n",
    "            return data\n",
    "\n",
    "\n",
    "def build_dataloader(X, Y, batch_size, shuffle=False):\n",
    "    \n",
    "    dataset = Semi_dataset(X, Y)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0\n",
    "                            )\n",
    "    return dataloader\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred,\n",
    "                        sample_weight=None,\n",
    "                        multioutput='uniform_average'):\n",
    "    \n",
    "    output_errors = np.average(np.abs(y_pred - y_true),\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class MLP_only_flatfeatures(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(MLP_only_flatfeatures, self).__init__()\n",
    "        self.num_classes = num_classes         \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            #nn.Linear(226, 1000),\n",
    "            nn.Linear(230, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            ####### Block 1 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            ####### Block 2 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            ####### Block 3 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            #######################\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, self.num_classes)\n",
    "            )             \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_layers(x)\n",
    "        return out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def build_model(device, model_name='mlp', weight_path=None):\n",
    "\n",
    "    if model_name == 'mlp':\n",
    "        model = MLP_only_flatfeatures(4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def validation(model, criterion, valid_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), 4))\n",
    "    valid_targets = np.zeros((len(valid_loader.dataset), 4))\n",
    "    val_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(valid_loader):\n",
    "            \n",
    "            valid_targets[i * batch_size: (i+1) * batch_size] = target.float().numpy().copy()\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "                \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_preds[i * batch_size: (i+1) * batch_size] = output.detach().cpu().numpy()\n",
    "            \n",
    "            val_loss += loss.item() / len(valid_loader)\n",
    "    \n",
    "    val_score = mean_absolute_error(valid_preds, valid_targets)\n",
    "\n",
    "    return val_loss, val_score\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "if cuda.is_available:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 13.2 s, sys: 1.89 s, total: 15 s\nWall time: 12.2 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "DATASET_PATH = '../wafer'\n",
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(DATASET_PATH, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 12.5 s, sys: 1.18 s, total: 13.7 s\nWall time: 4.05 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "train_df['226'] = train_df.iloc[:,4:229].std(axis=1)\n",
    "train_df['227'] = train_df.iloc[:,4:229].min(axis=1)\n",
    "train_df['228'] = train_df.iloc[:,4:229].max(axis=1)\n",
    "train_df['229'] = train_df.iloc[:,4:229].mean(axis=1)\n",
    "\n",
    "test_df['226'] = test_df.iloc[:,1:226].std(axis=1)\n",
    "test_df['227'] = test_df.iloc[:,1:226].min(axis=1)\n",
    "test_df['228'] = test_df.iloc[:,1:226].max(axis=1)\n",
    "test_df['229'] = test_df.iloc[:,1:226].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_df.iloc[:, 4:], train_df.iloc[:, :4], test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "batch_size = 2048\n",
    "train_loader = build_dataloader(X_train, y_train, batch_size, shuffle=True)\n",
    "valid_loader = build_dataloader(X_val, y_val, batch_size, shuffle=False)\n",
    "\n",
    "test_df.iloc[:, 1:] = scaler.transform(test_df.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "d: 77m 41s\nEpoch 812 / 999  train Loss: 3.1970  val_loss: 0.9082  val_score: 0.9081  lr: 0.00080  elapsed: 77m 47s\nEpoch 813 / 999  train Loss: 3.1980  val_loss: 0.9337  val_score: 0.9337  lr: 0.00079  elapsed: 77m 53s\nEpoch 814 / 999  train Loss: 3.1954  val_loss: 0.9548  val_score: 0.9545  lr: 0.00080  elapsed: 77m 58s\nEpoch 815 / 999  train Loss: 3.1972  val_loss: 0.9413  val_score: 0.9412  lr: 0.00080  elapsed: 78m 4s\nEpoch 816 / 999  train Loss: 3.1984  val_loss: 0.9593  val_score: 0.9592  lr: 0.00080  elapsed: 78m 10s\nEpoch 817 / 999  train Loss: 3.2056  val_loss: 0.9130  val_score: 0.9135  lr: 0.00080  elapsed: 78m 16s\nEpoch 818 / 999  train Loss: 3.1993  val_loss: 0.9489  val_score: 0.9489  lr: 0.00080  elapsed: 78m 21s\nEpoch 819 / 999  train Loss: 3.2035  val_loss: 0.9484  val_score: 0.9482  lr: 0.00080  elapsed: 78m 27s\nEpoch 820 / 999  train Loss: 3.1941  val_loss: 0.9527  val_score: 0.9528  lr: 0.00080  elapsed: 78m 33s\nEpoch 821 / 999  train Loss: 3.1928  val_loss: 0.8982  val_score: 0.8984  lr: 0.00080  elapsed: 78m 39s\nEpoch 822 / 999  train Loss: 3.1866  val_loss: 0.9675  val_score: 0.9675  lr: 0.00079  elapsed: 78m 45s\nEpoch 823 / 999  train Loss: 3.1904  val_loss: 0.9191  val_score: 0.9191  lr: 0.00080  elapsed: 78m 50s\nEpoch 824 / 999  train Loss: 3.1881  val_loss: 0.9306  val_score: 0.9304  lr: 0.00080  elapsed: 78m 56s\nEpoch 825 / 999  train Loss: 3.2004  val_loss: 0.9245  val_score: 0.9241  lr: 0.00080  elapsed: 79m 2s\nEpoch 826 / 999  train Loss: 3.1940  val_loss: 0.9395  val_score: 0.9392  lr: 0.00080  elapsed: 79m 7s\nEpoch 827 / 999  train Loss: 3.1958  val_loss: 0.9131  val_score: 0.9129  lr: 0.00080  elapsed: 79m 13s\nEpoch 828 / 999  train Loss: 3.2081  val_loss: 0.8967  val_score: 0.8968  lr: 0.00080  elapsed: 79m 19s\nEpoch 829 / 999  train Loss: 3.1933  val_loss: 0.9008  val_score: 0.9007  lr: 0.00079  elapsed: 79m 25s\nEpoch 830 / 999  train Loss: 3.1936  val_loss: 0.8939  val_score: 0.8938  lr: 0.00079  elapsed: 79m 31s\nEpoch 831 / 999  train Loss: 3.1913  val_loss: 0.8966  val_score: 0.8964  lr: 0.00079  elapsed: 79m 36s\nEpoch 832 / 999  train Loss: 3.1871  val_loss: 0.9159  val_score: 0.9160  lr: 0.00079  elapsed: 79m 42s\nEpoch 833 / 999  train Loss: 3.1893  val_loss: 0.8809  val_score: 0.8807  lr: 0.00080  elapsed: 79m 48s\nEpoch 834 / 999  train Loss: 3.1896  val_loss: 0.9109  val_score: 0.9109  lr: 0.00079  elapsed: 79m 54s\nEpoch 835 / 999  train Loss: 3.1894  val_loss: 0.9371  val_score: 0.9368  lr: 0.00080  elapsed: 79m 59s\nEpoch 836 / 999  train Loss: 3.1948  val_loss: 0.9642  val_score: 0.9643  lr: 0.00080  elapsed: 80m 5s\nEpoch 837 / 999  train Loss: 3.2004  val_loss: 0.9428  val_score: 0.9428  lr: 0.00080  elapsed: 80m 11s\nEpoch 838 / 999  train Loss: 3.1890  val_loss: 0.8985  val_score: 0.8984  lr: 0.00080  elapsed: 80m 17s\nEpoch 839 / 999  train Loss: 3.1876  val_loss: 0.9119  val_score: 0.9121  lr: 0.00079  elapsed: 80m 22s\nEpoch 840 / 999  train Loss: 3.1874  val_loss: 0.8618  val_score: 0.8615  lr: 0.00080  elapsed: 80m 28s\n----------------------------------------------------------------------->> loss improved to 0.86155\nEpoch 841 / 999  train Loss: 3.1866  val_loss: 0.9102  val_score: 0.9101  lr: 0.00079  elapsed: 80m 34s\nEpoch 842 / 999  train Loss: 3.1827  val_loss: 0.9689  val_score: 0.9687  lr: 0.00080  elapsed: 80m 40s\nEpoch 843 / 999  train Loss: 3.1954  val_loss: 0.9130  val_score: 0.9134  lr: 0.00080  elapsed: 80m 45s\nEpoch 844 / 999  train Loss: 3.1878  val_loss: 0.9639  val_score: 0.9639  lr: 0.00080  elapsed: 80m 51s\nEpoch 845 / 999  train Loss: 3.2062  val_loss: 0.9242  val_score: 0.9246  lr: 0.00080  elapsed: 80m 57s\nEpoch 846 / 999  train Loss: 3.1872  val_loss: 0.8501  val_score: 0.8500  lr: 0.00080  elapsed: 81m 2s\n----------------------------------------------------------------------->> loss improved to 0.84996\nEpoch 847 / 999  train Loss: 3.1810  val_loss: 0.8669  val_score: 0.8667  lr: 0.00079  elapsed: 81m 8s\nEpoch 848 / 999  train Loss: 3.1827  val_loss: 0.8550  val_score: 0.8548  lr: 0.00079  elapsed: 81m 14s\nEpoch 849 / 999  train Loss: 3.1841  val_loss: 0.9255  val_score: 0.9257  lr: 0.00079  elapsed: 81m 20s\nEpoch 850 / 999  train Loss: 3.1848  val_loss: 1.0477  val_score: 1.0478  lr: 0.00080  elapsed: 81m 25s\nEpoch 851 / 999  train Loss: 3.1875  val_loss: 0.9094  val_score: 0.9094  lr: 0.00081  elapsed: 81m 31s\nEpoch 852 / 999  train Loss: 3.1871  val_loss: 0.8943  val_score: 0.8944  lr: 0.00080  elapsed: 81m 37s\nEpoch 853 / 999  train Loss: 3.1799  val_loss: 0.9150  val_score: 0.9151  lr: 0.00079  elapsed: 81m 42s\nEpoch 854 / 999  train Loss: 3.1837  val_loss: 0.8935  val_score: 0.8935  lr: 0.00080  elapsed: 81m 48s\nEpoch 855 / 999  train Loss: 3.1815  val_loss: 0.9613  val_score: 0.9614  lr: 0.00079  elapsed: 81m 54s\nEpoch 856 / 999  train Loss: 3.1825  val_loss: 0.8906  val_score: 0.8903  lr: 0.00080  elapsed: 81m 60s\nEpoch 857 / 999  train Loss: 3.1962  val_loss: 0.9293  val_score: 0.9296  lr: 0.00079  elapsed: 82m 5s\nEpoch 858 / 999  train Loss: 3.1904  val_loss: 0.8882  val_score: 0.8882  lr: 0.00080  elapsed: 82m 11s\nEpoch 859 / 999  train Loss: 3.1799  val_loss: 0.8791  val_score: 0.8792  lr: 0.00079  elapsed: 82m 17s\nEpoch 860 / 999  train Loss: 3.1801  val_loss: 0.9080  val_score: 0.9083  lr: 0.00079  elapsed: 82m 22s\nEpoch 861 / 999  train Loss: 3.1931  val_loss: 0.8934  val_score: 0.8934  lr: 0.00080  elapsed: 82m 28s\nEpoch 862 / 999  train Loss: 3.1826  val_loss: 0.9721  val_score: 0.9720  lr: 0.00079  elapsed: 82m 34s\nEpoch 863 / 999  train Loss: 3.1911  val_loss: 0.8698  val_score: 0.8698  lr: 0.00080  elapsed: 82m 40s\nEpoch 864 / 999  train Loss: 3.1829  val_loss: 0.9140  val_score: 0.9141  lr: 0.00079  elapsed: 82m 45s\nEpoch 865 / 999  train Loss: 3.1823  val_loss: 0.8715  val_score: 0.8714  lr: 0.00080  elapsed: 82m 51s\nEpoch 866 / 999  train Loss: 3.1788  val_loss: 0.9309  val_score: 0.9308  lr: 0.00079  elapsed: 82m 57s\nEpoch 867 / 999  train Loss: 3.1799  val_loss: 0.9631  val_score: 0.9632  lr: 0.00080  elapsed: 83m 2s\nEpoch 868 / 999  train Loss: 3.1846  val_loss: 0.8762  val_score: 0.8758  lr: 0.00080  elapsed: 83m 8s\nEpoch 869 / 999  train Loss: 3.1758  val_loss: 0.8674  val_score: 0.8673  lr: 0.00079  elapsed: 83m 14s\nEpoch 870 / 999  train Loss: 3.1841  val_loss: 0.8912  val_score: 0.8913  lr: 0.00079  elapsed: 83m 20s\nEpoch 871 / 999  train Loss: 3.1792  val_loss: 0.8727  val_score: 0.8732  lr: 0.00079  elapsed: 83m 26s\nEpoch 872 / 999  train Loss: 3.1771  val_loss: 0.9420  val_score: 0.9420  lr: 0.00079  elapsed: 83m 31s\nEpoch 873 / 999  train Loss: 3.1878  val_loss: 0.8781  val_score: 0.8782  lr: 0.00080  elapsed: 83m 37s\nEpoch 874 / 999  train Loss: 3.1894  val_loss: 0.8902  val_score: 0.8904  lr: 0.00079  elapsed: 83m 43s\nEpoch 875 / 999  train Loss: 3.1784  val_loss: 0.8923  val_score: 0.8924  lr: 0.00079  elapsed: 83m 48s\nEpoch 876 / 999  train Loss: 3.1759  val_loss: 0.9085  val_score: 0.9086  lr: 0.00079  elapsed: 83m 54s\nEpoch 877 / 999  train Loss: 3.1759  val_loss: 0.8537  val_score: 0.8538  lr: 0.00080  elapsed: 84m 0s\nEpoch 878 / 999  train Loss: 3.1717  val_loss: 0.8620  val_score: 0.8618  lr: 0.00079  elapsed: 84m 6s\nEpoch 879 / 999  train Loss: 3.1778  val_loss: 0.8947  val_score: 0.8948  lr: 0.00079  elapsed: 84m 11s\nEpoch 880 / 999  train Loss: 3.1797  val_loss: 0.8856  val_score: 0.8856  lr: 0.00079  elapsed: 84m 17s\nEpoch 881 / 999  train Loss: 3.1706  val_loss: 0.8892  val_score: 0.8890  lr: 0.00079  elapsed: 84m 23s\nEpoch 882 / 999  train Loss: 3.1720  val_loss: 0.9535  val_score: 0.9531  lr: 0.00079  elapsed: 84m 29s\nEpoch 883 / 999  train Loss: 3.1755  val_loss: 0.8848  val_score: 0.8845  lr: 0.00080  elapsed: 84m 34s\nEpoch 884 / 999  train Loss: 3.1795  val_loss: 0.9250  val_score: 0.9250  lr: 0.00079  elapsed: 84m 40s\nEpoch 885 / 999  train Loss: 3.1839  val_loss: 0.8596  val_score: 0.8596  lr: 0.00080  elapsed: 84m 46s\nEpoch 886 / 999  train Loss: 3.1728  val_loss: 0.9434  val_score: 0.9432  lr: 0.00079  elapsed: 84m 52s\nEpoch 887 / 999  train Loss: 3.1722  val_loss: 0.8860  val_score: 0.8862  lr: 0.00080  elapsed: 84m 57s\nEpoch 888 / 999  train Loss: 3.1751  val_loss: 0.8772  val_score: 0.8770  lr: 0.00079  elapsed: 85m 3s\nEpoch 889 / 999  train Loss: 3.1780  val_loss: 0.8748  val_score: 0.8747  lr: 0.00079  elapsed: 85m 9s\nEpoch 890 / 999  train Loss: 3.1869  val_loss: 0.9068  val_score: 0.9071  lr: 0.00079  elapsed: 85m 15s\nEpoch 891 / 999  train Loss: 3.1657  val_loss: 0.8704  val_score: 0.8704  lr: 0.00079  elapsed: 85m 20s\nEpoch 892 / 999  train Loss: 3.1722  val_loss: 0.9417  val_score: 0.9413  lr: 0.00079  elapsed: 85m 26s\nEpoch 893 / 999  train Loss: 3.1722  val_loss: 0.8907  val_score: 0.8907  lr: 0.00080  elapsed: 85m 32s\nEpoch 894 / 999  train Loss: 3.1647  val_loss: 0.9247  val_score: 0.9247  lr: 0.00079  elapsed: 85m 38s\nEpoch 895 / 999  train Loss: 3.1729  val_loss: 0.8761  val_score: 0.8763  lr: 0.00080  elapsed: 85m 44s\nEpoch 896 / 999  train Loss: 3.1751  val_loss: 0.9210  val_score: 0.9209  lr: 0.00079  elapsed: 85m 50s\nEpoch 897 / 999  train Loss: 3.1738  val_loss: 0.8812  val_score: 0.8812  lr: 0.00080  elapsed: 85m 55s\nEpoch 898 / 999  train Loss: 3.1725  val_loss: 0.8803  val_score: 0.8801  lr: 0.00079  elapsed: 86m 1s\nEpoch 899 / 999  train Loss: 3.1678  val_loss: 0.9206  val_score: 0.9205  lr: 0.00079  elapsed: 86m 7s\nEpoch 900 / 999  train Loss: 3.1744  val_loss: 0.8762  val_score: 0.8755  lr: 0.00080  elapsed: 86m 12s\nEpoch 901 / 999  train Loss: 3.1694  val_loss: 0.9460  val_score: 0.9457  lr: 0.00079  elapsed: 86m 18s\nEpoch 902 / 999  train Loss: 3.1759  val_loss: 0.8778  val_score: 0.8777  lr: 0.00080  elapsed: 86m 24s\nEpoch 903 / 999  train Loss: 3.1682  val_loss: 0.9035  val_score: 0.9036  lr: 0.00079  elapsed: 86m 29s\nEpoch 904 / 999  train Loss: 3.1658  val_loss: 0.8686  val_score: 0.8686  lr: 0.00079  elapsed: 86m 35s\nEpoch 905 / 999  train Loss: 3.1712  val_loss: 0.9631  val_score: 0.9627  lr: 0.00079  elapsed: 86m 41s\nEpoch 906 / 999  train Loss: 3.1671  val_loss: 0.9188  val_score: 0.9187  lr: 0.00080  elapsed: 86m 47s\nEpoch 907 / 999  train Loss: 3.1651  val_loss: 0.9254  val_score: 0.9250  lr: 0.00080  elapsed: 86m 53s\nEpoch 908 / 999  train Loss: 3.1653  val_loss: 0.9218  val_score: 0.9216  lr: 0.00080  elapsed: 86m 58s\nEpoch 909 / 999  train Loss: 3.1684  val_loss: 0.8642  val_score: 0.8641  lr: 0.00080  elapsed: 87m 4s\nEpoch 910 / 999  train Loss: 3.1679  val_loss: 0.9383  val_score: 0.9381  lr: 0.00079  elapsed: 87m 10s\nEpoch 911 / 999  train Loss: 3.1721  val_loss: 0.8987  val_score: 0.8987  lr: 0.00080  elapsed: 87m 16s\nEpoch 912 / 999  train Loss: 3.1710  val_loss: 0.8759  val_score: 0.8759  lr: 0.00079  elapsed: 87m 21s\nEpoch 913 / 999  train Loss: 3.1739  val_loss: 0.8945  val_score: 0.8944  lr: 0.00079  elapsed: 87m 27s\nEpoch 914 / 999  train Loss: 3.1651  val_loss: 0.8910  val_score: 0.8909  lr: 0.00079  elapsed: 87m 33s\nEpoch 915 / 999  train Loss: 3.1619  val_loss: 0.9148  val_score: 0.9148  lr: 0.00079  elapsed: 87m 39s\nEpoch 916 / 999  train Loss: 3.1637  val_loss: 0.9790  val_score: 0.9790  lr: 0.00080  elapsed: 87m 44s\nEpoch 917 / 999  train Loss: 3.1669  val_loss: 0.8925  val_score: 0.8924  lr: 0.00080  elapsed: 87m 50s\nEpoch 918 / 999  train Loss: 3.1629  val_loss: 0.8825  val_score: 0.8822  lr: 0.00079  elapsed: 87m 56s\nEpoch 919 / 999  train Loss: 3.1635  val_loss: 0.8829  val_score: 0.8829  lr: 0.00079  elapsed: 88m 1s\nEpoch 920 / 999  train Loss: 3.1643  val_loss: 0.8686  val_score: 0.8686  lr: 0.00079  elapsed: 88m 7s\nEpoch 921 / 999  train Loss: 3.1677  val_loss: 0.8723  val_score: 0.8724  lr: 0.00079  elapsed: 88m 13s\nEpoch 922 / 999  train Loss: 3.1677  val_loss: 0.8939  val_score: 0.8937  lr: 0.00079  elapsed: 88m 19s\nEpoch 923 / 999  train Loss: 3.1608  val_loss: 0.8590  val_score: 0.8587  lr: 0.00079  elapsed: 88m 24s\nEpoch 924 / 999  train Loss: 3.1624  val_loss: 0.9216  val_score: 0.9214  lr: 0.00079  elapsed: 88m 30s\nEpoch 925 / 999  train Loss: 3.1677  val_loss: 0.9143  val_score: 0.9142  lr: 0.00080  elapsed: 88m 36s\nEpoch 926 / 999  train Loss: 3.1601  val_loss: 0.8581  val_score: 0.8579  lr: 0.00080  elapsed: 88m 42s\nEpoch 927 / 999  train Loss: 3.1609  val_loss: 0.8935  val_score: 0.8935  lr: 0.00079  elapsed: 88m 47s\nEpoch 928 / 999  train Loss: 3.1674  val_loss: 0.9342  val_score: 0.9343  lr: 0.00079  elapsed: 88m 53s\nEpoch 929 / 999  train Loss: 3.1622  val_loss: 0.8677  val_score: 0.8678  lr: 0.00080  elapsed: 88m 59s\nEpoch 930 / 999  train Loss: 3.1605  val_loss: 0.8893  val_score: 0.8893  lr: 0.00079  elapsed: 89m 4s\nEpoch 931 / 999  train Loss: 3.1570  val_loss: 0.9507  val_score: 0.9508  lr: 0.00079  elapsed: 89m 10s\nEpoch 932 / 999  train Loss: 3.1654  val_loss: 0.9121  val_score: 0.9118  lr: 0.00080  elapsed: 89m 15s\nEpoch 933 / 999  train Loss: 3.1596  val_loss: 0.9327  val_score: 0.9327  lr: 0.00080  elapsed: 89m 21s\nEpoch 934 / 999  train Loss: 3.1618  val_loss: 0.8727  val_score: 0.8728  lr: 0.00080  elapsed: 89m 27s\nEpoch 935 / 999  train Loss: 3.1577  val_loss: 0.9177  val_score: 0.9180  lr: 0.00079  elapsed: 89m 32s\nEpoch 936 / 999  train Loss: 3.1695  val_loss: 0.9065  val_score: 0.9064  lr: 0.00080  elapsed: 89m 38s\nEpoch 937 / 999  train Loss: 3.1598  val_loss: 0.9354  val_score: 0.9351  lr: 0.00079  elapsed: 89m 44s\nEpoch 938 / 999  train Loss: 3.1535  val_loss: 0.9015  val_score: 0.9013  lr: 0.00080  elapsed: 89m 49s\nEpoch 939 / 999  train Loss: 3.1592  val_loss: 0.8700  val_score: 0.8701  lr: 0.00079  elapsed: 89m 55s\nEpoch 940 / 999  train Loss: 3.1677  val_loss: 0.9136  val_score: 0.9133  lr: 0.00079  elapsed: 90m 1s\nEpoch 941 / 999  train Loss: 3.1665  val_loss: 0.8922  val_score: 0.8922  lr: 0.00080  elapsed: 90m 7s\nEpoch 942 / 999  train Loss: 3.1583  val_loss: 0.8664  val_score: 0.8663  lr: 0.00079  elapsed: 90m 12s\nEpoch 943 / 999  train Loss: 3.1519  val_loss: 0.8773  val_score: 0.8773  lr: 0.00079  elapsed: 90m 18s\nEpoch 944 / 999  train Loss: 3.1508  val_loss: 0.8954  val_score: 0.8952  lr: 0.00079  elapsed: 90m 24s\nEpoch 945 / 999  train Loss: 3.1542  val_loss: 0.8431  val_score: 0.8435  lr: 0.00079  elapsed: 90m 30s\n----------------------------------------------------------------------->> loss improved to 0.84351\nEpoch 946 / 999  train Loss: 3.1603  val_loss: 0.9762  val_score: 0.9762  lr: 0.00079  elapsed: 90m 35s\nEpoch 947 / 999  train Loss: 3.1699  val_loss: 0.9392  val_score: 0.9392  lr: 0.00080  elapsed: 90m 41s\nEpoch 948 / 999  train Loss: 3.1648  val_loss: 0.9493  val_score: 0.9486  lr: 0.00080  elapsed: 90m 47s\nEpoch 949 / 999  train Loss: 3.1555  val_loss: 0.8656  val_score: 0.8655  lr: 0.00080  elapsed: 90m 53s\nEpoch 950 / 999  train Loss: 3.1542  val_loss: 0.8582  val_score: 0.8584  lr: 0.00079  elapsed: 90m 58s\nEpoch 951 / 999  train Loss: 3.1540  val_loss: 0.8541  val_score: 0.8543  lr: 0.00079  elapsed: 91m 4s\nEpoch 952 / 999  train Loss: 3.1629  val_loss: 0.9334  val_score: 0.9333  lr: 0.00079  elapsed: 91m 10s\nEpoch 953 / 999  train Loss: 3.1537  val_loss: 0.8310  val_score: 0.8309  lr: 0.00080  elapsed: 91m 16s\n----------------------------------------------------------------------->> loss improved to 0.83086\nEpoch 954 / 999  train Loss: 3.1491  val_loss: 0.8798  val_score: 0.8800  lr: 0.00079  elapsed: 91m 21s\nEpoch 955 / 999  train Loss: 3.1557  val_loss: 0.8800  val_score: 0.8800  lr: 0.00079  elapsed: 91m 27s\nEpoch 956 / 999  train Loss: 3.1574  val_loss: 0.8947  val_score: 0.8948  lr: 0.00079  elapsed: 91m 33s\nEpoch 957 / 999  train Loss: 3.1532  val_loss: 0.8938  val_score: 0.8939  lr: 0.00079  elapsed: 91m 39s\nEpoch 958 / 999  train Loss: 3.1492  val_loss: 0.8931  val_score: 0.8935  lr: 0.00079  elapsed: 91m 44s\nEpoch 959 / 999  train Loss: 3.1537  val_loss: 0.8817  val_score: 0.8816  lr: 0.00079  elapsed: 91m 50s\nEpoch 960 / 999  train Loss: 3.1499  val_loss: 0.8861  val_score: 0.8861  lr: 0.00079  elapsed: 91m 56s\nEpoch 961 / 999  train Loss: 3.1482  val_loss: 0.9269  val_score: 0.9267  lr: 0.00079  elapsed: 92m 2s\nEpoch 962 / 999  train Loss: 3.1507  val_loss: 0.9077  val_score: 0.9077  lr: 0.00080  elapsed: 92m 7s\nEpoch 963 / 999  train Loss: 3.1543  val_loss: 0.9287  val_score: 0.9285  lr: 0.00079  elapsed: 92m 13s\nEpoch 964 / 999  train Loss: 3.1500  val_loss: 0.8441  val_score: 0.8441  lr: 0.00080  elapsed: 92m 19s\nEpoch 965 / 999  train Loss: 3.1496  val_loss: 0.8586  val_score: 0.8588  lr: 0.00079  elapsed: 92m 25s\nEpoch 966 / 999  train Loss: 3.1492  val_loss: 0.9144  val_score: 0.9146  lr: 0.00079  elapsed: 92m 30s\nEpoch 967 / 999  train Loss: 3.1514  val_loss: 0.8690  val_score: 0.8690  lr: 0.00080  elapsed: 92m 36s\nEpoch 968 / 999  train Loss: 3.1433  val_loss: 0.9339  val_score: 0.9338  lr: 0.00079  elapsed: 92m 41s\nEpoch 969 / 999  train Loss: 3.1529  val_loss: 0.8941  val_score: 0.8940  lr: 0.00080  elapsed: 92m 47s\nEpoch 970 / 999  train Loss: 3.1514  val_loss: 0.9328  val_score: 0.9331  lr: 0.00079  elapsed: 92m 53s\nEpoch 971 / 999  train Loss: 3.1666  val_loss: 0.9515  val_score: 0.9510  lr: 0.00080  elapsed: 92m 58s\nEpoch 972 / 999  train Loss: 3.1529  val_loss: 0.9102  val_score: 0.9101  lr: 0.00080  elapsed: 93m 4s\nEpoch 973 / 999  train Loss: 3.1456  val_loss: 0.9143  val_score: 0.9140  lr: 0.00080  elapsed: 93m 10s\nEpoch 974 / 999  train Loss: 3.1497  val_loss: 0.9112  val_score: 0.9111  lr: 0.00080  elapsed: 93m 16s\nEpoch 975 / 999  train Loss: 3.1476  val_loss: 0.8668  val_score: 0.8668  lr: 0.00080  elapsed: 93m 21s\nEpoch 976 / 999  train Loss: 3.1528  val_loss: 0.8947  val_score: 0.8947  lr: 0.00079  elapsed: 93m 27s\nEpoch 977 / 999  train Loss: 3.1590  val_loss: 0.8407  val_score: 0.8405  lr: 0.00079  elapsed: 93m 33s\nEpoch 978 / 999  train Loss: 3.1385  val_loss: 0.8515  val_score: 0.8515  lr: 0.00079  elapsed: 93m 38s\nEpoch 979 / 999  train Loss: 3.1463  val_loss: 0.8672  val_score: 0.8674  lr: 0.00079  elapsed: 93m 44s\nEpoch 980 / 999  train Loss: 3.1470  val_loss: 0.8894  val_score: 0.8894  lr: 0.00079  elapsed: 93m 50s\nEpoch 981 / 999  train Loss: 3.1430  val_loss: 0.8910  val_score: 0.8910  lr: 0.00079  elapsed: 93m 56s\nEpoch 982 / 999  train Loss: 3.1494  val_loss: 0.9726  val_score: 0.9725  lr: 0.00079  elapsed: 94m 2s\nEpoch 983 / 999  train Loss: 3.1486  val_loss: 0.8757  val_score: 0.8754  lr: 0.00080  elapsed: 94m 7s\nEpoch 984 / 999  train Loss: 3.1392  val_loss: 0.8930  val_score: 0.8927  lr: 0.00079  elapsed: 94m 13s\nEpoch 985 / 999  train Loss: 3.1461  val_loss: 0.8675  val_score: 0.8674  lr: 0.00079  elapsed: 94m 19s\nEpoch 986 / 999  train Loss: 3.1435  val_loss: 0.8708  val_score: 0.8708  lr: 0.00079  elapsed: 94m 24s\nEpoch 987 / 999  train Loss: 3.1482  val_loss: 0.8655  val_score: 0.8654  lr: 0.00079  elapsed: 94m 30s\nEpoch 988 / 999  train Loss: 3.1477  val_loss: 0.8652  val_score: 0.8652  lr: 0.00079  elapsed: 94m 36s\nEpoch 989 / 999  train Loss: 3.1456  val_loss: 0.9485  val_score: 0.9483  lr: 0.00079  elapsed: 94m 42s\nEpoch 990 / 999  train Loss: 3.1447  val_loss: 0.8545  val_score: 0.8543  lr: 0.00080  elapsed: 94m 48s\nEpoch 991 / 999  train Loss: 3.1452  val_loss: 0.8897  val_score: 0.8896  lr: 0.00079  elapsed: 94m 54s\nEpoch 992 / 999  train Loss: 3.1416  val_loss: 0.9132  val_score: 0.9130  lr: 0.00079  elapsed: 94m 59s\nEpoch 993 / 999  train Loss: 3.1510  val_loss: 0.8772  val_score: 0.8771  lr: 0.00080  elapsed: 95m 5s\nEpoch 994 / 999  train Loss: 3.1561  val_loss: 0.8903  val_score: 0.8903  lr: 0.00079  elapsed: 95m 11s\nEpoch 995 / 999  train Loss: 3.1529  val_loss: 0.8923  val_score: 0.8920  lr: 0.00079  elapsed: 95m 16s\nEpoch 996 / 999  train Loss: 3.1401  val_loss: 0.8861  val_score: 0.8861  lr: 0.00079  elapsed: 95m 22s\nEpoch 997 / 999  train Loss: 3.1475  val_loss: 0.8669  val_score: 0.8667  lr: 0.00079  elapsed: 95m 28s\nEpoch 998 / 999  train Loss: 3.1443  val_loss: 0.8754  val_score: 0.8751  lr: 0.00079  elapsed: 95m 34s\nEpoch 999 / 999  train Loss: 3.1471  val_loss: 0.8703  val_score: 0.8700  lr: 0.00079  elapsed: 95m 39s\n==================== mlp - Best val_loss - 0.83086 =================\n"
    }
   ],
   "source": [
    "# output path\n",
    "#output_dir = Path('./', 'output')\n",
    "#output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "num_epochs = 1000\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "best_epoch_list = []\n",
    "best_valid_score_list = []\n",
    "\n",
    "# build model\n",
    "model = build_model(device, model_name='mlp')\n",
    "model.to(device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = AdamW(model.parameters(), lr)\n",
    "\n",
    "eta_min = 0.000001\n",
    "T_max = 10\n",
    "T_mult = 1\n",
    "restart_decay = 0.97\n",
    "scheduler = CosineAnnealingWithRestartsLR(optimizer, T_max=T_max, eta_min=eta_min, T_mult=T_mult, restart_decay=restart_decay)\n",
    "\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "best_epoch = 0\n",
    "best_train_loss = 1000\n",
    "best_valid_score = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "        else:\n",
    "            target = target.float()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    val_loss, val_score = validation(model, criterion, valid_loader, device)\n",
    "\n",
    "    elapsed = time() - start_time\n",
    "\n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "\n",
    "    scheduler.step(val_score)\n",
    "    \n",
    "    print('Epoch {} / {}  train Loss: {:.4f}  val_loss: {:.4f}  val_score: {:.4f}  lr: {:.5f}  elapsed: {:.0f}m {:.0f}s' \\\n",
    "          .format(epoch,  num_epochs - 1, train_loss, val_loss, val_score, lr[0], elapsed // 60, elapsed % 60))\n",
    "        \n",
    "    #model_path = output_dir / 'best_model.pt'\n",
    "    model_path = '../wafer/mlp_weights/mlp_v1_stats_1000e_cosine.pt'\n",
    "\n",
    "    if val_score < best_valid_score:\n",
    "        best_valid_score = val_score\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print('----------------------------------------------------------------------->> loss improved to {:.5f}'.format(best_valid_score))\n",
    "\n",
    "    best_epoch_list.append(best_epoch)\n",
    "    best_valid_score_list.append(best_valid_score)\n",
    "print(\"==================== mlp - Best val_loss - {:.5f} =================\".format(best_valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_df = pd.DataFrame()\n",
    "epoch_df['epoch'] = best_epoch_list\n",
    "epoch_df['val_score'] = best_valid_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>val_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>999</th>\n      <td>953</td>\n      <td>0.830861</td>\n    </tr>\n    <tr>\n      <th>972</th>\n      <td>953</td>\n      <td>0.830861</td>\n    </tr>\n    <tr>\n      <th>971</th>\n      <td>953</td>\n      <td>0.830861</td>\n    </tr>\n    <tr>\n      <th>970</th>\n      <td>953</td>\n      <td>0.830861</td>\n    </tr>\n    <tr>\n      <th>969</th>\n      <td>953</td>\n      <td>0.830861</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     epoch  val_score\n999    953   0.830861\n972    953   0.830861\n971    953   0.830861\n970    953   0.830861\n969    953   0.830861"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_df.sort_values('val_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.830861"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_to = round(min(best_valid_score_list),6)\n",
    "score_to"
   ]
  },
  {
   "source": [
    "%%time\n",
    "batch_size = 2048\n",
    "test_loader = build_dataloader(test_df.iloc[:, 1:].values, Y=None, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = build_model(device, model_name='mlp')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_preds = np.zeros((len(test_loader.dataset), 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "        outputs = model(data)\n",
    "        test_preds[batch_idx * batch_size:(batch_idx+1) * batch_size] = outputs.detach().cpu().numpy()"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "no label\nCPU times: user 86.2 ms, sys: 16 ms, total: 102 ms\nWall time: 103 ms\n"
    }
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "source": [
    "submission = pd.DataFrame({'id': submission_df['id'],\n",
    "                           'layer_1':test_preds.transpose()[0],\n",
    "                           'layer_2':test_preds.transpose()[1],\n",
    "                           'layer_3':test_preds.transpose()[2],\n",
    "                           'layer_4':test_preds.transpose()[3]})\n",
    "submission.to_csv('../wafer/mlp_submission/mlp_v1_stats_1000e_cosine_{}_submission.csv'.format(score_to), index=False)\n",
    "\n",
    "submission.head()"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id     layer_1     layer_2     layer_3     layer_4\n0   0  253.882034  230.181641  130.377167   89.371307\n1   1  158.991135  127.112167  235.546143   99.952805\n2   2  148.341812  176.705551  273.988434  155.042053\n3   3   91.433372  230.720779  187.542053   82.531372\n4   4  275.130920  295.080292  243.270081  272.185272",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>layer_1</th>\n      <th>layer_2</th>\n      <th>layer_3</th>\n      <th>layer_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>253.882034</td>\n      <td>230.181641</td>\n      <td>130.377167</td>\n      <td>89.371307</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>158.991135</td>\n      <td>127.112167</td>\n      <td>235.546143</td>\n      <td>99.952805</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>148.341812</td>\n      <td>176.705551</td>\n      <td>273.988434</td>\n      <td>155.042053</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>91.433372</td>\n      <td>230.720779</td>\n      <td>187.542053</td>\n      <td>82.531372</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>275.130920</td>\n      <td>295.080292</td>\n      <td>243.270081</td>\n      <td>272.185272</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {},
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}