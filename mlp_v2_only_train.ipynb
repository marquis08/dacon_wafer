{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils import CosineAnnealingWithRestartsLR\n",
    "\n",
    "from torch.optim import Adam, SGD, Optimizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Semi_dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.X_dataset = []\n",
    "        self.Y_dataset = []\n",
    "        for x in X:\n",
    "            self.X_dataset.append(torch.FloatTensor(x))\n",
    "        try:\n",
    "            for y in Y.values:\n",
    "                self.Y_dataset.append(torch.FloatTensor(y))\n",
    "        except:\n",
    "            print(\"no label\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X_dataset[index]\n",
    "        try:\n",
    "            target = self.Y_dataset[index]\n",
    "            return data, target\n",
    "        except:\n",
    "            return data\n",
    "\n",
    "\n",
    "def build_dataloader(X, Y, batch_size, shuffle=False):\n",
    "    \n",
    "    dataset = Semi_dataset(X, Y)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=8\n",
    "                            )\n",
    "    return dataloader\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred,\n",
    "                        sample_weight=None,\n",
    "                        multioutput='uniform_average'):\n",
    "    \n",
    "    output_errors = np.average(np.abs(y_pred - y_true),\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class MLP_only_flatfeatures(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(MLP_only_flatfeatures, self).__init__()\n",
    "        self.num_classes = num_classes         \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            #nn.Linear(226, 1000),\n",
    "            nn.Linear(226, 512),\n",
    "            \n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            ####### Block 1 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            ####### Block 2 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            ####### Block 3 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            #######################\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, self.num_classes)\n",
    "            )             \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_layers(x)\n",
    "        return out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def build_model(device, model_name='mlp', weight_path=None):\n",
    "\n",
    "    if model_name == 'mlp':\n",
    "        model = MLP_only_flatfeatures(4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def validation(model, criterion, valid_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), 4))\n",
    "    valid_targets = np.zeros((len(valid_loader.dataset), 4))\n",
    "    val_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(valid_loader):\n",
    "            \n",
    "            valid_targets[i * batch_size: (i+1) * batch_size] = target.float().numpy().copy()\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "                \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_preds[i * batch_size: (i+1) * batch_size] = output.detach().cpu().numpy()\n",
    "            \n",
    "            val_loss += loss.item() / len(valid_loader)\n",
    "    \n",
    "    val_score = mean_absolute_error(valid_preds, valid_targets)\n",
    "\n",
    "    return val_loss, val_score\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "if cuda.is_available:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 34.1 s, sys: 5.25 s, total: 39.3 s\nWall time: 22.9 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "DATASET_PATH = '../wafer'\n",
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(DATASET_PATH, 'sample_submission.csv'))\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(train_df.iloc[:, 4:], train_df.iloc[:, :4], test_size=0.1, random_state=42, shuffle=True)\n",
    "X_train, y_train = train_df.iloc[:, 4:], train_df.iloc[:, :4]\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "#X_val = X_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "#y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "#X_val = scaler.transform(X_val)\n",
    "\n",
    "batch_size = 2048\n",
    "train_loader = build_dataloader(X_train, y_train, batch_size, shuffle=True)\n",
    "#valid_loader = build_dataloader(X_val, y_val, batch_size, shuffle=False)\n",
    "\n",
    "test_df.iloc[:, 1:] = scaler.transform(test_df.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "lapsed: 146m 45s\nEpoch 1756 / 1999  train Loss: 2.9903  lr: 0.00100  elapsed: 146m 50s\nEpoch 1757 / 1999  train Loss: 2.9947  lr: 0.00100  elapsed: 146m 55s\nEpoch 1758 / 1999  train Loss: 2.9947  lr: 0.00100  elapsed: 147m 0s\nEpoch 1759 / 1999  train Loss: 3.0003  lr: 0.00100  elapsed: 147m 5s\nEpoch 1760 / 1999  train Loss: 2.9943  lr: 0.00100  elapsed: 147m 10s\nEpoch 1761 / 1999  train Loss: 2.9958  lr: 0.00100  elapsed: 147m 15s\nEpoch 1762 / 1999  train Loss: 2.9959  lr: 0.00100  elapsed: 147m 20s\nEpoch 1763 / 1999  train Loss: 2.9959  lr: 0.00100  elapsed: 147m 25s\nEpoch 1764 / 1999  train Loss: 2.9913  lr: 0.00100  elapsed: 147m 30s\nEpoch 1765 / 1999  train Loss: 2.9910  lr: 0.00100  elapsed: 147m 35s\nEpoch 1766 / 1999  train Loss: 2.9956  lr: 0.00100  elapsed: 147m 40s\nEpoch 1767 / 1999  train Loss: 2.9973  lr: 0.00100  elapsed: 147m 45s\nEpoch 1768 / 1999  train Loss: 2.9930  lr: 0.00100  elapsed: 147m 50s\nEpoch 1769 / 1999  train Loss: 2.9920  lr: 0.00100  elapsed: 147m 55s\nEpoch 1770 / 1999  train Loss: 2.9909  lr: 0.00100  elapsed: 148m 0s\nEpoch 1771 / 1999  train Loss: 2.9943  lr: 0.00100  elapsed: 148m 5s\nEpoch 1772 / 1999  train Loss: 2.9959  lr: 0.00100  elapsed: 148m 11s\nEpoch 1773 / 1999  train Loss: 2.9979  lr: 0.00100  elapsed: 148m 16s\nEpoch 1774 / 1999  train Loss: 2.9931  lr: 0.00100  elapsed: 148m 21s\nEpoch 1775 / 1999  train Loss: 2.9900  lr: 0.00100  elapsed: 148m 26s\n----------------------------------------------------------------------->> loss improved to 2.98831\nEpoch 1776 / 1999  train Loss: 2.9883  lr: 0.00100  elapsed: 148m 31s\nEpoch 1777 / 1999  train Loss: 2.9913  lr: 0.00100  elapsed: 148m 36s\nEpoch 1778 / 1999  train Loss: 2.9956  lr: 0.00100  elapsed: 148m 41s\n----------------------------------------------------------------------->> loss improved to 2.98766\nEpoch 1779 / 1999  train Loss: 2.9877  lr: 0.00100  elapsed: 148m 46s\nEpoch 1780 / 1999  train Loss: 2.9916  lr: 0.00100  elapsed: 148m 51s\nEpoch 1781 / 1999  train Loss: 2.9884  lr: 0.00100  elapsed: 148m 56s\nEpoch 1782 / 1999  train Loss: 2.9940  lr: 0.00100  elapsed: 149m 1s\nEpoch 1783 / 1999  train Loss: 2.9898  lr: 0.00100  elapsed: 149m 6s\nEpoch 1784 / 1999  train Loss: 2.9927  lr: 0.00100  elapsed: 149m 11s\nEpoch 1785 / 1999  train Loss: 2.9907  lr: 0.00100  elapsed: 149m 16s\nEpoch 1786 / 1999  train Loss: 2.9923  lr: 0.00100  elapsed: 149m 21s\nEpoch 1787 / 1999  train Loss: 2.9921  lr: 0.00100  elapsed: 149m 26s\n----------------------------------------------------------------------->> loss improved to 2.98701\nEpoch 1788 / 1999  train Loss: 2.9870  lr: 0.00100  elapsed: 149m 31s\nEpoch 1789 / 1999  train Loss: 2.9948  lr: 0.00100  elapsed: 149m 36s\nEpoch 1790 / 1999  train Loss: 2.9915  lr: 0.00100  elapsed: 149m 41s\nEpoch 1791 / 1999  train Loss: 2.9922  lr: 0.00100  elapsed: 149m 46s\nEpoch 1792 / 1999  train Loss: 2.9927  lr: 0.00100  elapsed: 149m 51s\nEpoch 1793 / 1999  train Loss: 2.9877  lr: 0.00100  elapsed: 149m 57s\n----------------------------------------------------------------------->> loss improved to 2.98664\nEpoch 1794 / 1999  train Loss: 2.9866  lr: 0.00100  elapsed: 150m 2s\nEpoch 1795 / 1999  train Loss: 2.9887  lr: 0.00100  elapsed: 150m 7s\nEpoch 1796 / 1999  train Loss: 2.9910  lr: 0.00100  elapsed: 150m 12s\nEpoch 1797 / 1999  train Loss: 2.9887  lr: 0.00100  elapsed: 150m 17s\nEpoch 1798 / 1999  train Loss: 2.9896  lr: 0.00100  elapsed: 150m 22s\nEpoch 1799 / 1999  train Loss: 2.9871  lr: 0.00100  elapsed: 150m 27s\nEpoch 1800 / 1999  train Loss: 2.9965  lr: 0.00100  elapsed: 150m 32s\n----------------------------------------------------------------------->> loss improved to 2.98603\nEpoch 1801 / 1999  train Loss: 2.9860  lr: 0.00100  elapsed: 150m 37s\nEpoch 1802 / 1999  train Loss: 2.9879  lr: 0.00100  elapsed: 150m 42s\nEpoch 1803 / 1999  train Loss: 2.9896  lr: 0.00100  elapsed: 150m 47s\nEpoch 1804 / 1999  train Loss: 2.9919  lr: 0.00100  elapsed: 150m 52s\nEpoch 1805 / 1999  train Loss: 2.9868  lr: 0.00100  elapsed: 150m 57s\nEpoch 1806 / 1999  train Loss: 2.9893  lr: 0.00100  elapsed: 151m 2s\nEpoch 1807 / 1999  train Loss: 2.9889  lr: 0.00100  elapsed: 151m 7s\nEpoch 1808 / 1999  train Loss: 2.9906  lr: 0.00100  elapsed: 151m 12s\nEpoch 1809 / 1999  train Loss: 2.9928  lr: 0.00100  elapsed: 151m 17s\nEpoch 1810 / 1999  train Loss: 2.9906  lr: 0.00100  elapsed: 151m 23s\nEpoch 1811 / 1999  train Loss: 2.9914  lr: 0.00100  elapsed: 151m 28s\nEpoch 1812 / 1999  train Loss: 2.9953  lr: 0.00100  elapsed: 151m 33s\nEpoch 1813 / 1999  train Loss: 2.9914  lr: 0.00100  elapsed: 151m 37s\nEpoch 1814 / 1999  train Loss: 2.9888  lr: 0.00100  elapsed: 151m 43s\nEpoch 1815 / 1999  train Loss: 2.9877  lr: 0.00100  elapsed: 151m 47s\nEpoch 1816 / 1999  train Loss: 2.9883  lr: 0.00100  elapsed: 151m 52s\nEpoch 1817 / 1999  train Loss: 2.9915  lr: 0.00100  elapsed: 151m 57s\nEpoch 1818 / 1999  train Loss: 2.9917  lr: 0.00100  elapsed: 152m 2s\nEpoch 1819 / 1999  train Loss: 2.9907  lr: 0.00100  elapsed: 152m 7s\nEpoch 1820 / 1999  train Loss: 2.9905  lr: 0.00100  elapsed: 152m 12s\nEpoch 1821 / 1999  train Loss: 2.9880  lr: 0.00100  elapsed: 152m 18s\nEpoch 1822 / 1999  train Loss: 2.9898  lr: 0.00100  elapsed: 152m 23s\nEpoch 1823 / 1999  train Loss: 2.9891  lr: 0.00100  elapsed: 152m 28s\nEpoch 1824 / 1999  train Loss: 2.9900  lr: 0.00100  elapsed: 152m 33s\nEpoch 1825 / 1999  train Loss: 2.9894  lr: 0.00100  elapsed: 152m 38s\nEpoch 1826 / 1999  train Loss: 2.9937  lr: 0.00100  elapsed: 152m 43s\nEpoch 1827 / 1999  train Loss: 2.9872  lr: 0.00100  elapsed: 152m 48s\nEpoch 1828 / 1999  train Loss: 2.9899  lr: 0.00100  elapsed: 152m 53s\n----------------------------------------------------------------------->> loss improved to 2.98592\nEpoch 1829 / 1999  train Loss: 2.9859  lr: 0.00100  elapsed: 152m 58s\nEpoch 1830 / 1999  train Loss: 2.9892  lr: 0.00100  elapsed: 153m 3s\nEpoch 1831 / 1999  train Loss: 2.9905  lr: 0.00100  elapsed: 153m 8s\nEpoch 1832 / 1999  train Loss: 2.9884  lr: 0.00100  elapsed: 153m 13s\n----------------------------------------------------------------------->> loss improved to 2.98433\nEpoch 1833 / 1999  train Loss: 2.9843  lr: 0.00100  elapsed: 153m 18s\nEpoch 1834 / 1999  train Loss: 2.9913  lr: 0.00100  elapsed: 153m 23s\nEpoch 1835 / 1999  train Loss: 2.9857  lr: 0.00100  elapsed: 153m 28s\nEpoch 1836 / 1999  train Loss: 2.9847  lr: 0.00100  elapsed: 153m 33s\nEpoch 1837 / 1999  train Loss: 2.9866  lr: 0.00100  elapsed: 153m 38s\n----------------------------------------------------------------------->> loss improved to 2.98430\nEpoch 1838 / 1999  train Loss: 2.9843  lr: 0.00100  elapsed: 153m 43s\nEpoch 1839 / 1999  train Loss: 2.9906  lr: 0.00100  elapsed: 153m 48s\nEpoch 1840 / 1999  train Loss: 2.9887  lr: 0.00100  elapsed: 153m 53s\nEpoch 1841 / 1999  train Loss: 2.9865  lr: 0.00100  elapsed: 153m 58s\nEpoch 1842 / 1999  train Loss: 2.9867  lr: 0.00100  elapsed: 154m 3s\n----------------------------------------------------------------------->> loss improved to 2.98274\nEpoch 1843 / 1999  train Loss: 2.9827  lr: 0.00100  elapsed: 154m 9s\nEpoch 1844 / 1999  train Loss: 2.9843  lr: 0.00100  elapsed: 154m 14s\nEpoch 1845 / 1999  train Loss: 2.9864  lr: 0.00100  elapsed: 154m 19s\nEpoch 1846 / 1999  train Loss: 2.9898  lr: 0.00100  elapsed: 154m 24s\nEpoch 1847 / 1999  train Loss: 2.9830  lr: 0.00100  elapsed: 154m 29s\nEpoch 1848 / 1999  train Loss: 2.9920  lr: 0.00100  elapsed: 154m 34s\nEpoch 1849 / 1999  train Loss: 2.9882  lr: 0.00100  elapsed: 154m 39s\nEpoch 1850 / 1999  train Loss: 2.9858  lr: 0.00100  elapsed: 154m 44s\nEpoch 1851 / 1999  train Loss: 2.9869  lr: 0.00100  elapsed: 154m 49s\nEpoch 1852 / 1999  train Loss: 2.9869  lr: 0.00100  elapsed: 154m 54s\nEpoch 1853 / 1999  train Loss: 2.9884  lr: 0.00100  elapsed: 154m 59s\nEpoch 1854 / 1999  train Loss: 2.9881  lr: 0.00100  elapsed: 155m 4s\n----------------------------------------------------------------------->> loss improved to 2.97913\nEpoch 1855 / 1999  train Loss: 2.9791  lr: 0.00100  elapsed: 155m 9s\nEpoch 1856 / 1999  train Loss: 2.9803  lr: 0.00100  elapsed: 155m 14s\nEpoch 1857 / 1999  train Loss: 3.0028  lr: 0.00100  elapsed: 155m 19s\nEpoch 1858 / 1999  train Loss: 2.9819  lr: 0.00100  elapsed: 155m 24s\nEpoch 1859 / 1999  train Loss: 2.9824  lr: 0.00100  elapsed: 155m 29s\nEpoch 1860 / 1999  train Loss: 2.9850  lr: 0.00100  elapsed: 155m 34s\nEpoch 1861 / 1999  train Loss: 2.9856  lr: 0.00100  elapsed: 155m 39s\nEpoch 1862 / 1999  train Loss: 2.9865  lr: 0.00100  elapsed: 155m 44s\nEpoch 1863 / 1999  train Loss: 2.9809  lr: 0.00100  elapsed: 155m 49s\nEpoch 1864 / 1999  train Loss: 2.9814  lr: 0.00100  elapsed: 155m 54s\nEpoch 1865 / 1999  train Loss: 2.9812  lr: 0.00100  elapsed: 155m 59s\nEpoch 1866 / 1999  train Loss: 2.9804  lr: 0.00100  elapsed: 156m 4s\nEpoch 1867 / 1999  train Loss: 2.9839  lr: 0.00100  elapsed: 156m 9s\n----------------------------------------------------------------------->> loss improved to 2.97741\nEpoch 1868 / 1999  train Loss: 2.9774  lr: 0.00100  elapsed: 156m 14s\nEpoch 1869 / 1999  train Loss: 2.9809  lr: 0.00100  elapsed: 156m 20s\nEpoch 1870 / 1999  train Loss: 2.9795  lr: 0.00100  elapsed: 156m 25s\nEpoch 1871 / 1999  train Loss: 2.9833  lr: 0.00100  elapsed: 156m 30s\nEpoch 1872 / 1999  train Loss: 2.9874  lr: 0.00100  elapsed: 156m 35s\nEpoch 1873 / 1999  train Loss: 2.9800  lr: 0.00100  elapsed: 156m 40s\nEpoch 1874 / 1999  train Loss: 2.9802  lr: 0.00100  elapsed: 156m 45s\n----------------------------------------------------------------------->> loss improved to 2.97665\nEpoch 1875 / 1999  train Loss: 2.9766  lr: 0.00100  elapsed: 156m 50s\nEpoch 1876 / 1999  train Loss: 2.9777  lr: 0.00100  elapsed: 156m 55s\nEpoch 1877 / 1999  train Loss: 2.9793  lr: 0.00100  elapsed: 156m 60s\n----------------------------------------------------------------------->> loss improved to 2.97650\nEpoch 1878 / 1999  train Loss: 2.9765  lr: 0.00100  elapsed: 157m 5s\nEpoch 1879 / 1999  train Loss: 2.9813  lr: 0.00100  elapsed: 157m 10s\n----------------------------------------------------------------------->> loss improved to 2.97605\nEpoch 1880 / 1999  train Loss: 2.9761  lr: 0.00100  elapsed: 157m 15s\n----------------------------------------------------------------------->> loss improved to 2.97562\nEpoch 1881 / 1999  train Loss: 2.9756  lr: 0.00100  elapsed: 157m 20s\nEpoch 1882 / 1999  train Loss: 2.9780  lr: 0.00100  elapsed: 157m 25s\nEpoch 1883 / 1999  train Loss: 2.9827  lr: 0.00100  elapsed: 157m 30s\n----------------------------------------------------------------------->> loss improved to 2.97330\nEpoch 1884 / 1999  train Loss: 2.9733  lr: 0.00100  elapsed: 157m 35s\nEpoch 1885 / 1999  train Loss: 2.9771  lr: 0.00100  elapsed: 157m 40s\nEpoch 1886 / 1999  train Loss: 2.9739  lr: 0.00100  elapsed: 157m 45s\nEpoch 1887 / 1999  train Loss: 2.9760  lr: 0.00100  elapsed: 157m 50s\nEpoch 1888 / 1999  train Loss: 2.9774  lr: 0.00100  elapsed: 157m 55s\n----------------------------------------------------------------------->> loss improved to 2.97166\nEpoch 1889 / 1999  train Loss: 2.9717  lr: 0.00100  elapsed: 158m 0s\nEpoch 1890 / 1999  train Loss: 2.9729  lr: 0.00100  elapsed: 158m 5s\nEpoch 1891 / 1999  train Loss: 2.9742  lr: 0.00100  elapsed: 158m 10s\nEpoch 1892 / 1999  train Loss: 2.9727  lr: 0.00100  elapsed: 158m 16s\n----------------------------------------------------------------------->> loss improved to 2.96829\nEpoch 1893 / 1999  train Loss: 2.9683  lr: 0.00100  elapsed: 158m 21s\nEpoch 1894 / 1999  train Loss: 2.9724  lr: 0.00100  elapsed: 158m 26s\nEpoch 1895 / 1999  train Loss: 2.9700  lr: 0.00100  elapsed: 158m 30s\nEpoch 1896 / 1999  train Loss: 2.9719  lr: 0.00100  elapsed: 158m 36s\nEpoch 1897 / 1999  train Loss: 2.9751  lr: 0.00100  elapsed: 158m 41s\nEpoch 1898 / 1999  train Loss: 2.9683  lr: 0.00100  elapsed: 158m 46s\nEpoch 1899 / 1999  train Loss: 2.9689  lr: 0.00100  elapsed: 158m 51s\nEpoch 1900 / 1999  train Loss: 2.9813  lr: 0.00100  elapsed: 158m 56s\nEpoch 1901 / 1999  train Loss: 2.9711  lr: 0.00100  elapsed: 159m 1s\nEpoch 1902 / 1999  train Loss: 2.9705  lr: 0.00100  elapsed: 159m 6s\nEpoch 1903 / 1999  train Loss: 2.9706  lr: 0.00100  elapsed: 159m 11s\nEpoch 1904 / 1999  train Loss: 2.9706  lr: 0.00100  elapsed: 159m 16s\n----------------------------------------------------------------------->> loss improved to 2.96754\nEpoch 1905 / 1999  train Loss: 2.9675  lr: 0.00100  elapsed: 159m 21s\nEpoch 1906 / 1999  train Loss: 2.9752  lr: 0.00100  elapsed: 159m 26s\n----------------------------------------------------------------------->> loss improved to 2.96564\nEpoch 1907 / 1999  train Loss: 2.9656  lr: 0.00100  elapsed: 159m 31s\n----------------------------------------------------------------------->> loss improved to 2.96558\nEpoch 1908 / 1999  train Loss: 2.9656  lr: 0.00100  elapsed: 159m 36s\nEpoch 1909 / 1999  train Loss: 2.9663  lr: 0.00100  elapsed: 159m 41s\nEpoch 1910 / 1999  train Loss: 2.9663  lr: 0.00100  elapsed: 159m 46s\nEpoch 1911 / 1999  train Loss: 2.9734  lr: 0.00100  elapsed: 159m 51s\nEpoch 1912 / 1999  train Loss: 2.9702  lr: 0.00100  elapsed: 159m 56s\n----------------------------------------------------------------------->> loss improved to 2.96534\nEpoch 1913 / 1999  train Loss: 2.9653  lr: 0.00100  elapsed: 160m 1s\nEpoch 1914 / 1999  train Loss: 2.9690  lr: 0.00100  elapsed: 160m 6s\nEpoch 1915 / 1999  train Loss: 2.9665  lr: 0.00100  elapsed: 160m 11s\nEpoch 1916 / 1999  train Loss: 2.9675  lr: 0.00100  elapsed: 160m 16s\nEpoch 1917 / 1999  train Loss: 2.9662  lr: 0.00100  elapsed: 160m 21s\n----------------------------------------------------------------------->> loss improved to 2.96323\nEpoch 1918 / 1999  train Loss: 2.9632  lr: 0.00100  elapsed: 160m 26s\nEpoch 1919 / 1999  train Loss: 2.9642  lr: 0.00100  elapsed: 160m 31s\nEpoch 1920 / 1999  train Loss: 2.9654  lr: 0.00100  elapsed: 160m 36s\nEpoch 1921 / 1999  train Loss: 2.9653  lr: 0.00100  elapsed: 160m 41s\nEpoch 1922 / 1999  train Loss: 2.9677  lr: 0.00100  elapsed: 160m 46s\nEpoch 1923 / 1999  train Loss: 2.9655  lr: 0.00100  elapsed: 160m 51s\nEpoch 1924 / 1999  train Loss: 2.9687  lr: 0.00100  elapsed: 160m 56s\nEpoch 1925 / 1999  train Loss: 2.9712  lr: 0.00100  elapsed: 161m 1s\nEpoch 1926 / 1999  train Loss: 2.9677  lr: 0.00100  elapsed: 161m 6s\nEpoch 1927 / 1999  train Loss: 2.9649  lr: 0.00100  elapsed: 161m 11s\nEpoch 1928 / 1999  train Loss: 2.9659  lr: 0.00100  elapsed: 161m 17s\n----------------------------------------------------------------------->> loss improved to 2.96123\nEpoch 1929 / 1999  train Loss: 2.9612  lr: 0.00100  elapsed: 161m 22s\nEpoch 1930 / 1999  train Loss: 2.9622  lr: 0.00100  elapsed: 161m 27s\nEpoch 1931 / 1999  train Loss: 2.9644  lr: 0.00100  elapsed: 161m 32s\nEpoch 1932 / 1999  train Loss: 2.9635  lr: 0.00100  elapsed: 161m 37s\nEpoch 1933 / 1999  train Loss: 2.9627  lr: 0.00100  elapsed: 161m 42s\nEpoch 1934 / 1999  train Loss: 2.9647  lr: 0.00100  elapsed: 161m 47s\nEpoch 1935 / 1999  train Loss: 2.9623  lr: 0.00100  elapsed: 161m 52s\nEpoch 1936 / 1999  train Loss: 2.9633  lr: 0.00100  elapsed: 161m 57s\nEpoch 1937 / 1999  train Loss: 2.9630  lr: 0.00100  elapsed: 162m 3s\nEpoch 1938 / 1999  train Loss: 2.9622  lr: 0.00100  elapsed: 162m 8s\nEpoch 1939 / 1999  train Loss: 2.9618  lr: 0.00100  elapsed: 162m 13s\nEpoch 1940 / 1999  train Loss: 2.9656  lr: 0.00100  elapsed: 162m 18s\n----------------------------------------------------------------------->> loss improved to 2.95888\nEpoch 1941 / 1999  train Loss: 2.9589  lr: 0.00100  elapsed: 162m 23s\nEpoch 1942 / 1999  train Loss: 2.9644  lr: 0.00100  elapsed: 162m 28s\nEpoch 1943 / 1999  train Loss: 2.9616  lr: 0.00100  elapsed: 162m 33s\nEpoch 1944 / 1999  train Loss: 2.9600  lr: 0.00100  elapsed: 162m 38s\nEpoch 1945 / 1999  train Loss: 2.9601  lr: 0.00100  elapsed: 162m 43s\nEpoch 1946 / 1999  train Loss: 2.9599  lr: 0.00100  elapsed: 162m 48s\nEpoch 1947 / 1999  train Loss: 2.9609  lr: 0.00100  elapsed: 162m 53s\nEpoch 1948 / 1999  train Loss: 2.9606  lr: 0.00100  elapsed: 162m 58s\nEpoch 1949 / 1999  train Loss: 2.9653  lr: 0.00100  elapsed: 163m 3s\nEpoch 1950 / 1999  train Loss: 2.9629  lr: 0.00100  elapsed: 163m 8s\nEpoch 1951 / 1999  train Loss: 2.9640  lr: 0.00100  elapsed: 163m 13s\nEpoch 1952 / 1999  train Loss: 2.9605  lr: 0.00100  elapsed: 163m 18s\nEpoch 1953 / 1999  train Loss: 2.9634  lr: 0.00100  elapsed: 163m 23s\n----------------------------------------------------------------------->> loss improved to 2.95630\nEpoch 1954 / 1999  train Loss: 2.9563  lr: 0.00100  elapsed: 163m 28s\nEpoch 1955 / 1999  train Loss: 2.9592  lr: 0.00100  elapsed: 163m 33s\nEpoch 1956 / 1999  train Loss: 2.9607  lr: 0.00100  elapsed: 163m 38s\nEpoch 1957 / 1999  train Loss: 2.9627  lr: 0.00100  elapsed: 163m 43s\nEpoch 1958 / 1999  train Loss: 2.9632  lr: 0.00100  elapsed: 163m 48s\nEpoch 1959 / 1999  train Loss: 2.9586  lr: 0.00100  elapsed: 163m 53s\nEpoch 1960 / 1999  train Loss: 2.9569  lr: 0.00100  elapsed: 163m 59s\nEpoch 1961 / 1999  train Loss: 2.9642  lr: 0.00100  elapsed: 164m 4s\nEpoch 1962 / 1999  train Loss: 2.9608  lr: 0.00100  elapsed: 164m 9s\nEpoch 1963 / 1999  train Loss: 2.9579  lr: 0.00100  elapsed: 164m 14s\nEpoch 1964 / 1999  train Loss: 2.9595  lr: 0.00100  elapsed: 164m 19s\nEpoch 1965 / 1999  train Loss: 2.9567  lr: 0.00100  elapsed: 164m 24s\nEpoch 1966 / 1999  train Loss: 2.9646  lr: 0.00100  elapsed: 164m 29s\nEpoch 1967 / 1999  train Loss: 2.9581  lr: 0.00100  elapsed: 164m 34s\n----------------------------------------------------------------------->> loss improved to 2.95226\nEpoch 1968 / 1999  train Loss: 2.9523  lr: 0.00100  elapsed: 164m 39s\n----------------------------------------------------------------------->> loss improved to 2.95216\nEpoch 1969 / 1999  train Loss: 2.9522  lr: 0.00100  elapsed: 164m 44s\nEpoch 1970 / 1999  train Loss: 2.9575  lr: 0.00100  elapsed: 164m 49s\nEpoch 1971 / 1999  train Loss: 2.9618  lr: 0.00100  elapsed: 164m 54s\nEpoch 1972 / 1999  train Loss: 2.9589  lr: 0.00100  elapsed: 164m 59s\nEpoch 1973 / 1999  train Loss: 2.9611  lr: 0.00100  elapsed: 165m 4s\nEpoch 1974 / 1999  train Loss: 2.9575  lr: 0.00100  elapsed: 165m 10s\nEpoch 1975 / 1999  train Loss: 2.9577  lr: 0.00100  elapsed: 165m 15s\nEpoch 1976 / 1999  train Loss: 2.9589  lr: 0.00100  elapsed: 165m 20s\nEpoch 1977 / 1999  train Loss: 2.9564  lr: 0.00100  elapsed: 165m 25s\nEpoch 1978 / 1999  train Loss: 2.9544  lr: 0.00100  elapsed: 165m 30s\nEpoch 1979 / 1999  train Loss: 2.9556  lr: 0.00100  elapsed: 165m 35s\nEpoch 1980 / 1999  train Loss: 2.9561  lr: 0.00100  elapsed: 165m 40s\nEpoch 1981 / 1999  train Loss: 2.9549  lr: 0.00100  elapsed: 165m 45s\nEpoch 1982 / 1999  train Loss: 2.9569  lr: 0.00100  elapsed: 165m 50s\nEpoch 1983 / 1999  train Loss: 2.9554  lr: 0.00100  elapsed: 165m 55s\nEpoch 1984 / 1999  train Loss: 2.9593  lr: 0.00100  elapsed: 165m 60s\nEpoch 1985 / 1999  train Loss: 2.9562  lr: 0.00100  elapsed: 166m 5s\nEpoch 1986 / 1999  train Loss: 2.9597  lr: 0.00100  elapsed: 166m 10s\nEpoch 1987 / 1999  train Loss: 2.9586  lr: 0.00100  elapsed: 166m 15s\nEpoch 1988 / 1999  train Loss: 2.9549  lr: 0.00100  elapsed: 166m 20s\n----------------------------------------------------------------------->> loss improved to 2.94930\nEpoch 1989 / 1999  train Loss: 2.9493  lr: 0.00100  elapsed: 166m 25s\nEpoch 1990 / 1999  train Loss: 2.9540  lr: 0.00100  elapsed: 166m 30s\nEpoch 1991 / 1999  train Loss: 2.9609  lr: 0.00100  elapsed: 166m 35s\nEpoch 1992 / 1999  train Loss: 2.9537  lr: 0.00100  elapsed: 166m 40s\nEpoch 1993 / 1999  train Loss: 2.9548  lr: 0.00100  elapsed: 166m 45s\nEpoch 1994 / 1999  train Loss: 2.9588  lr: 0.00100  elapsed: 166m 51s\nEpoch 1995 / 1999  train Loss: 2.9524  lr: 0.00100  elapsed: 166m 56s\nEpoch 1996 / 1999  train Loss: 2.9534  lr: 0.00100  elapsed: 167m 1s\nEpoch 1997 / 1999  train Loss: 2.9564  lr: 0.00100  elapsed: 167m 6s\nEpoch 1998 / 1999  train Loss: 2.9496  lr: 0.00100  elapsed: 167m 11s\nEpoch 1999 / 1999  train Loss: 2.9561  lr: 0.00100  elapsed: 167m 16s\n==================== mlp - Best train_loss - 2.94930 =================\n"
    }
   ],
   "source": [
    "# output path\n",
    "#output_dir = Path('./', 'output')\n",
    "#output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "num_epochs = 2000\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "best_epoch_list = []\n",
    "best_train_score_list = []\n",
    "\n",
    "# build model\n",
    "model = build_model(device, model_name='mlp')\n",
    "model.to(device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = AdamW(model.parameters(), lr)\n",
    "###################  Scheduler ################\n",
    "#eta_min = 0.000001\n",
    "#T_max = 10\n",
    "#T_mult = 1\n",
    "#restart_decay = 0.97\n",
    "#scheduler = CosineAnnealingWithRestartsLR(optimizer, T_max=T_max, eta_min=eta_min, T_mult=T_mult, restart_decay=restart_decay)\n",
    "###############################################\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "best_epoch = 0\n",
    "best_train_loss = 1000\n",
    "best_valid_score = 1000\n",
    "model_path = '../wafer/mlp_weights/mlp_v2_only_train.pt'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "        else:\n",
    "            target = target.float()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print('----------------------------------------------------------------------->> loss improved to {:.5f}'.format(best_train_loss))\n",
    "\n",
    "    #val_loss, val_score = validation(model, criterion, valid_loader, device)\n",
    "\n",
    "    elapsed = time() - start_time\n",
    "\n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "\n",
    "    #scheduler.step(val_score)\n",
    "    \n",
    "    print('Epoch {} / {}  train Loss: {:.4f}  lr: {:.5f}  elapsed: {:.0f}m {:.0f}s' \\\n",
    "          .format(epoch,  num_epochs - 1, train_loss, lr[0], elapsed // 60, elapsed % 60))\n",
    "        \n",
    "    #model_path = output_dir / 'best_model.pt'\n",
    "    \n",
    "\n",
    "    #if val_score < best_valid_score:\n",
    "    #    best_valid_score = val_score\n",
    "    #    best_epoch = epoch\n",
    "    #    torch.save(model.state_dict(), model_path)\n",
    "    #    print('----------------------------------------------------------------------->> loss improved to {:.5f}'.format(best_valid_score))\n",
    "\n",
    "    best_epoch_list.append(best_epoch)\n",
    "    best_train_score_list.append(best_train_loss)\n",
    "print(\"==================== mlp - Best train_loss - {:.5f} =================\".format(best_train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_df = pd.DataFrame()\n",
    "epoch_df['epoch'] = best_epoch_list\n",
    "epoch_df['train_loss'] = best_train_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>train_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1999</th>\n      <td>1989</td>\n      <td>2.949303</td>\n    </tr>\n    <tr>\n      <th>1989</th>\n      <td>1989</td>\n      <td>2.949303</td>\n    </tr>\n    <tr>\n      <th>1990</th>\n      <td>1989</td>\n      <td>2.949303</td>\n    </tr>\n    <tr>\n      <th>1991</th>\n      <td>1989</td>\n      <td>2.949303</td>\n    </tr>\n    <tr>\n      <th>1992</th>\n      <td>1989</td>\n      <td>2.949303</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      epoch  train_loss\n1999   1989    2.949303\n1989   1989    2.949303\n1990   1989    2.949303\n1991   1989    2.949303\n1992   1989    2.949303"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_df.sort_values('train_loss').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.949303"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_to = round(min(best_train_score_list),6)\n",
    "score_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "no label\nCPU times: user 82.7 ms, sys: 375 ms, total: 457 ms\nWall time: 524 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 2048\n",
    "test_loader = build_dataloader(test_df.iloc[:, 1:].values, Y=None, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = build_model(device, model_name='mlp')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_preds = np.zeros((len(test_loader.dataset), 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "        outputs = model(data)\n",
    "        test_preds[batch_idx * batch_size:(batch_idx+1) * batch_size] = outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>layer_1</th>\n      <th>layer_2</th>\n      <th>layer_3</th>\n      <th>layer_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>253.624512</td>\n      <td>227.385910</td>\n      <td>128.903107</td>\n      <td>89.995964</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>157.375778</td>\n      <td>128.144760</td>\n      <td>235.094910</td>\n      <td>100.054642</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>148.590576</td>\n      <td>177.123566</td>\n      <td>272.525665</td>\n      <td>160.315552</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>92.109222</td>\n      <td>228.369583</td>\n      <td>190.757385</td>\n      <td>80.320137</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>272.635254</td>\n      <td>296.319366</td>\n      <td>244.994904</td>\n      <td>270.283142</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   id     layer_1     layer_2     layer_3     layer_4\n0   0  253.624512  227.385910  128.903107   89.995964\n1   1  157.375778  128.144760  235.094910  100.054642\n2   2  148.590576  177.123566  272.525665  160.315552\n3   3   92.109222  228.369583  190.757385   80.320137\n4   4  272.635254  296.319366  244.994904  270.283142"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': submission_df['id'],\n",
    "                           'layer_1':test_preds.transpose()[0],\n",
    "                           'layer_2':test_preds.transpose()[1],\n",
    "                           'layer_3':test_preds.transpose()[2],\n",
    "                           'layer_4':test_preds.transpose()[3]})\n",
    "submission.to_csv('../wafer/mlp_submission/mlp_v2_only_train_2000e_{}_submission.csv'.format(score_to), index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}