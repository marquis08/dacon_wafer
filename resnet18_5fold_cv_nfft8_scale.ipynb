{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#from torch_fun.dataloader import build_dataloader\n",
    "#from torch_fun.model import build_model\n",
    "#from torch_fun.utils import count_parameters, seed_everything, AdamW, CosineAnnealingWithRestartsLR\n",
    "from torch import cuda\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def get_spectrogram_feature(data):\n",
    "    # data shape -> batch * dim\n",
    "\n",
    "    stft = torch.stft(torch.FloatTensor(data),\n",
    "                        8,\n",
    "                        center=False,\n",
    "                        normalized=False,\n",
    "                        onesided=True)\n",
    "\n",
    "    stft = (stft[:,:,0].pow(2) + stft[:,:,1].pow(2)).pow(0.5)\n",
    "    amag = stft.numpy()\n",
    "    feat = torch.FloatTensor(amag)\n",
    "    feat = torch.unsqueeze(feat,dim=0)\n",
    "#     feat = torch.FloatTensor(feat).transpose(1, -1)\n",
    "\n",
    "    return feat\n",
    "\n",
    "class Semi_dataset(Dataset):\n",
    "    def __init__(self, data_frame):\n",
    "        \n",
    "        self.data_list = list()\n",
    "        for data in data_frame[[str(x) for x in range(226)]].values:\n",
    "            self.data_list.append(get_spectrogram_feature(data))\n",
    "        data_frame.drop(columns=[str(x) for x in range(226)], inplace=True)\n",
    "        self.df = data_frame\n",
    "        try:\n",
    "            self.label = data_frame[['layer_' + str(x) for x in range(1, 5)]].values\n",
    "        except:\n",
    "            print('This dataframe does not have target value')\n",
    "            self.label = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         data = get_spectrogram_feature(self.df.iloc[index][[str(x) for x in range(226)]].values)\n",
    "        data = self.data_list[index]\n",
    "\n",
    "        if self.label is None:\n",
    "            return data\n",
    "        else:\n",
    "            target = torch.tensor(self.label[index, :])\n",
    "            return data, target\n",
    "\n",
    "\n",
    "def build_dataloader(data_frame, batch_size, shuffle):\n",
    "    dataset = Semi_dataset(data_frame)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=1\n",
    "                            )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, device, model_name='efficient', weight_path=None):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.weight_path = weight_path\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        ) \n",
    "                \n",
    "        if model_name == 'efficient':\n",
    "            self.backbone = EfficientNet.from_pretrained('efficientnet-b0', num_classes=1)\n",
    "            self.backbone.requires_grad = True\n",
    "            in_features = self.backbone._fc.in_features\n",
    "            self.backbone._fc = nn.Sequential(\n",
    "                nn.Linear(in_features=in_features, out_features=256, bias=True),\n",
    "                nn.BatchNorm1d(num_features=256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=256, out_features=4, bias=True),\n",
    "            )\n",
    "    def loss(self, pred, label):\n",
    "        loss = self.criterion(pred, label)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_img, target=None):\n",
    "        input_img = input_img.to(self.device)\n",
    "        \n",
    "        if target is not None:\n",
    "            target = target.to(self.device)\n",
    "        \n",
    "        x = self.first_layer(input_img)\n",
    "        print(1)\n",
    "        pred = self.backbone(x)\n",
    "        print(2)\n",
    "        pred = x.float()\n",
    "        loss = self.loss(pred, target)\n",
    "        if self.training:\n",
    "            return pred, loss\n",
    "        else:\n",
    "            return pred, loss\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=hidden_size, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, out_size, 4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.net(image).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        )\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(512, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(self.first_layer(x)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnet50(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        ) \n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(self.first_layer(x)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnext50(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnext101(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        model = models.resnext101_32x8d(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.1))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        \n",
    "def build_model(device, model_name='efficient', weight_path=None):\n",
    "    if model_name == 'efficient':\n",
    "        model = Model(device, model_name, weight_path)\n",
    "    elif model_name == 'resnet50':\n",
    "        model = Resnet50(4, False)\n",
    "    elif model_name == 'resnet18':\n",
    "        model = Resnet18(4, False)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    '''\n",
    "    Count of trainable weights in a model\n",
    "    '''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Implements AdamW algorithm.\n",
    "\n",
    "    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "    .. Fixing Weight Decay Regularization in Adam:\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # according to the paper, this penalty should come after the bias correction\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class CosineAnnealingWithRestartsLR(_LRScheduler):\n",
    "    '''\n",
    "    SGDR\\: Stochastic Gradient Descent with Warm Restarts: https://arxiv.org/abs/1608.03983\n",
    "    code: https://github.com/gurucharanmk/PyTorch_CosineAnnealingWithRestartsLR/blob/master/CosineAnnealingWithRestartsLR.py\n",
    "    added restart_decay value to decrease lr for every restarts\n",
    "    '''\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, T_mult=1, restart_decay=0.95):\n",
    "        self.T_max = T_max\n",
    "        self.T_mult = T_mult\n",
    "        self.next_restart = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.restarts = 0\n",
    "        self.last_restart = 0\n",
    "        self.T_num = 0\n",
    "        self.restart_decay = restart_decay\n",
    "        super(CosineAnnealingWithRestartsLR,self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        self.Tcur = self.last_epoch - self.last_restart\n",
    "        if self.Tcur >= self.next_restart:\n",
    "            self.next_restart *= self.T_mult\n",
    "            self.last_restart = self.last_epoch\n",
    "            self.T_num += 1\n",
    "        learning_rate = [(self.eta_min + ((base_lr)*self.restart_decay**self.T_num - self.eta_min) * (1 + math.cos(math.pi * self.Tcur / self.next_restart)) / 2) for base_lr in self.base_lrs]\n",
    "        return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "DATASET_PATH = '../wafer'\n",
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "submission = pd.read_csv(os.path.join(DATASET_PATH, 'sample_submission.csv'))\n",
    "\n",
    "if DEBUG:\n",
    "    train_df = train_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 1min 34s, sys: 5.28 s, total: 1min 39s\nWall time: 16 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "######## Scale\n",
    "scaler = StandardScaler()\n",
    "train_df.iloc[:,4:] = scaler.fit_transform(train_df.iloc[:,4:])\n",
    "test_df.iloc[:,1:] = scaler.fit_transform(test_df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "This dataframe does not have target value\n"
    }
   ],
   "source": [
    "# hyper parameter\n",
    "lr = 2.5e-4 / 4\n",
    "start_epoch = 0\n",
    "num_epochs = 300\n",
    "best_loss = 99999999\n",
    "loss_list = []\n",
    "\n",
    "batch_size = 4096\n",
    "\n",
    "test_loader = build_dataloader(test_df, batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda.is_available:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_len = int(len(train_df)*0.1)\n",
    "\n",
    "#X_train = train_df[valid_len:]\n",
    "#X_valid = train_df[:valid_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = build_dataloader(X_train, batch_size, True)\n",
    "#valid_loader = build_dataloader(X_valid, batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "#model = build_model(device, model_name='resnet')\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "#criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output path\n",
    "#output_dir = Path('./', 'output')\n",
    "#output_dir.mkdir(exist_ok=True, parents=True)\n",
    "#model_path = output_dir / 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train model\n",
    "# load_model_path = Path('../input/daconsemimodel/model (1).pt')\n",
    "\n",
    "# model.load_state_dict(torch.load(load_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, valid_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), 4))\n",
    "    valid_targets = np.zeros((len(valid_loader.dataset), 4))\n",
    "    val_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(valid_loader):\n",
    "            \n",
    "            valid_targets[i * batch_size: (i+1) * batch_size] = target.float().numpy().copy()\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "                \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_preds[i * batch_size: (i+1) * batch_size] = output.detach().cpu().numpy()\n",
    "            \n",
    "            val_loss += loss.item() / len(valid_loader)\n",
    "        \n",
    "    val_score = mean_absolute_error(valid_preds, valid_targets)\n",
    "    \n",
    "    return val_loss, val_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.vgg16(pretrained=False)\n",
    "#first_layer = nn.Conv2d(1, 3, (3, 1))\n",
    "#features = list(model.features)\n",
    "#features.insert(0, nn.Conv2d(1, 3, (3, 1)))\n",
    "#model.features = nn.Sequential(*features)\n",
    "#num_features = model.classifier[6].in_features # last layer's in_feature\n",
    "#features = list(model.classifier.children())[:-1] # remove last layer\n",
    "#features.extend([nn.Linear(num_features, 4)]) # Add new layer with 4 outputs\n",
    "#model.classifier = nn.Sequential(*features)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " 86 / 299\n--------------------------->> score improved..! \ntrain_loss- 2.28401  valid_loss- 2.09755  valid_score- 2.09078 in 1m 22s\nEpoch 87 / 299\ntrain_loss- 2.21998  valid_loss- 2.37398  valid_score- 2.37563 in 1m 22s\nEpoch 88 / 299\ntrain_loss- 2.29092  valid_loss- 2.35297  valid_score- 2.35106 in 1m 22s\nEpoch 89 / 299\ntrain_loss- 2.24399  valid_loss- 2.26631  valid_score- 2.26140 in 1m 22s\nEpoch 90 / 299\ntrain_loss- 2.25137  valid_loss- 2.17599  valid_score- 2.17565 in 1m 22s\nEpoch 91 / 299\ntrain_loss- 2.24717  valid_loss- 2.50979  valid_score- 2.50473 in 1m 22s\nEpoch 92 / 299\ntrain_loss- 2.18400  valid_loss- 2.15894  valid_score- 2.15760 in 1m 22s\nEpoch 93 / 299\ntrain_loss- 2.15843  valid_loss- 2.16317  valid_score- 2.16163 in 1m 22s\nEpoch 94 / 299\ntrain_loss- 2.16939  valid_loss- 2.22106  valid_score- 2.21993 in 1m 21s\nEpoch 95 / 299\ntrain_loss- 2.14493  valid_loss- 2.16804  valid_score- 2.16665 in 1m 21s\nEpoch 96 / 299\ntrain_loss- 2.18574  valid_loss- 2.19264  valid_score- 2.18211 in 1m 21s\nEpoch 97 / 299\ntrain_loss- 2.16850  valid_loss- 2.20715  valid_score- 2.20522 in 1m 21s\nEpoch 98 / 299\n--------------------------->> score improved..! \ntrain_loss- 2.18369  valid_loss- 2.05377  valid_score- 2.05393 in 1m 22s\nEpoch 99 / 299\ntrain_loss- 2.14617  valid_loss- 2.49822  valid_score- 2.49166 in 1m 21s\nEpoch 100 / 299\ntrain_loss- 2.19843  valid_loss- 2.30103  valid_score- 2.29838 in 1m 21s\nEpoch 101 / 299\ntrain_loss- 2.12626  valid_loss- 2.42226  valid_score- 2.41824 in 1m 21s\nEpoch 102 / 299\ntrain_loss- 2.12076  valid_loss- 2.08503  valid_score- 2.08420 in 1m 21s\nEpoch 103 / 299\ntrain_loss- 2.11868  valid_loss- 2.10474  valid_score- 2.10106 in 1m 21s\nEpoch 104 / 299\ntrain_loss- 2.15042  valid_loss- 2.22538  valid_score- 2.22293 in 1m 21s\nEpoch 105 / 299\ntrain_loss- 2.10911  valid_loss- 2.07567  valid_score- 2.07417 in 1m 21s\nEpoch 106 / 299\ntrain_loss- 2.07481  valid_loss- 2.25974  valid_score- 2.25282 in 1m 21s\nEpoch 107 / 299\n--------------------------->> score improved..! \ntrain_loss- 2.11193  valid_loss- 2.03996  valid_score- 2.03742 in 1m 22s\nEpoch 108 / 299\ntrain_loss- 2.10825  valid_loss- 2.13281  valid_score- 2.12894 in 1m 21s\nEpoch 109 / 299\ntrain_loss- 2.06878  valid_loss- 2.28529  valid_score- 2.28215 in 1m 21s\nEpoch 110 / 299\ntrain_loss- 2.09204  valid_loss- 2.14719  valid_score- 2.14252 in 1m 21s\nEpoch 111 / 299\ntrain_loss- 2.03612  valid_loss- 2.15897  valid_score- 2.15798 in 1m 21s\nEpoch 112 / 299\ntrain_loss- 2.10333  valid_loss- 2.20821  valid_score- 2.20743 in 1m 21s\nEpoch 113 / 299\ntrain_loss- 2.05451  valid_loss- 2.13276  valid_score- 2.13150 in 1m 22s\nEpoch 114 / 299\n--------------------------->> score improved..! \ntrain_loss- 2.03486  valid_loss- 1.95794  valid_score- 1.95275 in 1m 22s\nEpoch 115 / 299\ntrain_loss- 1.99841  valid_loss- 2.13433  valid_score- 2.13240 in 1m 22s\nEpoch 116 / 299\ntrain_loss- 2.00735  valid_loss- 2.09435  valid_score- 2.08865 in 1m 22s\nEpoch 117 / 299\ntrain_loss- 2.02715  valid_loss- 2.00155  valid_score- 1.99924 in 1m 22s\nEpoch 118 / 299\n--------------------------->> score improved..! \ntrain_loss- 2.02937  valid_loss- 1.81491  valid_score- 1.81503 in 1m 22s\nEpoch 119 / 299\ntrain_loss- 2.01912  valid_loss- 2.03202  valid_score- 2.03327 in 1m 22s\nEpoch 120 / 299\ntrain_loss- 2.02523  valid_loss- 2.22315  valid_score- 2.22215 in 1m 22s\nEpoch 121 / 299\ntrain_loss- 2.01546  valid_loss- 2.05814  valid_score- 2.05411 in 1m 22s\nEpoch 122 / 299\ntrain_loss- 2.01068  valid_loss- 2.04176  valid_score- 2.04189 in 1m 22s\nEpoch 123 / 299\ntrain_loss- 2.02856  valid_loss- 2.04793  valid_score- 2.04406 in 1m 22s\nEpoch 124 / 299\ntrain_loss- 2.01066  valid_loss- 1.88711  valid_score- 1.88310 in 1m 22s\nEpoch 125 / 299\ntrain_loss- 1.99537  valid_loss- 1.98802  valid_score- 1.98228 in 1m 22s\nEpoch 126 / 299\ntrain_loss- 1.96391  valid_loss- 1.92870  valid_score- 1.92963 in 1m 22s\nEpoch 127 / 299\ntrain_loss- 1.98210  valid_loss- 1.98110  valid_score- 1.97426 in 1m 22s\nEpoch 128 / 299\ntrain_loss- 1.96265  valid_loss- 1.84844  valid_score- 1.84595 in 1m 22s\nEpoch 129 / 299\ntrain_loss- 1.92274  valid_loss- 1.99076  valid_score- 1.99165 in 1m 22s\nEpoch 130 / 299\ntrain_loss- 1.99794  valid_loss- 1.93884  valid_score- 1.93607 in 1m 22s\nEpoch 131 / 299\ntrain_loss- 1.92762  valid_loss- 1.88750  valid_score- 1.88333 in 1m 22s\nEpoch 132 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.96884  valid_loss- 1.80177  valid_score- 1.79697 in 1m 22s\nEpoch 133 / 299\ntrain_loss- 1.95557  valid_loss- 2.11330  valid_score- 2.10635 in 1m 22s\nEpoch 134 / 299\ntrain_loss- 1.96044  valid_loss- 1.95260  valid_score- 1.95071 in 1m 22s\nEpoch 135 / 299\ntrain_loss- 1.96998  valid_loss- 1.93403  valid_score- 1.93454 in 1m 22s\nEpoch 136 / 299\ntrain_loss- 1.95009  valid_loss- 1.86679  valid_score- 1.86391 in 1m 22s\nEpoch 137 / 299\ntrain_loss- 1.93067  valid_loss- 1.95459  valid_score- 1.95113 in 1m 22s\nEpoch 138 / 299\ntrain_loss- 1.93748  valid_loss- 2.01346  valid_score- 2.00882 in 1m 22s\nEpoch 139 / 299\ntrain_loss- 1.92885  valid_loss- 2.09067  valid_score- 2.09178 in 1m 22s\nEpoch 140 / 299\ntrain_loss- 1.94319  valid_loss- 2.01815  valid_score- 2.01174 in 1m 22s\nEpoch 141 / 299\ntrain_loss- 1.93897  valid_loss- 1.86318  valid_score- 1.86349 in 1m 22s\nEpoch 142 / 299\ntrain_loss- 1.89791  valid_loss- 1.89755  valid_score- 1.90035 in 1m 22s\nEpoch 143 / 299\ntrain_loss- 1.93508  valid_loss- 2.00373  valid_score- 1.99349 in 1m 22s\nEpoch 144 / 299\ntrain_loss- 1.91519  valid_loss- 1.86301  valid_score- 1.86054 in 1m 22s\nEpoch 145 / 299\ntrain_loss- 1.93078  valid_loss- 1.90548  valid_score- 1.90347 in 1m 22s\nEpoch 146 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.88992  valid_loss- 1.71434  valid_score- 1.71130 in 1m 22s\nEpoch 147 / 299\ntrain_loss- 1.90678  valid_loss- 2.21617  valid_score- 2.21463 in 1m 22s\nEpoch 148 / 299\ntrain_loss- 1.92918  valid_loss- 1.91249  valid_score- 1.91100 in 1m 22s\nEpoch 149 / 299\ntrain_loss- 1.86827  valid_loss- 1.78885  valid_score- 1.78863 in 1m 22s\nEpoch 150 / 299\ntrain_loss- 1.90987  valid_loss- 1.82979  valid_score- 1.82985 in 1m 22s\nEpoch 151 / 299\ntrain_loss- 1.85044  valid_loss- 1.95997  valid_score- 1.95408 in 1m 22s\nEpoch 152 / 299\ntrain_loss- 1.92062  valid_loss- 1.92248  valid_score- 1.92070 in 1m 22s\nEpoch 153 / 299\ntrain_loss- 1.88411  valid_loss- 1.99353  valid_score- 1.98994 in 1m 22s\nEpoch 154 / 299\ntrain_loss- 1.88974  valid_loss- 2.06866  valid_score- 2.06524 in 1m 22s\nEpoch 155 / 299\ntrain_loss- 1.86074  valid_loss- 1.88713  valid_score- 1.88579 in 1m 22s\nEpoch 156 / 299\ntrain_loss- 1.90767  valid_loss- 2.13370  valid_score- 2.13374 in 1m 22s\nEpoch 157 / 299\ntrain_loss- 1.86856  valid_loss- 2.22877  valid_score- 2.22136 in 1m 22s\nEpoch 158 / 299\ntrain_loss- 1.93454  valid_loss- 1.77415  valid_score- 1.77461 in 1m 22s\nEpoch 159 / 299\ntrain_loss- 1.82757  valid_loss- 1.87833  valid_score- 1.87845 in 1m 22s\nEpoch 160 / 299\ntrain_loss- 1.89203  valid_loss- 2.33601  valid_score- 2.33145 in 1m 22s\nEpoch 161 / 299\ntrain_loss- 1.87864  valid_loss- 1.93189  valid_score- 1.92921 in 1m 22s\nEpoch 162 / 299\ntrain_loss- 1.85173  valid_loss- 1.92652  valid_score- 1.92487 in 1m 22s\nEpoch 163 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.85320  valid_loss- 1.68704  valid_score- 1.68520 in 1m 23s\nEpoch 164 / 299\ntrain_loss- 1.83949  valid_loss- 1.78719  valid_score- 1.78623 in 1m 22s\nEpoch 165 / 299\ntrain_loss- 1.84841  valid_loss- 1.82831  valid_score- 1.82471 in 1m 22s\nEpoch 166 / 299\ntrain_loss- 1.88312  valid_loss- 1.97447  valid_score- 1.96951 in 1m 22s\nEpoch 167 / 299\ntrain_loss- 1.79994  valid_loss- 1.75656  valid_score- 1.75478 in 1m 22s\nEpoch 168 / 299\ntrain_loss- 1.83217  valid_loss- 1.83509  valid_score- 1.83566 in 1m 22s\nEpoch 169 / 299\ntrain_loss- 1.84385  valid_loss- 1.88398  valid_score- 1.88092 in 1m 22s\nEpoch 170 / 299\ntrain_loss- 1.84751  valid_loss- 1.69606  valid_score- 1.69043 in 1m 22s\nEpoch 171 / 299\ntrain_loss- 1.83304  valid_loss- 1.98450  valid_score- 1.97708 in 1m 22s\nEpoch 172 / 299\ntrain_loss- 1.83960  valid_loss- 1.92509  valid_score- 1.92028 in 1m 22s\nEpoch 173 / 299\ntrain_loss- 1.81472  valid_loss- 1.81573  valid_score- 1.80987 in 1m 22s\nEpoch 174 / 299\ntrain_loss- 1.82441  valid_loss- 1.72163  valid_score- 1.72273 in 1m 22s\nEpoch 175 / 299\ntrain_loss- 1.80708  valid_loss- 1.86187  valid_score- 1.86037 in 1m 22s\nEpoch 176 / 299\ntrain_loss- 1.82784  valid_loss- 1.76803  valid_score- 1.76465 in 1m 22s\nEpoch 177 / 299\ntrain_loss- 1.84809  valid_loss- 1.89749  valid_score- 1.89552 in 1m 22s\nEpoch 178 / 299\ntrain_loss- 1.83132  valid_loss- 1.88362  valid_score- 1.88335 in 1m 22s\nEpoch 179 / 299\ntrain_loss- 1.81423  valid_loss- 1.91702  valid_score- 1.91797 in 1m 22s\nEpoch 180 / 299\ntrain_loss- 1.80565  valid_loss- 1.75372  valid_score- 1.75240 in 1m 22s\nEpoch 181 / 299\ntrain_loss- 1.78734  valid_loss- 1.87685  valid_score- 1.87654 in 1m 22s\nEpoch 182 / 299\ntrain_loss- 1.79738  valid_loss- 1.93701  valid_score- 1.93771 in 1m 22s\nEpoch 183 / 299\ntrain_loss- 1.78987  valid_loss- 1.93643  valid_score- 1.93394 in 1m 22s\nEpoch 184 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.79845  valid_loss- 1.68550  valid_score- 1.68659 in 1m 22s\nEpoch 185 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.73276  valid_loss- 1.59662  valid_score- 1.59644 in 1m 23s\nEpoch 186 / 299\ntrain_loss- 1.79215  valid_loss- 1.83381  valid_score- 1.83514 in 1m 22s\nEpoch 187 / 299\ntrain_loss- 1.80474  valid_loss- 1.65337  valid_score- 1.65235 in 1m 22s\nEpoch 188 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.79177  valid_loss- 1.57027  valid_score- 1.56906 in 1m 22s\nEpoch 189 / 299\ntrain_loss- 1.76215  valid_loss- 1.76294  valid_score- 1.75993 in 1m 22s\nEpoch 190 / 299\ntrain_loss- 1.81292  valid_loss- 1.75799  valid_score- 1.75777 in 1m 22s\nEpoch 191 / 299\ntrain_loss- 1.80504  valid_loss- 2.07139  valid_score- 2.06661 in 1m 22s\nEpoch 192 / 299\ntrain_loss- 1.75284  valid_loss- 1.74331  valid_score- 1.74424 in 1m 22s\nEpoch 193 / 299\ntrain_loss- 1.73726  valid_loss- 1.58027  valid_score- 1.57807 in 1m 22s\nEpoch 194 / 299\ntrain_loss- 1.66701  valid_loss- 1.63421  valid_score- 1.63304 in 1m 22s\nEpoch 195 / 299\ntrain_loss- 1.76529  valid_loss- 1.74034  valid_score- 1.73969 in 1m 22s\nEpoch 196 / 299\ntrain_loss- 1.75426  valid_loss- 1.76363  valid_score- 1.75881 in 1m 22s\nEpoch 197 / 299\ntrain_loss- 1.77768  valid_loss- 1.69757  valid_score- 1.69348 in 1m 22s\nEpoch 198 / 299\ntrain_loss- 1.75623  valid_loss- 2.06533  valid_score- 2.06043 in 1m 22s\nEpoch 199 / 299\ntrain_loss- 1.78263  valid_loss- 1.63515  valid_score- 1.63708 in 1m 22s\nEpoch 200 / 299\ntrain_loss- 1.74779  valid_loss- 1.65962  valid_score- 1.65787 in 1m 22s\nEpoch 201 / 299\ntrain_loss- 1.74887  valid_loss- 1.85556  valid_score- 1.85540 in 1m 22s\nEpoch 202 / 299\ntrain_loss- 1.72645  valid_loss- 1.80046  valid_score- 1.79204 in 1m 22s\nEpoch 203 / 299\ntrain_loss- 1.72337  valid_loss- 1.70429  valid_score- 1.70210 in 1m 22s\nEpoch 204 / 299\ntrain_loss- 1.71771  valid_loss- 1.67096  valid_score- 1.66815 in 1m 22s\nEpoch 205 / 299\ntrain_loss- 1.73818  valid_loss- 1.63668  valid_score- 1.63659 in 1m 22s\nEpoch 206 / 299\ntrain_loss- 1.74313  valid_loss- 1.89519  valid_score- 1.89186 in 1m 22s\nEpoch 207 / 299\ntrain_loss- 1.72486  valid_loss- 1.58851  valid_score- 1.59003 in 1m 22s\nEpoch 208 / 299\ntrain_loss- 1.74080  valid_loss- 1.67572  valid_score- 1.67270 in 1m 22s\nEpoch 209 / 299\ntrain_loss- 1.78579  valid_loss- 1.61758  valid_score- 1.61584 in 1m 22s\nEpoch 210 / 299\ntrain_loss- 1.73413  valid_loss- 1.62015  valid_score- 1.62000 in 1m 22s\nEpoch 211 / 299\ntrain_loss- 1.73618  valid_loss- 1.69728  valid_score- 1.69610 in 1m 22s\nEpoch 212 / 299\ntrain_loss- 1.72655  valid_loss- 1.62202  valid_score- 1.61672 in 1m 22s\nEpoch 213 / 299\ntrain_loss- 1.76476  valid_loss- 1.70453  valid_score- 1.70359 in 1m 22s\nEpoch 214 / 299\ntrain_loss- 1.70211  valid_loss- 1.67108  valid_score- 1.66989 in 1m 22s\nEpoch 215 / 299\ntrain_loss- 1.74281  valid_loss- 1.62590  valid_score- 1.62634 in 1m 22s\nEpoch 216 / 299\ntrain_loss- 1.66438  valid_loss- 1.65122  valid_score- 1.64766 in 1m 22s\nEpoch 217 / 299\ntrain_loss- 1.72769  valid_loss- 1.71227  valid_score- 1.70998 in 1m 22s\nEpoch 218 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.70371  valid_loss- 1.54570  valid_score- 1.54637 in 1m 22s\nEpoch 219 / 299\ntrain_loss- 1.73098  valid_loss- 1.92825  valid_score- 1.92541 in 1m 22s\nEpoch 220 / 299\ntrain_loss- 1.71332  valid_loss- 1.61984  valid_score- 1.62254 in 1m 22s\nEpoch 221 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.71631  valid_loss- 1.54523  valid_score- 1.54406 in 1m 22s\nEpoch 222 / 299\ntrain_loss- 1.68607  valid_loss- 1.60186  valid_score- 1.60085 in 1m 22s\nEpoch 223 / 299\ntrain_loss- 1.69533  valid_loss- 1.73228  valid_score- 1.73233 in 1m 22s\nEpoch 224 / 299\ntrain_loss- 1.72716  valid_loss- 1.75801  valid_score- 1.75729 in 1m 22s\nEpoch 225 / 299\ntrain_loss- 1.67331  valid_loss- 1.94983  valid_score- 1.95086 in 1m 22s\nEpoch 226 / 299\ntrain_loss- 1.69409  valid_loss- 1.61728  valid_score- 1.61866 in 1m 22s\nEpoch 227 / 299\ntrain_loss- 1.71108  valid_loss- 1.74325  valid_score- 1.73942 in 1m 22s\nEpoch 228 / 299\ntrain_loss- 1.67832  valid_loss- 1.54886  valid_score- 1.54879 in 1m 22s\nEpoch 229 / 299\ntrain_loss- 1.69995  valid_loss- 1.79670  valid_score- 1.79270 in 1m 22s\nEpoch 230 / 299\ntrain_loss- 1.68066  valid_loss- 1.80672  valid_score- 1.80566 in 1m 22s\nEpoch 231 / 299\ntrain_loss- 1.72476  valid_loss- 1.66579  valid_score- 1.66459 in 1m 22s\nEpoch 232 / 299\ntrain_loss- 1.74048  valid_loss- 1.70463  valid_score- 1.70121 in 1m 22s\nEpoch 233 / 299\ntrain_loss- 1.63751  valid_loss- 1.67422  valid_score- 1.67065 in 1m 22s\nEpoch 234 / 299\ntrain_loss- 1.63399  valid_loss- 1.66348  valid_score- 1.66105 in 1m 22s\nEpoch 235 / 299\ntrain_loss- 1.64810  valid_loss- 1.64451  valid_score- 1.64135 in 1m 22s\nEpoch 236 / 299\ntrain_loss- 1.68722  valid_loss- 1.55286  valid_score- 1.54871 in 1m 22s\nEpoch 237 / 299\ntrain_loss- 1.65754  valid_loss- 1.64760  valid_score- 1.64275 in 1m 22s\nEpoch 238 / 299\ntrain_loss- 1.67822  valid_loss- 1.78080  valid_score- 1.77828 in 1m 22s\nEpoch 239 / 299\ntrain_loss- 1.69650  valid_loss- 1.56678  valid_score- 1.56862 in 1m 22s\nEpoch 240 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.65876  valid_loss- 1.53295  valid_score- 1.53350 in 1m 23s\nEpoch 241 / 299\ntrain_loss- 1.66751  valid_loss- 1.57361  valid_score- 1.56897 in 1m 22s\nEpoch 242 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.69922  valid_loss- 1.49654  valid_score- 1.49547 in 1m 22s\nEpoch 243 / 299\ntrain_loss- 1.68489  valid_loss- 1.74981  valid_score- 1.75001 in 1m 22s\nEpoch 244 / 299\ntrain_loss- 1.65663  valid_loss- 1.62388  valid_score- 1.62171 in 1m 22s\nEpoch 245 / 299\ntrain_loss- 1.63186  valid_loss- 1.73309  valid_score- 1.73338 in 1m 22s\nEpoch 246 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.61410  valid_loss- 1.46788  valid_score- 1.46731 in 1m 23s\nEpoch 247 / 299\ntrain_loss- 1.65846  valid_loss- 1.63140  valid_score- 1.63203 in 1m 22s\nEpoch 248 / 299\ntrain_loss- 1.67697  valid_loss- 1.49078  valid_score- 1.49183 in 1m 22s\nEpoch 249 / 299\ntrain_loss- 1.65583  valid_loss- 1.65215  valid_score- 1.64972 in 1m 22s\nEpoch 250 / 299\ntrain_loss- 1.66961  valid_loss- 1.48554  valid_score- 1.48723 in 1m 22s\nEpoch 251 / 299\ntrain_loss- 1.65025  valid_loss- 1.68141  valid_score- 1.67976 in 1m 22s\nEpoch 252 / 299\ntrain_loss- 1.67408  valid_loss- 1.78760  valid_score- 1.78530 in 1m 22s\nEpoch 253 / 299\ntrain_loss- 1.63355  valid_loss- 1.63690  valid_score- 1.63736 in 1m 22s\nEpoch 254 / 299\ntrain_loss- 1.62277  valid_loss- 1.71490  valid_score- 1.71171 in 1m 22s\nEpoch 255 / 299\ntrain_loss- 1.65456  valid_loss- 1.92142  valid_score- 1.91635 in 1m 22s\nEpoch 256 / 299\ntrain_loss- 1.63111  valid_loss- 1.58926  valid_score- 1.59042 in 1m 22s\nEpoch 257 / 299\ntrain_loss- 1.68622  valid_loss- 1.65439  valid_score- 1.65409 in 1m 22s\nEpoch 258 / 299\ntrain_loss- 1.67448  valid_loss- 1.98960  valid_score- 1.98341 in 1m 22s\nEpoch 259 / 299\ntrain_loss- 1.64323  valid_loss- 1.63361  valid_score- 1.62912 in 1m 22s\nEpoch 260 / 299\ntrain_loss- 1.61945  valid_loss- 1.63244  valid_score- 1.62988 in 1m 22s\nEpoch 261 / 299\ntrain_loss- 1.63045  valid_loss- 1.54065  valid_score- 1.54180 in 1m 22s\nEpoch 262 / 299\ntrain_loss- 1.62396  valid_loss- 1.62938  valid_score- 1.63161 in 1m 22s\nEpoch 263 / 299\ntrain_loss- 1.62540  valid_loss- 1.69196  valid_score- 1.68653 in 1m 22s\nEpoch 264 / 299\ntrain_loss- 1.63024  valid_loss- 1.70791  valid_score- 1.70727 in 1m 22s\nEpoch 265 / 299\ntrain_loss- 1.62910  valid_loss- 1.49498  valid_score- 1.49388 in 1m 22s\nEpoch 266 / 299\ntrain_loss- 1.60647  valid_loss- 1.47094  valid_score- 1.47249 in 1m 22s\nEpoch 267 / 299\ntrain_loss- 1.63981  valid_loss- 1.81173  valid_score- 1.81016 in 1m 22s\nEpoch 268 / 299\ntrain_loss- 1.64896  valid_loss- 1.65999  valid_score- 1.65634 in 1m 22s\nEpoch 269 / 299\ntrain_loss- 1.60730  valid_loss- 1.71525  valid_score- 1.71529 in 1m 22s\nEpoch 270 / 299\ntrain_loss- 1.59468  valid_loss- 1.63873  valid_score- 1.63700 in 1m 22s\nEpoch 271 / 299\ntrain_loss- 1.61841  valid_loss- 1.66992  valid_score- 1.66518 in 1m 22s\nEpoch 272 / 299\ntrain_loss- 1.65131  valid_loss- 1.55201  valid_score- 1.55179 in 1m 22s\nEpoch 273 / 299\ntrain_loss- 1.64500  valid_loss- 1.57030  valid_score- 1.56722 in 1m 22s\nEpoch 274 / 299\ntrain_loss- 1.64459  valid_loss- 1.58348  valid_score- 1.58222 in 1m 22s\nEpoch 275 / 299\ntrain_loss- 1.62310  valid_loss- 1.70386  valid_score- 1.70019 in 1m 22s\nEpoch 276 / 299\ntrain_loss- 1.60969  valid_loss- 1.57129  valid_score- 1.56841 in 1m 22s\nEpoch 277 / 299\ntrain_loss- 1.62734  valid_loss- 1.51502  valid_score- 1.51000 in 1m 22s\nEpoch 278 / 299\ntrain_loss- 1.62289  valid_loss- 1.61682  valid_score- 1.61529 in 1m 22s\nEpoch 279 / 299\ntrain_loss- 1.60179  valid_loss- 1.66260  valid_score- 1.66033 in 1m 22s\nEpoch 280 / 299\ntrain_loss- 1.58700  valid_loss- 1.47061  valid_score- 1.46838 in 1m 22s\nEpoch 281 / 299\ntrain_loss- 1.57981  valid_loss- 1.56115  valid_score- 1.56077 in 1m 22s\nEpoch 282 / 299\ntrain_loss- 1.61971  valid_loss- 1.48830  valid_score- 1.48535 in 1m 22s\nEpoch 283 / 299\ntrain_loss- 1.58443  valid_loss- 1.78736  valid_score- 1.78703 in 1m 22s\nEpoch 284 / 299\ntrain_loss- 1.59205  valid_loss- 1.48943  valid_score- 1.48533 in 1m 22s\nEpoch 285 / 299\ntrain_loss- 1.61713  valid_loss- 1.68290  valid_score- 1.68205 in 1m 22s\nEpoch 286 / 299\ntrain_loss- 1.62228  valid_loss- 1.77234  valid_score- 1.76584 in 1m 22s\nEpoch 287 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.60240  valid_loss- 1.45283  valid_score- 1.45006 in 1m 22s\nEpoch 288 / 299\ntrain_loss- 1.60666  valid_loss- 1.66874  valid_score- 1.66250 in 1m 22s\nEpoch 289 / 299\ntrain_loss- 1.58683  valid_loss- 1.49757  valid_score- 1.49890 in 1m 22s\nEpoch 290 / 299\ntrain_loss- 1.59894  valid_loss- 1.52544  valid_score- 1.52270 in 1m 22s\nEpoch 291 / 299\ntrain_loss- 1.60742  valid_loss- 1.65324  valid_score- 1.64571 in 1m 22s\nEpoch 292 / 299\ntrain_loss- 1.55208  valid_loss- 1.60566  valid_score- 1.60629 in 1m 22s\nEpoch 293 / 299\ntrain_loss- 1.56209  valid_loss- 1.48497  valid_score- 1.48270 in 1m 22s\nEpoch 294 / 299\ntrain_loss- 1.58503  valid_loss- 1.66471  valid_score- 1.66320 in 1m 22s\nEpoch 295 / 299\n--------------------------->> score improved..! \ntrain_loss- 1.56358  valid_loss- 1.33893  valid_score- 1.33647 in 1m 22s\nEpoch 296 / 299\ntrain_loss- 1.58923  valid_loss- 1.60962  valid_score- 1.61052 in 1m 22s\nEpoch 297 / 299\ntrain_loss- 1.59193  valid_loss- 1.45059  valid_score- 1.45014 in 1m 22s\nEpoch 298 / 299\ntrain_loss- 1.61975  valid_loss- 1.67733  valid_score- 1.67147 in 1m 22s\nEpoch 299 / 299\ntrain_loss- 1.56001  valid_loss- 1.63612  valid_score- 1.63783 in 1m 22s\n==================== Resnet18 nfft8 Fold 5 / 5 - Best val_loss - 1.33893 =================\n"
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "n_splits = 5\n",
    "seed_everything(seed)\n",
    "splits = list(KFold(n_splits=n_splits, shuffle=True, random_state=seed).split(train_df))\n",
    "\n",
    "fold_loss = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(\"===== Fold {} / {} Starts....\".format(fold+1, n_splits))\n",
    "    fold_num = str(fold+1)\n",
    "    trn_index, val_index = splits[fold]\n",
    "    x_tr, x_val = train_df.iloc[trn_index, :], train_df.iloc[val_index, :]\n",
    "    \n",
    "    # build model\n",
    "    \n",
    "    model = build_model(device, model_name='resnet18')\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    train_loader = build_dataloader(x_tr, batch_size, True)\n",
    "    valid_loader = build_dataloader(x_val, batch_size, False)\n",
    "    \n",
    "    best_loss = 99999999\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        since = time()\n",
    "\n",
    "        print('Epoch {} / {}'.format(epoch,  num_epochs - 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            if device:\n",
    "                data = data.to(device)\n",
    "                target = target.float().to(device)\n",
    "            else:\n",
    "                target = target.float()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        val_loss, val_score = validation(model, criterion, valid_loader, device)\n",
    "\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"F{}_nfft8_scale_resnet18_model.pt\".format(fold_num))\n",
    "            print(\"--------------------------->> score improved..! \")\n",
    "\n",
    "        time_elapsed = time() - since\n",
    "        print('train_loss- {:.5f}  valid_loss- {:.5f}  valid_score- {:.5f} in {:.0f}m {:.0f}s'.format(\n",
    "            train_loss, val_loss, val_score, time_elapsed // 60, time_elapsed % 60))\n",
    "        #print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"==================== Resnet18 nfft8 Fold {} / {} - Best val_loss - {:.5f} =================\".format(fold+1, n_splits, best_loss))\n",
    "    fold_loss.append(best_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1.3845470219850535,\n 1.2730849385261538,\n 1.3619767844676973,\n 1.3246213018894195,\n 1.338927274942398]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "===== Fold 1 / 5 Inference Starts....\nprediction value check:  tensor([290.2043, 157.1879, 248.8690, 206.9893], device='cuda:0')\n(10000, 4)\n===== Fold 2 / 5 Inference Starts....\nprediction value check:  tensor([290.9750, 160.4279, 250.9993, 205.4199], device='cuda:0')\n(10000, 4)\n===== Fold 3 / 5 Inference Starts....\nprediction value check:  tensor([289.6442, 158.2900, 250.2275, 204.2264], device='cuda:0')\n(10000, 4)\n===== Fold 4 / 5 Inference Starts....\nprediction value check:  tensor([289.2694, 158.1774, 249.0704, 206.9840], device='cuda:0')\n(10000, 4)\n===== Fold 5 / 5 Inference Starts....\nprediction value check:  tensor([290.4727, 157.1521, 246.7804, 205.2207], device='cuda:0')\n(10000, 4)\n"
    }
   ],
   "source": [
    "for fold in range(n_splits):\n",
    "    print(\"===== Fold {} / {} Inference Starts....\".format(fold+1, n_splits))\n",
    "    fold_num = str(fold+1)\n",
    "    # inference #########\n",
    "    model.load_state_dict(torch.load(\"F{}_nfft8_scale_resnet18_model.pt\".format(fold_num)))\n",
    "    model.eval()\n",
    "\n",
    "    predictions = np.zeros((len(test_loader.dataset),4))\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions[i*batch_size: (i+1)*batch_size] = output.detach().cpu().numpy()\n",
    "    print('prediction value check: ', output[0])\n",
    "    print(predictions.shape)\n",
    "    np.savetxt('../wafer/resnet18_submission/F{}_nfft8_scale_resnet18.csv'.format(fold_num), predictions, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': submission['id'],\n",
    "                           'layer_1':predictions.transpose()[0],\n",
    "                           'layer_2':predictions.transpose()[1],\n",
    "                           'layer_3':predictions.transpose()[2],\n",
    "                           'layer_4':predictions.transpose()[3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>layer_1</th>\n      <th>layer_2</th>\n      <th>layer_3</th>\n      <th>layer_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>255.240219</td>\n      <td>230.362427</td>\n      <td>132.332916</td>\n      <td>85.161140</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>155.673691</td>\n      <td>127.147659</td>\n      <td>235.691788</td>\n      <td>98.724014</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>146.466339</td>\n      <td>177.290543</td>\n      <td>273.294220</td>\n      <td>156.929001</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>91.446251</td>\n      <td>228.349091</td>\n      <td>190.216812</td>\n      <td>81.306694</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>274.826447</td>\n      <td>294.910217</td>\n      <td>244.904617</td>\n      <td>271.772247</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   id     layer_1     layer_2     layer_3     layer_4\n0   0  255.240219  230.362427  132.332916   85.161140\n1   1  155.673691  127.147659  235.691788   98.724014\n2   2  146.466339  177.290543  273.294220  156.929001\n3   3   91.446251  228.349091  190.216812   81.306694\n4   4  274.826447  294.910217  244.904617  271.772247"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../wafer/resnet18_submission/resnet18_5fold_nfft8_scale_300e.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}