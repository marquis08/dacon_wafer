{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils import CosineAnnealingWithRestartsLR\n",
    "\n",
    "from torch.optim import Adam, SGD, Optimizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Semi_dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.X_dataset = []\n",
    "        self.Y_dataset = []\n",
    "        for x in X:\n",
    "            self.X_dataset.append(torch.FloatTensor(x))\n",
    "        try:\n",
    "            for y in Y.values:\n",
    "                self.Y_dataset.append(torch.tensor(y))\n",
    "        except:\n",
    "            print(\"no label\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X_dataset[index]\n",
    "        try:\n",
    "            target = self.Y_dataset[index]\n",
    "            return data, target\n",
    "        except:\n",
    "            return data\n",
    "\n",
    "\n",
    "def build_dataloader(X, Y, batch_size, shuffle=False):\n",
    "    \n",
    "    dataset = Semi_dataset(X, Y)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=8\n",
    "                            )\n",
    "    return dataloader\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred,\n",
    "                        sample_weight=None,\n",
    "                        multioutput='uniform_average'):\n",
    "    \n",
    "    output_errors = np.average(np.abs(y_pred - y_true),\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class MLP_only_flatfeatures(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(MLP_only_flatfeatures, self).__init__()\n",
    "        self.num_classes = num_classes         \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            #nn.Linear(226, 1000),\n",
    "            nn.Linear(226, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            ####### Block 1 #######\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            ####### Block 2 #######\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            ####### Block 3 #######\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            ####### Block 4 #######\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            ####### Block 5 #######\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            ####### Block 6 #######\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            ####### Block 7 #######\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            ####### Block 8 #######\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            ######### LAST ##########\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, self.num_classes)\n",
    "            )             \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_layers(x)\n",
    "        return out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def build_model(device, model_name='mlp', weight_path=None):\n",
    "\n",
    "    if model_name == 'mlp':\n",
    "        model = MLP_only_flatfeatures(4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def validation(model, criterion, valid_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), 4))\n",
    "    valid_targets = np.zeros((len(valid_loader.dataset), 4))\n",
    "    val_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(valid_loader):\n",
    "            \n",
    "            valid_targets[i * batch_size: (i+1) * batch_size] = target.float().numpy().copy()\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "                \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_preds[i * batch_size: (i+1) * batch_size] = output.detach().cpu().numpy()\n",
    "            \n",
    "            val_loss += loss.item() / len(valid_loader)\n",
    "    \n",
    "    val_score = mean_absolute_error(valid_preds, valid_targets)\n",
    "\n",
    "    return val_loss, val_score\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "if cuda.is_available:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.5 s, sys: 4.86 s, total: 40.4 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATASET_PATH = '../wafer'\n",
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(DATASET_PATH, 'sample_submission.csv'))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df.iloc[:, 4:], train_df.iloc[:, :4], test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "batch_size = 2048\n",
    "train_loader = build_dataloader(X_train, y_train, batch_size, shuffle=True)\n",
    "valid_loader = build_dataloader(X_val, y_val, batch_size, shuffle=False)\n",
    "\n",
    "test_df.iloc[:, 1:] = scaler.transform(test_df.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532  val_loss: 3.0597  val_score: 3.0602  lr: 0.00100  elapsed: 49m 40s\n",
      "Epoch 320 / 19999  train Loss: 6.2484  val_loss: 3.1612  val_score: 3.1610  lr: 0.00100  elapsed: 49m 49s\n",
      "Epoch 321 / 19999  train Loss: 6.2549  val_loss: 2.9586  val_score: 2.9595  lr: 0.00100  elapsed: 49m 59s\n",
      "Epoch 322 / 19999  train Loss: 6.2093  val_loss: 3.0348  val_score: 3.0352  lr: 0.00100  elapsed: 50m 8s\n",
      "Epoch 323 / 19999  train Loss: 6.2340  val_loss: 3.0129  val_score: 3.0144  lr: 0.00100  elapsed: 50m 17s\n",
      "Epoch 324 / 19999  train Loss: 6.2639  val_loss: 3.2287  val_score: 3.2285  lr: 0.00100  elapsed: 50m 27s\n",
      "Epoch 325 / 19999  train Loss: 6.2270  val_loss: 3.0607  val_score: 3.0608  lr: 0.00100  elapsed: 50m 36s\n",
      "Epoch 326 / 19999  train Loss: 6.1415  val_loss: 2.8638  val_score: 2.8646  lr: 0.00100  elapsed: 50m 45s\n",
      "Epoch 327 / 19999  train Loss: 6.1642  val_loss: 3.1133  val_score: 3.1144  lr: 0.00100  elapsed: 50m 54s\n",
      "Epoch 328 / 19999  train Loss: 6.2314  val_loss: 3.0220  val_score: 3.0232  lr: 0.00100  elapsed: 51m 4s\n",
      "Epoch 329 / 19999  train Loss: 6.1998  val_loss: 2.8327  val_score: 2.8328  lr: 0.00100  elapsed: 51m 13s\n",
      "----------------------------------------------------------------------->> loss improved to 2.83281\n",
      "Epoch 330 / 19999  train Loss: 6.1979  val_loss: 3.0113  val_score: 3.0117  lr: 0.00100  elapsed: 51m 22s\n",
      "Epoch 331 / 19999  train Loss: 6.2105  val_loss: 3.1539  val_score: 3.1533  lr: 0.00100  elapsed: 51m 31s\n",
      "Epoch 332 / 19999  train Loss: 6.2476  val_loss: 3.2366  val_score: 3.2353  lr: 0.00100  elapsed: 51m 41s\n",
      "Epoch 333 / 19999  train Loss: 6.1803  val_loss: 2.9205  val_score: 2.9219  lr: 0.00100  elapsed: 51m 50s\n",
      "Epoch 334 / 19999  train Loss: 6.1711  val_loss: 2.9614  val_score: 2.9618  lr: 0.00100  elapsed: 51m 60s\n",
      "Epoch 335 / 19999  train Loss: 6.1952  val_loss: 2.8688  val_score: 2.8688  lr: 0.00100  elapsed: 52m 9s\n",
      "Epoch 336 / 19999  train Loss: 6.1677  val_loss: 2.9656  val_score: 2.9665  lr: 0.00100  elapsed: 52m 18s\n",
      "Epoch 337 / 19999  train Loss: 6.1683  val_loss: 2.8961  val_score: 2.8974  lr: 0.00100  elapsed: 52m 28s\n",
      "Epoch 338 / 19999  train Loss: 6.1549  val_loss: 3.1306  val_score: 3.1321  lr: 0.00100  elapsed: 52m 37s\n",
      "Epoch 339 / 19999  train Loss: 6.2153  val_loss: 3.1214  val_score: 3.1226  lr: 0.00100  elapsed: 52m 46s\n",
      "Epoch 340 / 19999  train Loss: 6.2074  val_loss: 3.0632  val_score: 3.0627  lr: 0.00100  elapsed: 52m 56s\n",
      "Epoch 341 / 19999  train Loss: 6.2123  val_loss: 2.8695  val_score: 2.8711  lr: 0.00100  elapsed: 53m 5s\n",
      "Epoch 342 / 19999  train Loss: 6.1266  val_loss: 2.9341  val_score: 2.9352  lr: 0.00100  elapsed: 53m 14s\n",
      "Epoch 343 / 19999  train Loss: 6.1276  val_loss: 3.0509  val_score: 3.0507  lr: 0.00100  elapsed: 53m 24s\n",
      "Epoch 344 / 19999  train Loss: 6.1218  val_loss: 2.8270  val_score: 2.8276  lr: 0.00100  elapsed: 53m 33s\n",
      "----------------------------------------------------------------------->> loss improved to 2.82764\n",
      "Epoch 345 / 19999  train Loss: 6.1393  val_loss: 2.9067  val_score: 2.9085  lr: 0.00100  elapsed: 53m 43s\n",
      "Epoch 346 / 19999  train Loss: 6.1779  val_loss: 3.2484  val_score: 3.2507  lr: 0.00100  elapsed: 53m 52s\n",
      "Epoch 347 / 19999  train Loss: 6.1584  val_loss: 2.9175  val_score: 2.9183  lr: 0.00100  elapsed: 54m 1s\n",
      "Epoch 348 / 19999  train Loss: 6.1463  val_loss: 2.8545  val_score: 2.8555  lr: 0.00100  elapsed: 54m 11s\n",
      "Epoch 349 / 19999  train Loss: 6.1014  val_loss: 2.9539  val_score: 2.9539  lr: 0.00100  elapsed: 54m 20s\n",
      "Epoch 350 / 19999  train Loss: 6.0997  val_loss: 3.1085  val_score: 3.1090  lr: 0.00100  elapsed: 54m 30s\n",
      "Epoch 351 / 19999  train Loss: 6.0668  val_loss: 2.9901  val_score: 2.9910  lr: 0.00100  elapsed: 54m 39s\n",
      "Epoch 352 / 19999  train Loss: 6.1616  val_loss: 2.9785  val_score: 2.9788  lr: 0.00100  elapsed: 54m 48s\n",
      "Epoch 353 / 19999  train Loss: 6.1469  val_loss: 3.0949  val_score: 3.0945  lr: 0.00100  elapsed: 54m 58s\n",
      "Epoch 354 / 19999  train Loss: 6.1788  val_loss: 2.8787  val_score: 2.8794  lr: 0.00100  elapsed: 55m 7s\n",
      "Epoch 355 / 19999  train Loss: 6.0697  val_loss: 2.9433  val_score: 2.9442  lr: 0.00100  elapsed: 55m 16s\n",
      "Epoch 356 / 19999  train Loss: 6.0978  val_loss: 2.8947  val_score: 2.8955  lr: 0.00100  elapsed: 55m 26s\n",
      "Epoch 357 / 19999  train Loss: 6.1104  val_loss: 2.9273  val_score: 2.9276  lr: 0.00100  elapsed: 55m 35s\n",
      "Epoch 358 / 19999  train Loss: 6.0831  val_loss: 2.9865  val_score: 2.9867  lr: 0.00100  elapsed: 55m 45s\n",
      "Epoch 359 / 19999  train Loss: 6.1229  val_loss: 3.0937  val_score: 3.0944  lr: 0.00100  elapsed: 55m 54s\n",
      "Epoch 360 / 19999  train Loss: 6.0467  val_loss: 2.9066  val_score: 2.9065  lr: 0.00100  elapsed: 56m 3s\n",
      "Epoch 361 / 19999  train Loss: 6.0603  val_loss: 2.8671  val_score: 2.8675  lr: 0.00100  elapsed: 56m 13s\n",
      "Epoch 362 / 19999  train Loss: 6.1144  val_loss: 2.8985  val_score: 2.8999  lr: 0.00100  elapsed: 56m 22s\n",
      "Epoch 363 / 19999  train Loss: 6.0874  val_loss: 3.0226  val_score: 3.0239  lr: 0.00100  elapsed: 56m 31s\n",
      "Epoch 364 / 19999  train Loss: 6.0992  val_loss: 2.9130  val_score: 2.9146  lr: 0.00100  elapsed: 56m 41s\n",
      "Epoch 365 / 19999  train Loss: 6.1073  val_loss: 3.1250  val_score: 3.1263  lr: 0.00100  elapsed: 56m 50s\n",
      "Epoch 366 / 19999  train Loss: 6.0342  val_loss: 2.7254  val_score: 2.7257  lr: 0.00100  elapsed: 56m 59s\n",
      "----------------------------------------------------------------------->> loss improved to 2.72574\n",
      "Epoch 367 / 19999  train Loss: 6.0446  val_loss: 2.7092  val_score: 2.7102  lr: 0.00100  elapsed: 57m 9s\n",
      "----------------------------------------------------------------------->> loss improved to 2.71017\n",
      "Epoch 368 / 19999  train Loss: 6.0014  val_loss: 2.9597  val_score: 2.9606  lr: 0.00100  elapsed: 57m 18s\n",
      "Epoch 369 / 19999  train Loss: 6.1271  val_loss: 2.9423  val_score: 2.9433  lr: 0.00100  elapsed: 57m 28s\n",
      "Epoch 370 / 19999  train Loss: 6.0750  val_loss: 2.8258  val_score: 2.8259  lr: 0.00100  elapsed: 57m 37s\n",
      "Epoch 371 / 19999  train Loss: 6.0792  val_loss: 2.7007  val_score: 2.7015  lr: 0.00100  elapsed: 57m 46s\n",
      "----------------------------------------------------------------------->> loss improved to 2.70149\n",
      "Epoch 372 / 19999  train Loss: 5.9958  val_loss: 2.7988  val_score: 2.7995  lr: 0.00100  elapsed: 57m 55s\n",
      "Epoch 373 / 19999  train Loss: 6.0034  val_loss: 2.8952  val_score: 2.8938  lr: 0.00100  elapsed: 58m 5s\n",
      "Epoch 374 / 19999  train Loss: 6.0829  val_loss: 2.7771  val_score: 2.7784  lr: 0.00100  elapsed: 58m 14s\n",
      "Epoch 375 / 19999  train Loss: 6.0663  val_loss: 3.0487  val_score: 3.0498  lr: 0.00100  elapsed: 58m 23s\n",
      "Epoch 376 / 19999  train Loss: 6.0670  val_loss: 2.8808  val_score: 2.8820  lr: 0.00100  elapsed: 58m 33s\n",
      "Epoch 377 / 19999  train Loss: 6.0295  val_loss: 3.0317  val_score: 3.0330  lr: 0.00100  elapsed: 58m 42s\n",
      "Epoch 378 / 19999  train Loss: 6.0489  val_loss: 2.8442  val_score: 2.8448  lr: 0.00100  elapsed: 58m 51s\n",
      "Epoch 379 / 19999  train Loss: 5.9508  val_loss: 2.8387  val_score: 2.8394  lr: 0.00100  elapsed: 59m 1s\n",
      "Epoch 380 / 19999  train Loss: 5.9541  val_loss: 2.9239  val_score: 2.9241  lr: 0.00100  elapsed: 59m 10s\n",
      "Epoch 381 / 19999  train Loss: 6.0275  val_loss: 2.8657  val_score: 2.8658  lr: 0.00100  elapsed: 59m 19s\n",
      "Epoch 382 / 19999  train Loss: 6.0226  val_loss: 2.6385  val_score: 2.6393  lr: 0.00100  elapsed: 59m 29s\n",
      "----------------------------------------------------------------------->> loss improved to 2.63931\n",
      "Epoch 383 / 19999  train Loss: 6.0156  val_loss: 2.9353  val_score: 2.9352  lr: 0.00100  elapsed: 59m 38s\n",
      "Epoch 384 / 19999  train Loss: 6.0199  val_loss: 2.8376  val_score: 2.8383  lr: 0.00100  elapsed: 59m 47s\n",
      "Epoch 385 / 19999  train Loss: 6.0016  val_loss: 2.6730  val_score: 2.6745  lr: 0.00100  elapsed: 59m 57s\n",
      "Epoch 386 / 19999  train Loss: 6.0381  val_loss: 2.8569  val_score: 2.8577  lr: 0.00100  elapsed: 60m 6s\n",
      "Epoch 387 / 19999  train Loss: 5.9869  val_loss: 2.8065  val_score: 2.8077  lr: 0.00100  elapsed: 60m 16s\n",
      "Epoch 388 / 19999  train Loss: 5.9730  val_loss: 2.7291  val_score: 2.7305  lr: 0.00100  elapsed: 60m 25s\n",
      "Epoch 389 / 19999  train Loss: 5.9587  val_loss: 2.7685  val_score: 2.7705  lr: 0.00100  elapsed: 60m 34s\n",
      "Epoch 390 / 19999  train Loss: 6.0011  val_loss: 2.9387  val_score: 2.9412  lr: 0.00100  elapsed: 60m 44s\n",
      "Epoch 391 / 19999  train Loss: 6.0916  val_loss: 2.9252  val_score: 2.9260  lr: 0.00100  elapsed: 60m 53s\n",
      "Epoch 392 / 19999  train Loss: 5.9616  val_loss: 2.9987  val_score: 2.9990  lr: 0.00100  elapsed: 61m 2s\n",
      "Epoch 393 / 19999  train Loss: 6.0019  val_loss: 2.8255  val_score: 2.8264  lr: 0.00100  elapsed: 61m 12s\n",
      "Epoch 394 / 19999  train Loss: 5.9772  val_loss: 2.7077  val_score: 2.7088  lr: 0.00100  elapsed: 61m 21s\n",
      "Epoch 395 / 19999  train Loss: 6.0437  val_loss: 2.8215  val_score: 2.8234  lr: 0.00100  elapsed: 61m 30s\n",
      "Epoch 396 / 19999  train Loss: 5.9122  val_loss: 2.5911  val_score: 2.5909  lr: 0.00100  elapsed: 61m 40s\n",
      "----------------------------------------------------------------------->> loss improved to 2.59087\n",
      "Epoch 397 / 19999  train Loss: 5.9513  val_loss: 2.7415  val_score: 2.7428  lr: 0.00100  elapsed: 61m 49s\n",
      "Epoch 398 / 19999  train Loss: 5.9705  val_loss: 2.8265  val_score: 2.8275  lr: 0.00100  elapsed: 61m 58s\n",
      "Epoch 399 / 19999  train Loss: 5.9671  val_loss: 2.7792  val_score: 2.7789  lr: 0.00100  elapsed: 62m 8s\n",
      "Epoch 400 / 19999  train Loss: 5.9330  val_loss: 2.8474  val_score: 2.8484  lr: 0.00100  elapsed: 62m 17s\n",
      "Epoch 401 / 19999  train Loss: 6.0331  val_loss: 2.9109  val_score: 2.9114  lr: 0.00100  elapsed: 62m 26s\n",
      "Epoch 402 / 19999  train Loss: 5.9122  val_loss: 2.8825  val_score: 2.8825  lr: 0.00100  elapsed: 62m 35s\n",
      "Epoch 403 / 19999  train Loss: 5.9299  val_loss: 2.7487  val_score: 2.7473  lr: 0.00100  elapsed: 62m 44s\n",
      "Epoch 404 / 19999  train Loss: 5.9472  val_loss: 2.8578  val_score: 2.8585  lr: 0.00100  elapsed: 62m 54s\n",
      "Epoch 405 / 19999  train Loss: 5.8811  val_loss: 2.7274  val_score: 2.7270  lr: 0.00100  elapsed: 63m 3s\n",
      "Epoch 406 / 19999  train Loss: 6.0210  val_loss: 2.9645  val_score: 2.9620  lr: 0.00100  elapsed: 63m 12s\n",
      "Epoch 407 / 19999  train Loss: 5.9788  val_loss: 2.8582  val_score: 2.8572  lr: 0.00100  elapsed: 63m 22s\n",
      "Epoch 408 / 19999  train Loss: 5.9267  val_loss: 2.7440  val_score: 2.7447  lr: 0.00100  elapsed: 63m 31s\n",
      "Epoch 409 / 19999  train Loss: 6.0150  val_loss: 2.8833  val_score: 2.8842  lr: 0.00100  elapsed: 63m 40s\n",
      "Epoch 410 / 19999  train Loss: 5.9105  val_loss: 2.8155  val_score: 2.8140  lr: 0.00100  elapsed: 63m 50s\n",
      "Epoch 411 / 19999  train Loss: 5.9187  val_loss: 2.8011  val_score: 2.8011  lr: 0.00100  elapsed: 63m 59s\n",
      "Epoch 412 / 19999  train Loss: 5.8959  val_loss: 2.7229  val_score: 2.7234  lr: 0.00100  elapsed: 64m 8s\n",
      "Epoch 413 / 19999  train Loss: 5.9161  val_loss: 2.6338  val_score: 2.6344  lr: 0.00100  elapsed: 64m 18s\n",
      "Epoch 414 / 19999  train Loss: 5.8779  val_loss: 2.7695  val_score: 2.7705  lr: 0.00100  elapsed: 64m 27s\n",
      "Epoch 415 / 19999  train Loss: 5.8768  val_loss: 2.9475  val_score: 2.9487  lr: 0.00100  elapsed: 64m 37s\n",
      "Epoch 416 / 19999  train Loss: 5.8903  val_loss: 2.7013  val_score: 2.7024  lr: 0.00100  elapsed: 64m 46s\n",
      "Epoch 417 / 19999  train Loss: 5.9124  val_loss: 2.8692  val_score: 2.8707  lr: 0.00100  elapsed: 64m 55s\n",
      "Epoch 418 / 19999  train Loss: 5.8840  val_loss: 2.7474  val_score: 2.7481  lr: 0.00100  elapsed: 65m 5s\n",
      "Epoch 419 / 19999  train Loss: 5.9178  val_loss: 2.8860  val_score: 2.8854  lr: 0.00100  elapsed: 65m 14s\n",
      "Epoch 420 / 19999  train Loss: 5.9056  val_loss: 2.7318  val_score: 2.7313  lr: 0.00100  elapsed: 65m 23s\n",
      "Epoch 421 / 19999  train Loss: 5.8454  val_loss: 2.7603  val_score: 2.7611  lr: 0.00100  elapsed: 65m 33s\n",
      "Epoch 422 / 19999  train Loss: 5.8798  val_loss: 2.7376  val_score: 2.7381  lr: 0.00100  elapsed: 65m 42s\n",
      "Epoch 423 / 19999  train Loss: 5.9434  val_loss: 2.9586  val_score: 2.9588  lr: 0.00100  elapsed: 65m 51s\n",
      "Epoch 424 / 19999  train Loss: 5.9367  val_loss: 2.7782  val_score: 2.7795  lr: 0.00100  elapsed: 66m 1s\n",
      "Epoch 425 / 19999  train Loss: 5.8869  val_loss: 2.9043  val_score: 2.9051  lr: 0.00100  elapsed: 66m 10s\n",
      "Epoch 426 / 19999  train Loss: 5.8485  val_loss: 2.6236  val_score: 2.6230  lr: 0.00100  elapsed: 66m 19s\n",
      "Epoch 427 / 19999  train Loss: 5.8306  val_loss: 2.6239  val_score: 2.6242  lr: 0.00100  elapsed: 66m 29s\n",
      "Epoch 428 / 19999  train Loss: 5.8716  val_loss: 2.6729  val_score: 2.6741  lr: 0.00100  elapsed: 66m 38s\n",
      "Epoch 429 / 19999  train Loss: 5.8503  val_loss: 2.6906  val_score: 2.6912  lr: 0.00100  elapsed: 66m 47s\n",
      "Epoch 430 / 19999  train Loss: 5.8591  val_loss: 2.6260  val_score: 2.6268  lr: 0.00100  elapsed: 66m 56s\n",
      "Epoch 431 / 19999  train Loss: 5.8135  val_loss: 2.6123  val_score: 2.6127  lr: 0.00100  elapsed: 67m 5s\n",
      "Epoch 432 / 19999  train Loss: 5.8582  val_loss: 2.7601  val_score: 2.7608  lr: 0.00100  elapsed: 67m 15s\n",
      "Epoch 433 / 19999  train Loss: 5.8327  val_loss: 2.6560  val_score: 2.6574  lr: 0.00100  elapsed: 67m 24s\n",
      "Epoch 434 / 19999  train Loss: 5.8897  val_loss: 2.7126  val_score: 2.7130  lr: 0.00100  elapsed: 67m 34s\n",
      "Epoch 435 / 19999  train Loss: 5.8558  val_loss: 2.6712  val_score: 2.6719  lr: 0.00100  elapsed: 67m 43s\n",
      "Epoch 436 / 19999  train Loss: 5.8308  val_loss: 2.6307  val_score: 2.6295  lr: 0.00100  elapsed: 67m 52s\n",
      "Epoch 437 / 19999  train Loss: 5.8484  val_loss: 2.9107  val_score: 2.9111  lr: 0.00100  elapsed: 68m 2s\n",
      "Epoch 438 / 19999  train Loss: 5.8111  val_loss: 2.6620  val_score: 2.6634  lr: 0.00100  elapsed: 68m 11s\n",
      "Epoch 439 / 19999  train Loss: 5.7857  val_loss: 2.6723  val_score: 2.6733  lr: 0.00100  elapsed: 68m 21s\n",
      "Epoch 440 / 19999  train Loss: 5.8223  val_loss: 2.7523  val_score: 2.7528  lr: 0.00100  elapsed: 68m 30s\n",
      "Epoch 441 / 19999  train Loss: 5.8797  val_loss: 2.7215  val_score: 2.7219  lr: 0.00100  elapsed: 68m 39s\n",
      "Epoch 442 / 19999  train Loss: 5.8760  val_loss: 2.7974  val_score: 2.7991  lr: 0.00100  elapsed: 68m 48s\n",
      "Epoch 443 / 19999  train Loss: 5.8534  val_loss: 2.7439  val_score: 2.7447  lr: 0.00100  elapsed: 68m 58s\n",
      "Epoch 444 / 19999  train Loss: 5.8010  val_loss: 2.7214  val_score: 2.7228  lr: 0.00100  elapsed: 69m 7s\n",
      "Epoch 445 / 19999  train Loss: 5.8362  val_loss: 2.6792  val_score: 2.6790  lr: 0.00100  elapsed: 69m 16s\n",
      "Epoch 446 / 19999  train Loss: 5.7693  val_loss: 2.6060  val_score: 2.6061  lr: 0.00100  elapsed: 69m 26s\n",
      "Epoch 447 / 19999  train Loss: 5.7941  val_loss: 2.7743  val_score: 2.7749  lr: 0.00100  elapsed: 69m 35s\n",
      "Epoch 448 / 19999  train Loss: 5.8322  val_loss: 2.5606  val_score: 2.5612  lr: 0.00100  elapsed: 69m 44s\n",
      "----------------------------------------------------------------------->> loss improved to 2.56122\n",
      "Epoch 449 / 19999  train Loss: 5.7524  val_loss: 2.5867  val_score: 2.5872  lr: 0.00100  elapsed: 69m 54s\n",
      "Epoch 450 / 19999  train Loss: 5.7898  val_loss: 2.7490  val_score: 2.7498  lr: 0.00100  elapsed: 70m 3s\n",
      "Epoch 451 / 19999  train Loss: 5.8553  val_loss: 2.6920  val_score: 2.6919  lr: 0.00100  elapsed: 70m 12s\n",
      "Epoch 452 / 19999  train Loss: 5.8176  val_loss: 2.6751  val_score: 2.6752  lr: 0.00100  elapsed: 70m 22s\n",
      "Epoch 453 / 19999  train Loss: 5.8328  val_loss: 2.6842  val_score: 2.6846  lr: 0.00100  elapsed: 70m 31s\n",
      "Epoch 454 / 19999  train Loss: 5.8777  val_loss: 2.6891  val_score: 2.6901  lr: 0.00100  elapsed: 70m 40s\n",
      "Epoch 455 / 19999  train Loss: 5.8566  val_loss: 2.6969  val_score: 2.6976  lr: 0.00100  elapsed: 70m 50s\n",
      "Epoch 456 / 19999  train Loss: 5.7404  val_loss: 2.6417  val_score: 2.6424  lr: 0.00100  elapsed: 70m 59s\n",
      "Epoch 457 / 19999  train Loss: 5.8005  val_loss: 2.9136  val_score: 2.9122  lr: 0.00100  elapsed: 71m 8s\n",
      "Epoch 458 / 19999  train Loss: 5.7286  val_loss: 2.6263  val_score: 2.6253  lr: 0.00100  elapsed: 71m 18s\n",
      "Epoch 459 / 19999  train Loss: 5.7970  val_loss: 2.5751  val_score: 2.5752  lr: 0.00100  elapsed: 71m 27s\n",
      "Epoch 460 / 19999  train Loss: 5.7022  val_loss: 2.5890  val_score: 2.5894  lr: 0.00100  elapsed: 71m 36s\n",
      "Epoch 461 / 19999  train Loss: 5.7588  val_loss: 2.6247  val_score: 2.6246  lr: 0.00100  elapsed: 71m 46s\n",
      "Epoch 462 / 19999  train Loss: 5.7987  val_loss: 2.5205  val_score: 2.5210  lr: 0.00100  elapsed: 71m 55s\n",
      "----------------------------------------------------------------------->> loss improved to 2.52099\n",
      "Epoch 463 / 19999  train Loss: 5.7815  val_loss: 2.6330  val_score: 2.6332  lr: 0.00100  elapsed: 72m 4s\n",
      "Epoch 464 / 19999  train Loss: 5.7593  val_loss: 2.5942  val_score: 2.5954  lr: 0.00100  elapsed: 72m 13s\n",
      "Epoch 465 / 19999  train Loss: 5.7742  val_loss: 2.8263  val_score: 2.8260  lr: 0.00100  elapsed: 72m 23s\n",
      "Epoch 466 / 19999  train Loss: 5.8016  val_loss: 2.8364  val_score: 2.8362  lr: 0.00100  elapsed: 72m 32s\n",
      "Epoch 467 / 19999  train Loss: 5.7497  val_loss: 2.5094  val_score: 2.5100  lr: 0.00100  elapsed: 72m 41s\n",
      "----------------------------------------------------------------------->> loss improved to 2.50999\n",
      "Epoch 468 / 19999  train Loss: 5.7392  val_loss: 2.7292  val_score: 2.7297  lr: 0.00100  elapsed: 72m 51s\n",
      "Epoch 469 / 19999  train Loss: 5.7542  val_loss: 2.6880  val_score: 2.6885  lr: 0.00100  elapsed: 73m 0s\n",
      "Epoch 470 / 19999  train Loss: 5.7676  val_loss: 2.9074  val_score: 2.9073  lr: 0.00100  elapsed: 73m 9s\n",
      "Epoch 471 / 19999  train Loss: 5.7714  val_loss: 2.6242  val_score: 2.6237  lr: 0.00100  elapsed: 73m 19s\n",
      "Epoch 472 / 19999  train Loss: 5.7257  val_loss: 2.5769  val_score: 2.5774  lr: 0.00100  elapsed: 73m 28s\n",
      "Epoch 473 / 19999  train Loss: 5.6943  val_loss: 2.6224  val_score: 2.6223  lr: 0.00100  elapsed: 73m 38s\n",
      "Epoch 474 / 19999  train Loss: 5.7154  val_loss: 3.2993  val_score: 3.2991  lr: 0.00100  elapsed: 73m 47s\n",
      "Epoch 475 / 19999  train Loss: 5.7783  val_loss: 2.6784  val_score: 2.6784  lr: 0.00100  elapsed: 73m 56s\n",
      "Epoch 476 / 19999  train Loss: 5.7643  val_loss: 2.6626  val_score: 2.6616  lr: 0.00100  elapsed: 74m 5s\n",
      "Epoch 477 / 19999  train Loss: 5.7472  val_loss: 2.6903  val_score: 2.6902  lr: 0.00100  elapsed: 74m 15s\n",
      "Epoch 478 / 19999  train Loss: 5.7509  val_loss: 2.5454  val_score: 2.5455  lr: 0.00100  elapsed: 74m 24s\n",
      "Epoch 479 / 19999  train Loss: 5.8245  val_loss: 2.6686  val_score: 2.6691  lr: 0.00100  elapsed: 74m 33s\n",
      "Epoch 480 / 19999  train Loss: 5.6830  val_loss: 2.7228  val_score: 2.7227  lr: 0.00100  elapsed: 74m 43s\n",
      "Epoch 481 / 19999  train Loss: 5.7193  val_loss: 2.6375  val_score: 2.6377  lr: 0.00100  elapsed: 74m 52s\n",
      "Epoch 482 / 19999  train Loss: 5.7287  val_loss: 2.6853  val_score: 2.6866  lr: 0.00100  elapsed: 75m 1s\n",
      "Epoch 483 / 19999  train Loss: 5.7211  val_loss: 2.5611  val_score: 2.5616  lr: 0.00100  elapsed: 75m 11s\n",
      "Epoch 484 / 19999  train Loss: 5.7012  val_loss: 2.6339  val_score: 2.6346  lr: 0.00100  elapsed: 75m 20s\n",
      "Epoch 485 / 19999  train Loss: 5.7341  val_loss: 2.6566  val_score: 2.6571  lr: 0.00100  elapsed: 75m 29s\n",
      "Epoch 486 / 19999  train Loss: 5.7136  val_loss: 2.6010  val_score: 2.6021  lr: 0.00100  elapsed: 75m 39s\n",
      "Epoch 487 / 19999  train Loss: 5.7098  val_loss: 2.5602  val_score: 2.5602  lr: 0.00100  elapsed: 75m 48s\n",
      "Epoch 488 / 19999  train Loss: 5.6926  val_loss: 2.6252  val_score: 2.6256  lr: 0.00100  elapsed: 75m 57s\n",
      "Epoch 489 / 19999  train Loss: 5.7531  val_loss: 2.7364  val_score: 2.7370  lr: 0.00100  elapsed: 76m 7s\n",
      "Epoch 490 / 19999  train Loss: 5.6658  val_loss: 2.5434  val_score: 2.5425  lr: 0.00100  elapsed: 76m 16s\n",
      "Epoch 491 / 19999  train Loss: 5.8743  val_loss: 2.6859  val_score: 2.6863  lr: 0.00100  elapsed: 76m 25s\n",
      "Epoch 492 / 19999  train Loss: 5.7086  val_loss: 2.6438  val_score: 2.6438  lr: 0.00100  elapsed: 76m 35s\n",
      "Epoch 493 / 19999  train Loss: 5.7015  val_loss: 2.5897  val_score: 2.5908  lr: 0.00100  elapsed: 76m 44s\n",
      "Epoch 494 / 19999  train Loss: 5.6764  val_loss: 2.6466  val_score: 2.6460  lr: 0.00100  elapsed: 76m 53s\n",
      "Epoch 495 / 19999  train Loss: 5.6656  val_loss: 2.5711  val_score: 2.5713  lr: 0.00100  elapsed: 77m 3s\n",
      "Epoch 496 / 19999  train Loss: 5.6527  val_loss: 2.4893  val_score: 2.4894  lr: 0.00100  elapsed: 77m 12s\n",
      "----------------------------------------------------------------------->> loss improved to 2.48935\n",
      "Epoch 497 / 19999  train Loss: 5.6939  val_loss: 2.6161  val_score: 2.6170  lr: 0.00100  elapsed: 77m 21s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e7ca0f0a4500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4bd1270bfba2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# output path\n",
    "#output_dir = Path('./', 'output')\n",
    "#output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "num_epochs = 20000\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "best_epoch_list = []\n",
    "best_valid_score_list = []\n",
    "\n",
    "# build model\n",
    "model = build_model(device, model_name='mlp')\n",
    "model.to(device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = AdamW(model.parameters(), lr)\n",
    "###################  Scheduler ################\n",
    "#eta_min = 0.000001\n",
    "#T_max = 10\n",
    "#T_mult = 1\n",
    "#restart_decay = 0.97\n",
    "#scheduler = CosineAnnealingWithRestartsLR(optimizer, T_max=T_max, eta_min=eta_min, T_mult=T_mult, restart_decay=restart_decay)\n",
    "###############################################\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "best_epoch = 0\n",
    "best_train_loss = 1000\n",
    "best_valid_score = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "        else:\n",
    "            target = target.float()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    val_loss, val_score = validation(model, criterion, valid_loader, device)\n",
    "\n",
    "    elapsed = time() - start_time\n",
    "\n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "\n",
    "    #scheduler.step(val_score)\n",
    "    \n",
    "    print('Epoch {} / {}  train Loss: {:.4f}  val_loss: {:.4f}  val_score: {:.4f}  lr: {:.5f}  elapsed: {:.0f}m {:.0f}s' \\\n",
    "          .format(epoch,  num_epochs - 1, train_loss, val_loss, val_score, lr[0], elapsed // 60, elapsed % 60))\n",
    "        \n",
    "    #model_path = output_dir / 'best_model.pt'\n",
    "    model_path = '../wafer/mlp_weights/mlp_v2.pt'\n",
    "\n",
    "    if val_score < best_valid_score:\n",
    "        best_valid_score = val_score\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print('----------------------------------------------------------------------->> loss improved to {:.5f}'.format(best_valid_score))\n",
    "\n",
    "    best_epoch_list.append(best_epoch)\n",
    "    best_valid_score_list.append(best_valid_score)\n",
    "print(\"==================== mlp - Best val_loss - {:.5f} =================\".format(best_valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_df = pd.DataFrame()\n",
    "epoch_df['epoch'] = best_epoch_list\n",
    "epoch_df['val_score'] = best_valid_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>3545</td>\n",
       "      <td>1.213821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>3545</td>\n",
       "      <td>1.213821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3573</th>\n",
       "      <td>3545</td>\n",
       "      <td>1.213821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>3545</td>\n",
       "      <td>1.213821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3571</th>\n",
       "      <td>3545</td>\n",
       "      <td>1.213821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch  val_score\n",
       "3610   3545   1.213821\n",
       "3574   3545   1.213821\n",
       "3573   3545   1.213821\n",
       "3572   3545   1.213821\n",
       "3571   3545   1.213821"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_df.sort_values('val_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.213821"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_to = round(min(best_valid_score_list),6)\n",
    "score_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no label\n",
      "CPU times: user 125 ms, sys: 364 ms, total: 489 ms\n",
      "Wall time: 539 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 2048\n",
    "test_loader = build_dataloader(test_df.iloc[:, 1:].values, Y=None, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = build_model(device, model_name='mlp')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_preds = np.zeros((len(test_loader.dataset), 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "        outputs = model(data)\n",
    "        test_preds[batch_idx * batch_size:(batch_idx+1) * batch_size] = outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>255.138824</td>\n",
       "      <td>227.893280</td>\n",
       "      <td>133.118896</td>\n",
       "      <td>85.314796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>159.078415</td>\n",
       "      <td>126.125671</td>\n",
       "      <td>235.977890</td>\n",
       "      <td>99.262184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>147.385742</td>\n",
       "      <td>179.830536</td>\n",
       "      <td>271.033752</td>\n",
       "      <td>156.604706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>92.139008</td>\n",
       "      <td>228.100906</td>\n",
       "      <td>189.203613</td>\n",
       "      <td>83.059128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>272.181763</td>\n",
       "      <td>299.291992</td>\n",
       "      <td>245.998352</td>\n",
       "      <td>269.601746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     layer_1     layer_2     layer_3     layer_4\n",
       "0   0  255.138824  227.893280  133.118896   85.314796\n",
       "1   1  159.078415  126.125671  235.977890   99.262184\n",
       "2   2  147.385742  179.830536  271.033752  156.604706\n",
       "3   3   92.139008  228.100906  189.203613   83.059128\n",
       "4   4  272.181763  299.291992  245.998352  269.601746"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': submission_df['id'],\n",
    "                           'layer_1':test_preds.transpose()[0],\n",
    "                           'layer_2':test_preds.transpose()[1],\n",
    "                           'layer_3':test_preds.transpose()[2],\n",
    "                           'layer_4':test_preds.transpose()[3]})\n",
    "submission.to_csv('../wafer/mlp_submission/mlp_v2_3610e_{}_submission.csv'.format(score_to), index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python37564bitpytorchconda133dde54c45c40c2946593d30b593426"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
