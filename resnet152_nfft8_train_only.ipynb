{
 "cells": [
  {
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#from torch_fun.dataloader import build_dataloader\n",
    "#from torch_fun.model import build_model\n",
    "#from torch_fun.utils import count_parameters, seed_everything, AdamW, CosineAnnealingWithRestartsLR\n",
    "from torch import cuda\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 1
  },
  {
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def get_spectrogram_feature(data):\n",
    "    # data shape -> batch * dim\n",
    "\n",
    "    stft = torch.stft(torch.FloatTensor(data),\n",
    "                        8,\n",
    "                        center=False,\n",
    "                        normalized=False,\n",
    "                        onesided=True)\n",
    "\n",
    "    stft = (stft[:,:,0].pow(2) + stft[:,:,1].pow(2)).pow(0.5)\n",
    "    amag = stft.numpy()\n",
    "    feat = torch.FloatTensor(amag)\n",
    "    feat = torch.unsqueeze(feat,dim=0)\n",
    "#     feat = torch.FloatTensor(feat).transpose(1, -1)\n",
    "\n",
    "    return feat\n",
    "\n",
    "class Semi_dataset(Dataset):\n",
    "    def __init__(self, data_frame):\n",
    "        \n",
    "        self.data_list = list()\n",
    "        for data in data_frame[[str(x) for x in range(226)]].values:\n",
    "            self.data_list.append(get_spectrogram_feature(data))\n",
    "        data_frame.drop(columns=[str(x) for x in range(226)], inplace=True)\n",
    "        self.df = data_frame\n",
    "        try:\n",
    "            self.label = data_frame[['layer_' + str(x) for x in range(1, 5)]].values\n",
    "        except:\n",
    "            print('This dataframe does not have target value')\n",
    "            self.label = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         data = get_spectrogram_feature(self.df.iloc[index][[str(x) for x in range(226)]].values)\n",
    "        data = self.data_list[index]\n",
    "\n",
    "        if self.label is None:\n",
    "            return data\n",
    "        else:\n",
    "            target = torch.tensor(self.label[index, :])\n",
    "            return data, target\n",
    "\n",
    "\n",
    "def build_dataloader(data_frame, batch_size, shuffle):\n",
    "    dataset = Semi_dataset(data_frame)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=0\n",
    "                            )\n",
    "    return dataloader"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 2
  },
  {
   "source": [
    "from torchvision import models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, device, model_name='efficient', weight_path=None):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.weight_path = weight_path\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        ) \n",
    "                \n",
    "        if model_name == 'efficient':\n",
    "            self.backbone = EfficientNet.from_pretrained('efficientnet-b0', num_classes=1)\n",
    "            self.backbone.requires_grad = True\n",
    "            in_features = self.backbone._fc.in_features\n",
    "            self.backbone._fc = nn.Sequential(\n",
    "                nn.Linear(in_features=in_features, out_features=256, bias=True),\n",
    "                nn.BatchNorm1d(num_features=256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=256, out_features=4, bias=True),\n",
    "            )\n",
    "    def loss(self, pred, label):\n",
    "        loss = self.criterion(pred, label)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_img, target=None):\n",
    "        input_img = input_img.to(self.device)\n",
    "        \n",
    "        if target is not None:\n",
    "            target = target.to(self.device)\n",
    "        \n",
    "        x = self.first_layer(input_img)\n",
    "        print(1)\n",
    "        pred = self.backbone(x)\n",
    "        print(2)\n",
    "        pred = x.float()\n",
    "        loss = self.loss(pred, target)\n",
    "        if self.training:\n",
    "            return pred, loss\n",
    "        else:\n",
    "            return pred, loss\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=hidden_size, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, out_size, 4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.net(image).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        )\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(512, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(self.first_layer(x)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnet50(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        ) \n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(self.first_layer(x)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "class Resnet152(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        ) \n",
    "        model = models.resnet152(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(self.first_layer(x)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnext50(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnext101(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        model = models.resnext101_32x8d(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.1))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        \n",
    "def build_model(device, model_name='efficient', weight_path=None):\n",
    "    if model_name == 'efficient':\n",
    "        model = Model(device, model_name, weight_path)\n",
    "    elif model_name == 'resnet50':\n",
    "        model = Resnet50(4, False)\n",
    "    elif model_name == 'resnet18':\n",
    "        model = Resnet18(4, False)\n",
    "    elif model_name == 'resnet152':\n",
    "        model = Resnet152(4, False)\n",
    "    model.to(device)\n",
    "    return model"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 3
  },
  {
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    '''\n",
    "    Count of trainable weights in a model\n",
    "    '''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Implements AdamW algorithm.\n",
    "\n",
    "    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "    .. Fixing Weight Decay Regularization in Adam:\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # according to the paper, this penalty should come after the bias correction\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class CosineAnnealingWithRestartsLR(_LRScheduler):\n",
    "    '''\n",
    "    SGDR\\: Stochastic Gradient Descent with Warm Restarts: https://arxiv.org/abs/1608.03983\n",
    "    code: https://github.com/gurucharanmk/PyTorch_CosineAnnealingWithRestartsLR/blob/master/CosineAnnealingWithRestartsLR.py\n",
    "    added restart_decay value to decrease lr for every restarts\n",
    "    '''\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, T_mult=1, restart_decay=0.95):\n",
    "        self.T_max = T_max\n",
    "        self.T_mult = T_mult\n",
    "        self.next_restart = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.restarts = 0\n",
    "        self.last_restart = 0\n",
    "        self.T_num = 0\n",
    "        self.restart_decay = restart_decay\n",
    "        super(CosineAnnealingWithRestartsLR,self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        self.Tcur = self.last_epoch - self.last_restart\n",
    "        if self.Tcur >= self.next_restart:\n",
    "            self.next_restart *= self.T_mult\n",
    "            self.last_restart = self.last_epoch\n",
    "            self.T_num += 1\n",
    "        learning_rate = [(self.eta_min + ((base_lr)*self.restart_decay**self.T_num - self.eta_min) * (1 + math.cos(math.pi * self.Tcur / self.next_restart)) / 2) for base_lr in self.base_lrs]\n",
    "        return learning_rate"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 4
  },
  {
   "source": [
    "DEBUG = False\n",
    "\n",
    "DATASET_PATH = '../wafer'\n",
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "submission = pd.read_csv(os.path.join(DATASET_PATH, 'sample_submission.csv'))\n",
    "\n",
    "if DEBUG:\n",
    "    train_df = train_df[:1000]"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 5
  },
  {
   "source": [
    "%%time\n",
    "######## Scale\n",
    "scaler = StandardScaler()\n",
    "train_df.iloc[:,4:] = scaler.fit_transform(train_df.iloc[:,4:])\n",
    "test_df.iloc[:,1:] = scaler.fit_transform(test_df.iloc[:,1:])"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 1min 31s, sys: 5.34 s, total: 1min 37s\nWall time: 15.7 s\n"
    }
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "source": [
    "# hyper parameter\n",
    "lr = 2.5e-4 / 4\n",
    "start_epoch = 0\n",
    "num_epochs = 30000\n",
    "best_loss = 99999999\n",
    "loss_list = []\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "test_loader = build_dataloader(test_df, batch_size, False)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "This dataframe does not have target value\n"
    }
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "source": [
    "if cuda.is_available:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 8
  },
  {
   "source": [
    "#valid_len = int(len(train_df)*0.1)\n",
    "\n",
    "#X_train = train_df[valid_len:]\n",
    "#X_valid = train_df[:valid_len]"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 9
  },
  {
   "source": [
    "#train_loader = build_dataloader(X_train, batch_size, True)\n",
    "#valid_loader = build_dataloader(X_valid, batch_size, False)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 10
  },
  {
   "source": [
    "# build model\n",
    "#model = build_model(device, model_name='resnet')\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "#criterion = nn.L1Loss()"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 11
  },
  {
   "source": [
    "# output path\n",
    "#output_dir = Path('./', 'output')\n",
    "#output_dir.mkdir(exist_ok=True, parents=True)\n",
    "#model_path = output_dir / 'model.pt'"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 12
  },
  {
   "source": [
    "# load train model\n",
    "# load_model_path = Path('../input/daconsemimodel/model (1).pt')\n",
    "\n",
    "# model.load_state_dict(torch.load(load_model_path))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 13
  },
  {
   "source": [
    "def validation(model, criterion, valid_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), 4))\n",
    "    valid_targets = np.zeros((len(valid_loader.dataset), 4))\n",
    "    val_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(valid_loader):\n",
    "            \n",
    "            valid_targets[i * batch_size: (i+1) * batch_size] = target.float().numpy().copy()\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "                \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_preds[i * batch_size: (i+1) * batch_size] = output.detach().cpu().numpy()\n",
    "            \n",
    "            val_loss += loss.item() / len(valid_loader)\n",
    "        \n",
    "    val_score = mean_absolute_error(valid_preds, valid_targets)\n",
    "    \n",
    "    return val_loss, val_score   "
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 14
  },
  {
   "source": [
    "#model = models.vgg16(pretrained=False)\n",
    "#first_layer = nn.Conv2d(1, 3, (3, 1))\n",
    "#features = list(model.features)\n",
    "#features.insert(0, nn.Conv2d(1, 3, (3, 1)))\n",
    "#model.features = nn.Sequential(*features)\n",
    "#num_features = model.classifier[6].in_features # last layer's in_feature\n",
    "#features = list(model.classifier.children())[:-1] # remove last layer\n",
    "#features.extend([nn.Linear(num_features, 4)]) # Add new layer with 4 outputs\n",
    "#model.classifier = nn.Sequential(*features)\n",
    "#model"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 15
  },
  {
   "source": [
    "seed = 42\n",
    "#n_splits = 5\n",
    "seed_everything(seed)\n",
    "\n",
    "#x_tr, x_val = train_df.iloc[:, :], train_df.iloc[val_index, :]\n",
    "\n",
    "# build model\n",
    "\n",
    "model = build_model(device, model_name='resnet152')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_loader = build_dataloader(train_df, batch_size, True)\n",
    "#valid_loader = build_dataloader(x_val, batch_size, False)\n",
    "\n",
    "#best_loss = 99999999\n",
    "start_time = time()\n",
    "\n",
    "best_epoch = 0\n",
    "best_train_loss = 1000\n",
    "#best_valid_score = 1000\n",
    "model_path = '../wafer/resnet152_weights/resnet152_nfft8_34e_train_only.pt'\n",
    "best_epoch_list = []\n",
    "best_train_score_list = []"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 16
  },
  {
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "        else:\n",
    "            target = target.float()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print('----------------------------------------------------------------------->> loss improved to {:.5f}'.format(best_train_loss))\n",
    "\n",
    "    #val_loss, val_score = validation(model, criterion, valid_loader, device)\n",
    "\n",
    "    #if val_loss < best_loss:\n",
    "    #    best_loss = val_loss\n",
    "    #    torch.save(model.state_dict(), \"F{}_nfft8_resnet18_model.pt\".format(fold_num))\n",
    "    #    print(\">> score improved..! \")\n",
    "\n",
    "    elapsed = time() - start_time\n",
    "    \n",
    "    lr = [_['lr'] for _ in optimizer.param_groups]\n",
    "    \n",
    "    print('Epoch {} / {}  train Loss: {:.4f}  lr: {:.5f}  elapsed: {:.0f}m {:.0f}s' \\\n",
    "          .format(epoch,  num_epochs - 1, train_loss, lr[0], elapsed // 60, elapsed % 60))\n",
    "    #print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    best_epoch_list.append(best_epoch)\n",
    "    best_train_score_list.append(best_train_loss)\n",
    "print(\"==================== Resnet18 - Best train_loss - {:.5f} =================\".format(best_train_loss))"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------------->> loss improved to 1.41867\nEpoch 0 / 29999  train Loss: 1.4187  lr: 0.00006  elapsed: 9m 9s\n----------------------------------------------------------------------->> loss improved to 1.41684\nEpoch 1 / 29999  train Loss: 1.4168  lr: 0.00006  elapsed: 18m 18s\nEpoch 2 / 29999  train Loss: 1.4339  lr: 0.00006  elapsed: 27m 26s\n----------------------------------------------------------------------->> loss improved to 1.35815\nEpoch 3 / 29999  train Loss: 1.3582  lr: 0.00006  elapsed: 36m 34s\nEpoch 4 / 29999  train Loss: 1.3754  lr: 0.00006  elapsed: 45m 42s\n----------------------------------------------------------------------->> loss improved to 1.29128\nEpoch 5 / 29999  train Loss: 1.2913  lr: 0.00006  elapsed: 54m 51s\nEpoch 6 / 29999  train Loss: 1.3365  lr: 0.00006  elapsed: 63m 58s\nEpoch 7 / 29999  train Loss: 1.3278  lr: 0.00006  elapsed: 73m 5s\nEpoch 8 / 29999  train Loss: 1.4734  lr: 0.00006  elapsed: 82m 12s\n----------------------------------------------------------------------->> loss improved to 1.26082\nEpoch 9 / 29999  train Loss: 1.2608  lr: 0.00006  elapsed: 91m 21s\nEpoch 10 / 29999  train Loss: 1.2703  lr: 0.00006  elapsed: 100m 29s\nEpoch 11 / 29999  train Loss: 1.3001  lr: 0.00006  elapsed: 109m 36s\nEpoch 12 / 29999  train Loss: 1.3335  lr: 0.00006  elapsed: 118m 42s\nEpoch 13 / 29999  train Loss: 1.3755  lr: 0.00006  elapsed: 127m 48s\nEpoch 14 / 29999  train Loss: 1.2809  lr: 0.00006  elapsed: 136m 54s\nEpoch 15 / 29999  train Loss: 1.3151  lr: 0.00006  elapsed: 146m 1s\nEpoch 16 / 29999  train Loss: 1.2951  lr: 0.00006  elapsed: 155m 7s\nEpoch 17 / 29999  train Loss: 1.4417  lr: 0.00006  elapsed: 164m 13s\nEpoch 18 / 29999  train Loss: 1.3989  lr: 0.00006  elapsed: 173m 20s\n----------------------------------------------------------------------->> loss improved to 1.20838\nEpoch 19 / 29999  train Loss: 1.2084  lr: 0.00006  elapsed: 182m 28s\nEpoch 20 / 29999  train Loss: 1.2860  lr: 0.00006  elapsed: 191m 33s\nEpoch 21 / 29999  train Loss: 1.2913  lr: 0.00006  elapsed: 200m 40s\nEpoch 22 / 29999  train Loss: 1.2589  lr: 0.00006  elapsed: 209m 46s\nEpoch 23 / 29999  train Loss: 1.2565  lr: 0.00006  elapsed: 218m 52s\nEpoch 24 / 29999  train Loss: 1.3157  lr: 0.00006  elapsed: 227m 59s\nEpoch 25 / 29999  train Loss: 1.2623  lr: 0.00006  elapsed: 237m 4s\nEpoch 26 / 29999  train Loss: 1.2611  lr: 0.00006  elapsed: 246m 10s\nEpoch 27 / 29999  train Loss: 1.3745  lr: 0.00006  elapsed: 255m 17s\nEpoch 28 / 29999  train Loss: 1.2377  lr: 0.00006  elapsed: 264m 24s\nEpoch 29 / 29999  train Loss: 1.2709  lr: 0.00006  elapsed: 273m 29s\nEpoch 30 / 29999  train Loss: 1.3220  lr: 0.00006  elapsed: 282m 35s\nEpoch 31 / 29999  train Loss: 1.3270  lr: 0.00006  elapsed: 291m 42s\nEpoch 32 / 29999  train Loss: 1.2534  lr: 0.00006  elapsed: 300m 48s\nEpoch 33 / 29999  train Loss: 1.3341  lr: 0.00006  elapsed: 309m 54s\nEpoch 34 / 29999  train Loss: 1.2755  lr: 0.00006  elapsed: 319m 0s\nEpoch 35 / 29999  train Loss: 1.3212  lr: 0.00006  elapsed: 328m 7s\nEpoch 36 / 29999  train Loss: 1.2829  lr: 0.00006  elapsed: 337m 13s\nEpoch 37 / 29999  train Loss: 1.3023  lr: 0.00006  elapsed: 346m 19s\nEpoch 38 / 29999  train Loss: 1.3390  lr: 0.00006  elapsed: 355m 25s\nEpoch 39 / 29999  train Loss: 1.3040  lr: 0.00006  elapsed: 364m 32s\nEpoch 40 / 29999  train Loss: 1.3085  lr: 0.00006  elapsed: 373m 37s\nEpoch 41 / 29999  train Loss: 1.2505  lr: 0.00006  elapsed: 382m 44s\nEpoch 42 / 29999  train Loss: 1.3472  lr: 0.00006  elapsed: 391m 50s\nEpoch 43 / 29999  train Loss: 1.3863  lr: 0.00006  elapsed: 400m 56s\n----------------------------------------------------------------------->> loss improved to 1.18778\nEpoch 44 / 29999  train Loss: 1.1878  lr: 0.00006  elapsed: 410m 5s\nEpoch 45 / 29999  train Loss: 1.2773  lr: 0.00006  elapsed: 419m 10s\nEpoch 46 / 29999  train Loss: 1.3224  lr: 0.00006  elapsed: 428m 16s\nEpoch 47 / 29999  train Loss: 1.2169  lr: 0.00006  elapsed: 437m 22s\nEpoch 48 / 29999  train Loss: 1.3004  lr: 0.00006  elapsed: 446m 29s\nEpoch 49 / 29999  train Loss: 1.3427  lr: 0.00006  elapsed: 455m 35s\nEpoch 50 / 29999  train Loss: 1.2347  lr: 0.00006  elapsed: 464m 41s\nEpoch 51 / 29999  train Loss: 1.2884  lr: 0.00006  elapsed: 473m 47s\nEpoch 52 / 29999  train Loss: 1.3049  lr: 0.00006  elapsed: 482m 53s\nEpoch 53 / 29999  train Loss: 1.2529  lr: 0.00006  elapsed: 491m 59s\nEpoch 54 / 29999  train Loss: 1.4557  lr: 0.00006  elapsed: 501m 5s\n----------------------------------------------------------------------->> loss improved to 1.16175\nEpoch 55 / 29999  train Loss: 1.1617  lr: 0.00006  elapsed: 510m 13s\nEpoch 56 / 29999  train Loss: 1.3013  lr: 0.00006  elapsed: 519m 19s\nEpoch 57 / 29999  train Loss: 1.3273  lr: 0.00006  elapsed: 528m 26s\nEpoch 58 / 29999  train Loss: 1.3124  lr: 0.00006  elapsed: 537m 32s\nEpoch 59 / 29999  train Loss: 1.2223  lr: 0.00006  elapsed: 546m 39s\nEpoch 60 / 29999  train Loss: 1.2658  lr: 0.00006  elapsed: 555m 47s\nEpoch 61 / 29999  train Loss: 1.3062  lr: 0.00006  elapsed: 564m 54s\nEpoch 62 / 29999  train Loss: 1.2470  lr: 0.00006  elapsed: 574m 1s\nEpoch 63 / 29999  train Loss: 1.2667  lr: 0.00006  elapsed: 583m 8s\nEpoch 64 / 29999  train Loss: 1.3696  lr: 0.00006  elapsed: 592m 16s\nEpoch 65 / 29999  train Loss: 1.2330  lr: 0.00006  elapsed: 601m 23s\nEpoch 66 / 29999  train Loss: 1.2675  lr: 0.00006  elapsed: 610m 30s\nEpoch 67 / 29999  train Loss: 1.2915  lr: 0.00006  elapsed: 619m 37s\nEpoch 68 / 29999  train Loss: 1.3024  lr: 0.00006  elapsed: 628m 44s\nEpoch 69 / 29999  train Loss: 1.2779  lr: 0.00006  elapsed: 637m 52s\nEpoch 70 / 29999  train Loss: 1.2487  lr: 0.00006  elapsed: 646m 59s\nEpoch 71 / 29999  train Loss: 1.3356  lr: 0.00006  elapsed: 656m 6s\nEpoch 72 / 29999  train Loss: 1.3041  lr: 0.00006  elapsed: 665m 13s\nEpoch 73 / 29999  train Loss: 1.2914  lr: 0.00006  elapsed: 674m 21s\nEpoch 74 / 29999  train Loss: 1.2436  lr: 0.00006  elapsed: 683m 28s\nEpoch 75 / 29999  train Loss: 1.2376  lr: 0.00006  elapsed: 692m 36s\nEpoch 76 / 29999  train Loss: 1.3615  lr: 0.00006  elapsed: 701m 43s\nEpoch 77 / 29999  train Loss: 1.2890  lr: 0.00006  elapsed: 710m 50s\nEpoch 78 / 29999  train Loss: 1.2949  lr: 0.00006  elapsed: 719m 57s\nEpoch 79 / 29999  train Loss: 1.2993  lr: 0.00006  elapsed: 729m 5s\nEpoch 80 / 29999  train Loss: 1.3347  lr: 0.00006  elapsed: 738m 12s\nEpoch 81 / 29999  train Loss: 1.2932  lr: 0.00006  elapsed: 747m 19s\nEpoch 82 / 29999  train Loss: 1.2461  lr: 0.00006  elapsed: 756m 27s\nEpoch 83 / 29999  train Loss: 1.2964  lr: 0.00006  elapsed: 765m 34s\nEpoch 84 / 29999  train Loss: 1.2516  lr: 0.00006  elapsed: 774m 41s\nEpoch 85 / 29999  train Loss: 1.2805  lr: 0.00006  elapsed: 783m 48s\nEpoch 86 / 29999  train Loss: 1.3047  lr: 0.00006  elapsed: 792m 56s\nEpoch 87 / 29999  train Loss: 1.2005  lr: 0.00006  elapsed: 802m 3s\nEpoch 88 / 29999  train Loss: 1.3643  lr: 0.00006  elapsed: 811m 10s\nEpoch 89 / 29999  train Loss: 1.2705  lr: 0.00006  elapsed: 820m 16s\nEpoch 90 / 29999  train Loss: 1.2526  lr: 0.00006  elapsed: 829m 23s\nEpoch 91 / 29999  train Loss: 1.1955  lr: 0.00006  elapsed: 838m 31s\nEpoch 92 / 29999  train Loss: 1.3690  lr: 0.00006  elapsed: 847m 38s\nEpoch 93 / 29999  train Loss: 1.2676  lr: 0.00006  elapsed: 856m 45s\nEpoch 94 / 29999  train Loss: 1.2501  lr: 0.00006  elapsed: 865m 52s\nEpoch 95 / 29999  train Loss: 1.2457  lr: 0.00006  elapsed: 874m 60s\nEpoch 96 / 29999  train Loss: 1.2951  lr: 0.00006  elapsed: 884m 7s\nEpoch 97 / 29999  train Loss: 1.2648  lr: 0.00006  elapsed: 893m 14s\nEpoch 98 / 29999  train Loss: 1.3038  lr: 0.00006  elapsed: 902m 21s\nEpoch 99 / 29999  train Loss: 1.2146  lr: 0.00006  elapsed: 911m 29s\nEpoch 100 / 29999  train Loss: 1.2898  lr: 0.00006  elapsed: 920m 35s\nEpoch 101 / 29999  train Loss: 1.3481  lr: 0.00006  elapsed: 929m 43s\nEpoch 102 / 29999  train Loss: 1.3916  lr: 0.00006  elapsed: 938m 50s\nEpoch 103 / 29999  train Loss: 1.2362  lr: 0.00006  elapsed: 947m 57s\n----------------------------------------------------------------------->> loss improved to 1.15546\nEpoch 104 / 29999  train Loss: 1.1555  lr: 0.00006  elapsed: 957m 7s\nEpoch 105 / 29999  train Loss: 1.3330  lr: 0.00006  elapsed: 966m 13s\nEpoch 106 / 29999  train Loss: 1.2429  lr: 0.00006  elapsed: 975m 20s\nEpoch 107 / 29999  train Loss: 1.2515  lr: 0.00006  elapsed: 984m 25s\nEpoch 108 / 29999  train Loss: 1.2847  lr: 0.00006  elapsed: 993m 32s\nEpoch 109 / 29999  train Loss: 1.3184  lr: 0.00006  elapsed: 1002m 40s\nEpoch 110 / 29999  train Loss: 1.3280  lr: 0.00006  elapsed: 1011m 46s\nEpoch 111 / 29999  train Loss: 1.1718  lr: 0.00006  elapsed: 1020m 54s\nEpoch 112 / 29999  train Loss: 1.2860  lr: 0.00006  elapsed: 1030m 1s\nEpoch 113 / 29999  train Loss: 1.2735  lr: 0.00006  elapsed: 1039m 7s\nEpoch 114 / 29999  train Loss: 1.2444  lr: 0.00006  elapsed: 1048m 15s\nEpoch 115 / 29999  train Loss: 1.1631  lr: 0.00006  elapsed: 1057m 22s\nEpoch 116 / 29999  train Loss: 1.3135  lr: 0.00006  elapsed: 1066m 29s\nEpoch 117 / 29999  train Loss: 1.2640  lr: 0.00006  elapsed: 1075m 36s\nEpoch 118 / 29999  train Loss: 1.3225  lr: 0.00006  elapsed: 1084m 43s\nEpoch 119 / 29999  train Loss: 1.2964  lr: 0.00006  elapsed: 1093m 51s\nEpoch 120 / 29999  train Loss: 1.2458  lr: 0.00006  elapsed: 1102m 57s\nEpoch 121 / 29999  train Loss: 1.2466  lr: 0.00006  elapsed: 1112m 4s\nEpoch 122 / 29999  train Loss: 1.2440  lr: 0.00006  elapsed: 1121m 10s\nEpoch 123 / 29999  train Loss: 1.2839  lr: 0.00006  elapsed: 1130m 17s\nEpoch 124 / 29999  train Loss: 1.2461  lr: 0.00006  elapsed: 1139m 25s\nEpoch 125 / 29999  train Loss: 1.3434  lr: 0.00006  elapsed: 1148m 32s\nEpoch 126 / 29999  train Loss: 1.2009  lr: 0.00006  elapsed: 1157m 39s\nEpoch 127 / 29999  train Loss: 1.3260  lr: 0.00006  elapsed: 1166m 46s\nEpoch 128 / 29999  train Loss: 1.2638  lr: 0.00006  elapsed: 1175m 53s\nEpoch 129 / 29999  train Loss: 1.3377  lr: 0.00006  elapsed: 1185m 1s\nEpoch 130 / 29999  train Loss: 1.2963  lr: 0.00006  elapsed: 1194m 8s\nEpoch 131 / 29999  train Loss: 1.1994  lr: 0.00006  elapsed: 1203m 15s\nEpoch 132 / 29999  train Loss: 1.2500  lr: 0.00006  elapsed: 1212m 22s\nEpoch 133 / 29999  train Loss: 1.3026  lr: 0.00006  elapsed: 1221m 29s\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7c9b5f9b6bd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {},
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "epoch_df = pd.DataFrame()\n",
    "epoch_df['epoch'] = best_epoch_list\n",
    "epoch_df['train_loss'] = best_train_score_list"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 18
  },
  {
   "source": [
    "epoch_df.sort_values('train_loss').head()"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    epoch  train_loss\n36     34    1.363878\n35     34    1.363878\n34     34    1.363878\n33     32    1.382772\n32     32    1.382772",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epoch</th>\n      <th>train_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>34</td>\n      <td>1.363878</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>34</td>\n      <td>1.363878</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>34</td>\n      <td>1.363878</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>32</td>\n      <td>1.382772</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>1.382772</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {},
   "execution_count": 19
  },
  {
   "source": [
    "score_to = round(min(best_train_score_list),6)\n",
    "score_to"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.363878"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {},
   "execution_count": 21
  },
  {
   "source": [
    "%%time\n",
    "batch_size = 1024\n",
    "#test_loader = build_dataloader(test_df.iloc[:, 1:].values, Y=None, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = build_model(device, model_name='resnet152')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_preds = np.zeros((len(test_loader.dataset), 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        if device:\n",
    "            data = data.to(device)\n",
    "        outputs = model(data)\n",
    "        test_preds[batch_idx * batch_size:(batch_idx+1) * batch_size] = outputs.detach().cpu().numpy()"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 4.13 s, sys: 244 ms, total: 4.38 s\nWall time: 2.48 s\n"
    }
   ],
   "metadata": {},
   "execution_count": 24
  },
  {
   "source": [
    "submission = pd.DataFrame({'id': test_df['id'],\n",
    "                           'layer_1':test_preds.transpose()[0],\n",
    "                           'layer_2':test_preds.transpose()[1],\n",
    "                           'layer_3':test_preds.transpose()[2],\n",
    "                           'layer_4':test_preds.transpose()[3]})\n",
    "submission.to_csv('../wafer/resnet152_submission/resnet152_nfft8_train_only_{}.csv'.format(score_to), index=False)\n",
    "\n",
    "submission.head()"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id     layer_1     layer_2     layer_3     layer_4\n0   0  253.157059  230.475861  131.713440   86.181999\n1   1  158.094223  127.168747  235.380234   98.242493\n2   2  148.412140  177.574493  273.701019  155.741150\n3   3   91.902367  228.991211  188.543716   82.892853\n4   4  272.687286  293.107208  245.112518  270.573212",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>layer_1</th>\n      <th>layer_2</th>\n      <th>layer_3</th>\n      <th>layer_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>253.157059</td>\n      <td>230.475861</td>\n      <td>131.713440</td>\n      <td>86.181999</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>158.094223</td>\n      <td>127.168747</td>\n      <td>235.380234</td>\n      <td>98.242493</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>148.412140</td>\n      <td>177.574493</td>\n      <td>273.701019</td>\n      <td>155.741150</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>91.902367</td>\n      <td>228.991211</td>\n      <td>188.543716</td>\n      <td>82.892853</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>272.687286</td>\n      <td>293.107208</td>\n      <td>245.112518</td>\n      <td>270.573212</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {},
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference #########\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "predictions = np.zeros((len(test_loader.dataset),4))\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        predictions[i*batch_size: (i+1)*batch_size] = output.detach().cpu().numpy()\n",
    "print('prediction value check: ', output[0])\n",
    "np.savetxt('../wafer/resnet18_submission/resnet18_nfft8_train_only.csv'.format(fold_num), predictions, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(n_splits):\n",
    "    fold_num = str(fold+1)\n",
    "    # inference #########\n",
    "    model.load_state_dict(torch.load(\"F{}_nfft8_resnet18_model.pt\".format(fold_num)))\n",
    "    model.eval()\n",
    "\n",
    "    predictions = np.zeros((len(test_loader.dataset),4))\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions[i*batch_size: (i+1)*batch_size] = output.detach().cpu().numpy()\n",
    "    print('prediction value check: ', output[0])\n",
    "    print(predictions.shape)\n",
    "    np.savetxt('../wafer/resnet18_submission/F{}_nfft8_resnet18.csv'.format(fold_num), predictions, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}