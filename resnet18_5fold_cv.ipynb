{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#from torch_fun.dataloader import build_dataloader\n",
    "#from torch_fun.model import build_model\n",
    "#from torch_fun.utils import count_parameters, seed_everything, AdamW, CosineAnnealingWithRestartsLR\n",
    "from torch import cuda\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def get_spectrogram_feature(data):\n",
    "    # data shape -> batch * dim\n",
    "\n",
    "    stft = torch.stft(torch.FloatTensor(data),\n",
    "                        64,\n",
    "                        center=False,\n",
    "                        normalized=False,\n",
    "                        onesided=True)\n",
    "\n",
    "    stft = (stft[:,:,0].pow(2) + stft[:,:,1].pow(2)).pow(0.5)\n",
    "    amag = stft.numpy()\n",
    "    feat = torch.FloatTensor(amag)\n",
    "    feat = torch.unsqueeze(feat,dim=0)\n",
    "#     feat = torch.FloatTensor(feat).transpose(1, -1)\n",
    "\n",
    "    return feat\n",
    "\n",
    "class Semi_dataset(Dataset):\n",
    "    def __init__(self, data_frame):\n",
    "        \n",
    "        self.data_list = list()\n",
    "        for data in data_frame[[str(x) for x in range(226)]].values:\n",
    "            self.data_list.append(get_spectrogram_feature(data))\n",
    "        data_frame.drop(columns=[str(x) for x in range(226)], inplace=True)\n",
    "        self.df = data_frame\n",
    "        try:\n",
    "            self.label = data_frame[['layer_' + str(x) for x in range(1, 5)]].values\n",
    "        except:\n",
    "            print('This dataframe does not have target value')\n",
    "            self.label = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         data = get_spectrogram_feature(self.df.iloc[index][[str(x) for x in range(226)]].values)\n",
    "        data = self.data_list[index]\n",
    "\n",
    "        if self.label is None:\n",
    "            return data\n",
    "        else:\n",
    "            target = torch.tensor(self.label[index, :])\n",
    "            return data, target\n",
    "\n",
    "\n",
    "def build_dataloader(data_frame, batch_size, shuffle):\n",
    "    dataset = Semi_dataset(data_frame)\n",
    "    dataloader = DataLoader(\n",
    "                            dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            num_workers=1\n",
    "                            )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, device, model_name='efficient', weight_path=None):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.weight_path = weight_path\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        ) \n",
    "                \n",
    "        if model_name == 'efficient':\n",
    "            self.backbone = EfficientNet.from_pretrained('efficientnet-b0', num_classes=1)\n",
    "            self.backbone.requires_grad = True\n",
    "            in_features = self.backbone._fc.in_features\n",
    "            self.backbone._fc = nn.Sequential(\n",
    "                nn.Linear(in_features=in_features, out_features=256, bias=True),\n",
    "                nn.BatchNorm1d(num_features=256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=256, out_features=4, bias=True),\n",
    "            )\n",
    "    def loss(self, pred, label):\n",
    "        loss = self.criterion(pred, label)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, input_img, target=None):\n",
    "        input_img = input_img.to(self.device)\n",
    "        \n",
    "        if target is not None:\n",
    "            target = target.to(self.device)\n",
    "        \n",
    "        x = self.first_layer(input_img)\n",
    "        print(1)\n",
    "        pred = self.backbone(x)\n",
    "        print(2)\n",
    "        pred = x.float()\n",
    "        loss = self.loss(pred, target)\n",
    "        if self.training:\n",
    "            return pred, loss\n",
    "        else:\n",
    "            return pred, loss\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=hidden_size, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, hidden_size, 3, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.Conv2d(hidden_size, out_size, 4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.net(image).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        )\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(512, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(self.first_layer(x)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnet50(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, (3, 1))\n",
    "        ) \n",
    "        model = models.resnet50(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(self.first_layer(x)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnext50(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "class Resnext101(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=False):\n",
    "        super().__init__()\n",
    "        model = models.resnext101_32x8d(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(0.1))\n",
    "        model.append(nn.Conv2d(2048, num_classes, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        \n",
    "def build_model(device, model_name='efficient', weight_path=None):\n",
    "    if model_name == 'efficient':\n",
    "        model = Model(device, model_name, weight_path)\n",
    "    elif model_name == 'resnet50':\n",
    "        model = Resnet50(4, False)\n",
    "    elif model_name == 'resnet18':\n",
    "        model = Resnet18(4, False)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    '''\n",
    "    Count of trainable weights in a model\n",
    "    '''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"Implements AdamW algorithm.\n",
    "\n",
    "    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "    .. Fixing Weight Decay Regularization in Adam:\n",
    "    https://arxiv.org/abs/1711.05101\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # according to the paper, this penalty should come after the bias correction\n",
    "                # if group['weight_decay'] != 0:\n",
    "                #     grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class CosineAnnealingWithRestartsLR(_LRScheduler):\n",
    "    '''\n",
    "    SGDR\\: Stochastic Gradient Descent with Warm Restarts: https://arxiv.org/abs/1608.03983\n",
    "    code: https://github.com/gurucharanmk/PyTorch_CosineAnnealingWithRestartsLR/blob/master/CosineAnnealingWithRestartsLR.py\n",
    "    added restart_decay value to decrease lr for every restarts\n",
    "    '''\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, T_mult=1, restart_decay=0.95):\n",
    "        self.T_max = T_max\n",
    "        self.T_mult = T_mult\n",
    "        self.next_restart = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.restarts = 0\n",
    "        self.last_restart = 0\n",
    "        self.T_num = 0\n",
    "        self.restart_decay = restart_decay\n",
    "        super(CosineAnnealingWithRestartsLR,self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        self.Tcur = self.last_epoch - self.last_restart\n",
    "        if self.Tcur >= self.next_restart:\n",
    "            self.next_restart *= self.T_mult\n",
    "            self.last_restart = self.last_epoch\n",
    "            self.T_num += 1\n",
    "        learning_rate = [(self.eta_min + ((base_lr)*self.restart_decay**self.T_num - self.eta_min) * (1 + math.cos(math.pi * self.Tcur / self.next_restart)) / 2) for base_lr in self.base_lrs]\n",
    "        return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "DATASET_PATH = '../wafer'\n",
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
    "submission = pd.read_csv(os.path.join(DATASET_PATH, 'sample_submission.csv'))\n",
    "\n",
    "if DEBUG:\n",
    "    train_df = train_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataframe does not have target value\n"
     ]
    }
   ],
   "source": [
    "# hyper parameter\n",
    "lr = 2.5e-4 / 4\n",
    "start_epoch = 0\n",
    "num_epochs = 300\n",
    "best_loss = 99999999\n",
    "loss_list = []\n",
    "\n",
    "batch_size = 4096\n",
    "\n",
    "test_loader = build_dataloader(test_df, batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda.is_available:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_len = int(len(train_df)*0.1)\n",
    "\n",
    "#X_train = train_df[valid_len:]\n",
    "#X_valid = train_df[:valid_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = build_dataloader(X_train, batch_size, True)\n",
    "#valid_loader = build_dataloader(X_valid, batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "#model = build_model(device, model_name='resnet')\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "#criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output path\n",
    "#output_dir = Path('./', 'output')\n",
    "#output_dir.mkdir(exist_ok=True, parents=True)\n",
    "#model_path = output_dir / 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train model\n",
    "# load_model_path = Path('../input/daconsemimodel/model (1).pt')\n",
    "\n",
    "# model.load_state_dict(torch.load(load_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, valid_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), 4))\n",
    "    valid_targets = np.zeros((len(valid_loader.dataset), 4))\n",
    "    val_loss = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(valid_loader):\n",
    "            \n",
    "            valid_targets[i * batch_size: (i+1) * batch_size] = target.float().numpy().copy()\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "                \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_preds[i * batch_size: (i+1) * batch_size] = output.detach().cpu().numpy()\n",
    "            \n",
    "            val_loss += loss.item() / len(valid_loader)\n",
    "        \n",
    "    val_score = mean_absolute_error(valid_preds, valid_targets)\n",
    "    \n",
    "    return val_loss, val_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.vgg16(pretrained=False)\n",
    "#first_layer = nn.Conv2d(1, 3, (3, 1))\n",
    "#features = list(model.features)\n",
    "#features.insert(0, nn.Conv2d(1, 3, (3, 1)))\n",
    "#model.features = nn.Sequential(*features)\n",
    "#num_features = model.classifier[6].in_features # last layer's in_feature\n",
    "#features = list(model.classifier.children())[:-1] # remove last layer\n",
    "#features.extend([nn.Linear(num_features, 4)]) # Add new layer with 4 outputs\n",
    "#model.classifier = nn.Sequential(*features)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poch 83 / 299\n",
      "train_loss- 3.03693  valid_loss- 3.55686  valid_score- 3.55640 in 0m 58s\n",
      "Epoch 84 / 299\n",
      "train_loss- 3.01816  valid_loss- 3.71492  valid_score- 3.71627 in 0m 58s\n",
      "Epoch 85 / 299\n",
      "train_loss- 3.00799  valid_loss- 3.61259  valid_score- 3.61435 in 0m 58s\n",
      "Epoch 86 / 299\n",
      ">> score improved..! \n",
      "train_loss- 3.01856  valid_loss- 3.39369  valid_score- 3.39486 in 0m 58s\n",
      "Epoch 87 / 299\n",
      "train_loss- 2.99334  valid_loss- 3.44890  valid_score- 3.44925 in 0m 58s\n",
      "Epoch 88 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.95925  valid_loss- 3.37899  valid_score- 3.37978 in 0m 58s\n",
      "Epoch 89 / 299\n",
      "train_loss- 2.96219  valid_loss- 3.42090  valid_score- 3.42194 in 0m 58s\n",
      "Epoch 90 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.91357  valid_loss- 3.27436  valid_score- 3.27506 in 0m 58s\n",
      "Epoch 91 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.88959  valid_loss- 3.17475  valid_score- 3.17500 in 0m 58s\n",
      "Epoch 92 / 299\n",
      "train_loss- 2.87902  valid_loss- 3.36849  valid_score- 3.36973 in 0m 58s\n",
      "Epoch 93 / 299\n",
      "train_loss- 2.80112  valid_loss- 3.38193  valid_score- 3.38171 in 0m 58s\n",
      "Epoch 94 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.85870  valid_loss- 3.12402  valid_score- 3.12433 in 0m 58s\n",
      "Epoch 95 / 299\n",
      "train_loss- 2.79046  valid_loss- 3.26314  valid_score- 3.26507 in 0m 58s\n",
      "Epoch 96 / 299\n",
      "train_loss- 2.87067  valid_loss- 3.22346  valid_score- 3.22471 in 0m 58s\n",
      "Epoch 97 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.78291  valid_loss- 2.99170  valid_score- 2.99250 in 0m 58s\n",
      "Epoch 98 / 299\n",
      "train_loss- 2.74613  valid_loss- 3.21320  valid_score- 3.21468 in 0m 58s\n",
      "Epoch 99 / 299\n",
      "train_loss- 2.75252  valid_loss- 3.36346  valid_score- 3.36385 in 0m 58s\n",
      "Epoch 100 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.73441  valid_loss- 2.98000  valid_score- 2.98118 in 0m 58s\n",
      "Epoch 101 / 299\n",
      "train_loss- 2.67603  valid_loss- 3.01496  valid_score- 3.01591 in 0m 58s\n",
      "Epoch 102 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.72945  valid_loss- 2.97439  valid_score- 2.97589 in 0m 58s\n",
      "Epoch 103 / 299\n",
      "train_loss- 2.65528  valid_loss- 3.24383  valid_score- 3.24589 in 0m 58s\n",
      "Epoch 104 / 299\n",
      "train_loss- 2.69580  valid_loss- 3.23718  valid_score- 3.23636 in 0m 58s\n",
      "Epoch 105 / 299\n",
      "train_loss- 2.66855  valid_loss- 3.20640  valid_score- 3.20638 in 0m 58s\n",
      "Epoch 106 / 299\n",
      "train_loss- 2.67928  valid_loss- 3.11457  valid_score- 3.11475 in 0m 58s\n",
      "Epoch 107 / 299\n",
      "train_loss- 2.61788  valid_loss- 3.21156  valid_score- 3.21220 in 0m 58s\n",
      "Epoch 108 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.60243  valid_loss- 2.80056  valid_score- 2.80156 in 0m 58s\n",
      "Epoch 109 / 299\n",
      "train_loss- 2.64730  valid_loss- 3.03701  valid_score- 3.03906 in 0m 58s\n",
      "Epoch 110 / 299\n",
      "train_loss- 2.58153  valid_loss- 3.02471  valid_score- 3.02462 in 0m 58s\n",
      "Epoch 111 / 299\n",
      "train_loss- 2.59388  valid_loss- 3.16907  valid_score- 3.16980 in 0m 58s\n",
      "Epoch 112 / 299\n",
      "train_loss- 2.54346  valid_loss- 2.96072  valid_score- 2.96132 in 0m 58s\n",
      "Epoch 113 / 299\n",
      "train_loss- 2.57428  valid_loss- 2.96898  valid_score- 2.97069 in 0m 58s\n",
      "Epoch 114 / 299\n",
      "train_loss- 2.54415  valid_loss- 2.87432  valid_score- 2.87497 in 0m 58s\n",
      "Epoch 115 / 299\n",
      "train_loss- 2.52245  valid_loss- 2.90617  valid_score- 2.90678 in 0m 58s\n",
      "Epoch 116 / 299\n",
      "train_loss- 2.50215  valid_loss- 2.88314  valid_score- 2.88333 in 0m 58s\n",
      "Epoch 117 / 299\n",
      "train_loss- 2.52377  valid_loss- 2.84932  valid_score- 2.85117 in 0m 58s\n",
      "Epoch 118 / 299\n",
      "train_loss- 2.53625  valid_loss- 2.92328  valid_score- 2.92485 in 0m 58s\n",
      "Epoch 119 / 299\n",
      "train_loss- 2.50268  valid_loss- 2.81355  valid_score- 2.81530 in 0m 58s\n",
      "Epoch 120 / 299\n",
      "train_loss- 2.48239  valid_loss- 2.96096  valid_score- 2.96169 in 0m 58s\n",
      "Epoch 121 / 299\n",
      "train_loss- 2.43913  valid_loss- 2.80303  valid_score- 2.80458 in 0m 58s\n",
      "Epoch 122 / 299\n",
      "train_loss- 2.48643  valid_loss- 2.91701  valid_score- 2.91818 in 0m 58s\n",
      "Epoch 123 / 299\n",
      "train_loss- 2.45730  valid_loss- 2.81922  valid_score- 2.81993 in 0m 58s\n",
      "Epoch 124 / 299\n",
      "train_loss- 2.44950  valid_loss- 2.89728  valid_score- 2.89749 in 0m 58s\n",
      "Epoch 125 / 299\n",
      "train_loss- 2.45633  valid_loss- 2.84420  valid_score- 2.84513 in 0m 58s\n",
      "Epoch 126 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.49494  valid_loss- 2.76591  valid_score- 2.76626 in 0m 58s\n",
      "Epoch 127 / 299\n",
      "train_loss- 2.42818  valid_loss- 3.05108  valid_score- 3.05295 in 0m 58s\n",
      "Epoch 128 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.38933  valid_loss- 2.71185  valid_score- 2.71209 in 0m 58s\n",
      "Epoch 129 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.42036  valid_loss- 2.56441  valid_score- 2.56542 in 0m 58s\n",
      "Epoch 130 / 299\n",
      "train_loss- 2.35670  valid_loss- 2.75464  valid_score- 2.75693 in 0m 58s\n",
      "Epoch 131 / 299\n",
      "train_loss- 2.43573  valid_loss- 2.80364  valid_score- 2.80476 in 0m 58s\n",
      "Epoch 132 / 299\n",
      "train_loss- 2.40644  valid_loss- 2.73592  valid_score- 2.73650 in 0m 58s\n",
      "Epoch 133 / 299\n",
      "train_loss- 2.36398  valid_loss- 3.07386  valid_score- 3.07521 in 0m 58s\n",
      "Epoch 134 / 299\n",
      "train_loss- 2.37518  valid_loss- 2.77855  valid_score- 2.77887 in 0m 58s\n",
      "Epoch 135 / 299\n",
      "train_loss- 2.34105  valid_loss- 2.73782  valid_score- 2.73860 in 0m 58s\n",
      "Epoch 136 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.33405  valid_loss- 2.52238  valid_score- 2.52350 in 0m 59s\n",
      "Epoch 137 / 299\n",
      "train_loss- 2.35702  valid_loss- 3.05793  valid_score- 3.05961 in 0m 58s\n",
      "Epoch 138 / 299\n",
      "train_loss- 2.36696  valid_loss- 2.65043  valid_score- 2.65161 in 0m 58s\n",
      "Epoch 139 / 299\n",
      "train_loss- 2.33317  valid_loss- 2.61068  valid_score- 2.61219 in 0m 58s\n",
      "Epoch 140 / 299\n",
      "train_loss- 2.32687  valid_loss- 2.72278  valid_score- 2.72317 in 0m 58s\n",
      "Epoch 141 / 299\n",
      "train_loss- 2.29091  valid_loss- 2.58975  valid_score- 2.59107 in 0m 58s\n",
      "Epoch 142 / 299\n",
      "train_loss- 2.23216  valid_loss- 2.64547  valid_score- 2.64662 in 0m 58s\n",
      "Epoch 143 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.33203  valid_loss- 2.45186  valid_score- 2.45276 in 0m 58s\n",
      "Epoch 144 / 299\n",
      "train_loss- 2.27590  valid_loss- 2.49451  valid_score- 2.49579 in 0m 58s\n",
      "Epoch 145 / 299\n",
      "train_loss- 2.25170  valid_loss- 2.51612  valid_score- 2.51693 in 0m 58s\n",
      "Epoch 146 / 299\n",
      "train_loss- 2.35643  valid_loss- 2.58218  valid_score- 2.58349 in 0m 58s\n",
      "Epoch 147 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.27655  valid_loss- 2.44508  valid_score- 2.44502 in 0m 58s\n",
      "Epoch 148 / 299\n",
      "train_loss- 2.28039  valid_loss- 2.48212  valid_score- 2.48304 in 0m 58s\n",
      "Epoch 149 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.25424  valid_loss- 2.38460  valid_score- 2.38567 in 0m 58s\n",
      "Epoch 150 / 299\n",
      "train_loss- 2.23529  valid_loss- 2.72773  valid_score- 2.72919 in 0m 58s\n",
      "Epoch 151 / 299\n",
      "train_loss- 2.27073  valid_loss- 2.65490  valid_score- 2.65541 in 0m 58s\n",
      "Epoch 152 / 299\n",
      "train_loss- 2.23875  valid_loss- 2.55610  valid_score- 2.55762 in 0m 58s\n",
      "Epoch 153 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.23204  valid_loss- 2.37735  valid_score- 2.37810 in 0m 58s\n",
      "Epoch 154 / 299\n",
      "train_loss- 2.20337  valid_loss- 2.47140  valid_score- 2.47219 in 0m 58s\n",
      "Epoch 155 / 299\n",
      "train_loss- 2.23637  valid_loss- 2.46664  valid_score- 2.46760 in 0m 58s\n",
      "Epoch 156 / 299\n",
      "train_loss- 2.21219  valid_loss- 2.52523  valid_score- 2.52592 in 0m 58s\n",
      "Epoch 157 / 299\n",
      "train_loss- 2.21646  valid_loss- 2.73992  valid_score- 2.74120 in 0m 58s\n",
      "Epoch 158 / 299\n",
      "train_loss- 2.24421  valid_loss- 2.60873  valid_score- 2.60925 in 0m 58s\n",
      "Epoch 159 / 299\n",
      "train_loss- 2.23814  valid_loss- 2.48541  valid_score- 2.48651 in 0m 58s\n",
      "Epoch 160 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.18789  valid_loss- 2.36446  valid_score- 2.36467 in 0m 58s\n",
      "Epoch 161 / 299\n",
      "train_loss- 2.17793  valid_loss- 2.86493  valid_score- 2.86503 in 0m 58s\n",
      "Epoch 162 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.19237  valid_loss- 2.29456  valid_score- 2.29508 in 0m 58s\n",
      "Epoch 163 / 299\n",
      "train_loss- 2.17908  valid_loss- 2.35762  valid_score- 2.35881 in 0m 58s\n",
      "Epoch 164 / 299\n",
      "train_loss- 2.14348  valid_loss- 2.42990  valid_score- 2.43009 in 0m 58s\n",
      "Epoch 165 / 299\n",
      "train_loss- 2.15279  valid_loss- 2.64550  valid_score- 2.64653 in 0m 58s\n",
      "Epoch 166 / 299\n",
      "train_loss- 2.17090  valid_loss- 2.55171  valid_score- 2.55185 in 0m 58s\n",
      "Epoch 167 / 299\n",
      "train_loss- 2.15895  valid_loss- 2.43830  valid_score- 2.43890 in 0m 58s\n",
      "Epoch 168 / 299\n",
      "train_loss- 2.13423  valid_loss- 2.52289  valid_score- 2.52300 in 0m 58s\n",
      "Epoch 169 / 299\n",
      "train_loss- 2.16038  valid_loss- 2.41891  valid_score- 2.42016 in 0m 58s\n",
      "Epoch 170 / 299\n",
      "train_loss- 2.12190  valid_loss- 2.53181  valid_score- 2.53259 in 0m 58s\n",
      "Epoch 171 / 299\n",
      "train_loss- 2.17656  valid_loss- 2.65513  valid_score- 2.65625 in 0m 58s\n",
      "Epoch 172 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.18141  valid_loss- 2.23015  valid_score- 2.23129 in 0m 58s\n",
      "Epoch 173 / 299\n",
      "train_loss- 2.11655  valid_loss- 2.27538  valid_score- 2.27665 in 0m 58s\n",
      "Epoch 174 / 299\n",
      "train_loss- 2.14646  valid_loss- 2.41340  valid_score- 2.41333 in 0m 58s\n",
      "Epoch 175 / 299\n",
      "train_loss- 2.08960  valid_loss- 2.42676  valid_score- 2.42783 in 0m 58s\n",
      "Epoch 176 / 299\n",
      "train_loss- 2.12082  valid_loss- 2.38288  valid_score- 2.38320 in 0m 58s\n",
      "Epoch 177 / 299\n",
      "train_loss- 2.14068  valid_loss- 2.44195  valid_score- 2.44245 in 0m 58s\n",
      "Epoch 178 / 299\n",
      "train_loss- 2.12061  valid_loss- 2.36861  valid_score- 2.36993 in 0m 58s\n",
      "Epoch 179 / 299\n",
      "train_loss- 2.10197  valid_loss- 2.52722  valid_score- 2.52872 in 0m 58s\n",
      "Epoch 180 / 299\n",
      "train_loss- 2.10472  valid_loss- 2.54286  valid_score- 2.54406 in 0m 58s\n",
      "Epoch 181 / 299\n",
      "train_loss- 2.07347  valid_loss- 2.29681  valid_score- 2.29784 in 0m 58s\n",
      "Epoch 182 / 299\n",
      "train_loss- 2.05224  valid_loss- 2.34929  valid_score- 2.34958 in 0m 58s\n",
      "Epoch 183 / 299\n",
      "train_loss- 2.08701  valid_loss- 2.28018  valid_score- 2.28080 in 0m 58s\n",
      "Epoch 184 / 299\n",
      "train_loss- 2.05976  valid_loss- 2.27307  valid_score- 2.27441 in 0m 58s\n",
      "Epoch 185 / 299\n",
      "train_loss- 2.09865  valid_loss- 2.27880  valid_score- 2.28029 in 0m 58s\n",
      "Epoch 186 / 299\n",
      "train_loss- 2.04799  valid_loss- 2.41861  valid_score- 2.41974 in 0m 58s\n",
      "Epoch 187 / 299\n",
      "train_loss- 2.06094  valid_loss- 2.43408  valid_score- 2.43509 in 0m 58s\n",
      "Epoch 188 / 299\n",
      "train_loss- 2.06965  valid_loss- 2.31734  valid_score- 2.31808 in 0m 58s\n",
      "Epoch 189 / 299\n",
      "train_loss- 2.07690  valid_loss- 2.41007  valid_score- 2.41111 in 0m 58s\n",
      "Epoch 190 / 299\n",
      "train_loss- 2.05093  valid_loss- 2.29241  valid_score- 2.29379 in 0m 58s\n",
      "Epoch 191 / 299\n",
      "train_loss- 2.06706  valid_loss- 2.61229  valid_score- 2.61249 in 0m 58s\n",
      "Epoch 192 / 299\n",
      "train_loss- 2.08020  valid_loss- 2.59213  valid_score- 2.59358 in 0m 58s\n",
      "Epoch 193 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.05441  valid_loss- 2.18466  valid_score- 2.18528 in 0m 58s\n",
      "Epoch 194 / 299\n",
      "train_loss- 2.02335  valid_loss- 2.29807  valid_score- 2.29912 in 0m 58s\n",
      "Epoch 195 / 299\n",
      "train_loss- 2.01436  valid_loss- 2.24490  valid_score- 2.24549 in 0m 58s\n",
      "Epoch 196 / 299\n",
      "train_loss- 2.02710  valid_loss- 2.48110  valid_score- 2.48066 in 0m 58s\n",
      "Epoch 197 / 299\n",
      "train_loss- 2.00604  valid_loss- 2.22922  valid_score- 2.23009 in 0m 58s\n",
      "Epoch 198 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.01311  valid_loss- 2.17673  valid_score- 2.17732 in 0m 58s\n",
      "Epoch 199 / 299\n",
      "train_loss- 2.04361  valid_loss- 2.37343  valid_score- 2.37427 in 0m 58s\n",
      "Epoch 200 / 299\n",
      "train_loss- 2.02431  valid_loss- 2.25244  valid_score- 2.25354 in 0m 58s\n",
      "Epoch 201 / 299\n",
      "train_loss- 2.00759  valid_loss- 2.27027  valid_score- 2.27120 in 0m 58s\n",
      "Epoch 202 / 299\n",
      ">> score improved..! \n",
      "train_loss- 1.98283  valid_loss- 2.11206  valid_score- 2.11326 in 0m 58s\n",
      "Epoch 203 / 299\n",
      "train_loss- 2.05472  valid_loss- 2.31760  valid_score- 2.31809 in 0m 58s\n",
      "Epoch 204 / 299\n",
      "train_loss- 2.00992  valid_loss- 2.35095  valid_score- 2.35128 in 0m 58s\n",
      "Epoch 205 / 299\n",
      "train_loss- 1.96785  valid_loss- 2.31763  valid_score- 2.31843 in 0m 58s\n",
      "Epoch 206 / 299\n",
      "train_loss- 2.03031  valid_loss- 2.31485  valid_score- 2.31558 in 0m 58s\n",
      "Epoch 207 / 299\n",
      "train_loss- 2.00594  valid_loss- 2.15599  valid_score- 2.15682 in 0m 58s\n",
      "Epoch 208 / 299\n",
      "train_loss- 1.98423  valid_loss- 2.31958  valid_score- 2.32072 in 0m 58s\n",
      "Epoch 209 / 299\n",
      "train_loss- 2.01787  valid_loss- 2.21099  valid_score- 2.21167 in 0m 58s\n",
      "Epoch 210 / 299\n",
      ">> score improved..! \n",
      "train_loss- 2.01015  valid_loss- 2.09526  valid_score- 2.09591 in 0m 58s\n",
      "Epoch 211 / 299\n",
      "train_loss- 1.97464  valid_loss- 2.12062  valid_score- 2.12138 in 0m 58s\n",
      "Epoch 212 / 299\n",
      "train_loss- 2.02326  valid_loss- 2.33271  valid_score- 2.33278 in 0m 58s\n",
      "Epoch 213 / 299\n",
      "train_loss- 2.01547  valid_loss- 2.15574  valid_score- 2.15636 in 0m 58s\n",
      "Epoch 214 / 299\n",
      ">> score improved..! \n",
      "train_loss- 1.97950  valid_loss- 2.02940  valid_score- 2.03004 in 0m 58s\n",
      "Epoch 215 / 299\n",
      "train_loss- 1.99056  valid_loss- 2.12861  valid_score- 2.12977 in 0m 58s\n",
      "Epoch 216 / 299\n",
      "train_loss- 1.96253  valid_loss- 2.09202  valid_score- 2.09276 in 0m 58s\n",
      "Epoch 217 / 299\n",
      "train_loss- 1.97099  valid_loss- 2.22170  valid_score- 2.22232 in 0m 58s\n",
      "Epoch 218 / 299\n",
      "train_loss- 1.97787  valid_loss- 2.15632  valid_score- 2.15646 in 0m 58s\n",
      "Epoch 219 / 299\n",
      "train_loss- 1.97036  valid_loss- 2.09693  valid_score- 2.09769 in 0m 58s\n",
      "Epoch 220 / 299\n",
      "train_loss- 1.95955  valid_loss- 2.10658  valid_score- 2.10716 in 0m 58s\n",
      "Epoch 221 / 299\n",
      "train_loss- 1.97351  valid_loss- 2.13778  valid_score- 2.13865 in 0m 58s\n",
      "Epoch 222 / 299\n",
      "train_loss- 1.93130  valid_loss- 2.13946  valid_score- 2.14041 in 0m 58s\n",
      "Epoch 223 / 299\n",
      "train_loss- 2.01763  valid_loss- 2.23235  valid_score- 2.23303 in 0m 58s\n",
      "Epoch 224 / 299\n",
      "train_loss- 1.94269  valid_loss- 2.31541  valid_score- 2.31619 in 0m 58s\n",
      "Epoch 225 / 299\n",
      "train_loss- 1.97406  valid_loss- 2.20831  valid_score- 2.20893 in 0m 58s\n",
      "Epoch 226 / 299\n",
      "train_loss- 1.95571  valid_loss- 2.21641  valid_score- 2.21719 in 0m 58s\n",
      "Epoch 227 / 299\n",
      "train_loss- 1.91671  valid_loss- 2.30792  valid_score- 2.30857 in 0m 58s\n",
      "Epoch 228 / 299\n",
      "train_loss- 1.95056  valid_loss- 2.35094  valid_score- 2.35155 in 0m 58s\n",
      "Epoch 229 / 299\n",
      "train_loss- 1.92826  valid_loss- 2.07391  valid_score- 2.07415 in 0m 58s\n",
      "Epoch 230 / 299\n",
      "train_loss- 1.93355  valid_loss- 2.10925  valid_score- 2.10952 in 0m 58s\n",
      "Epoch 231 / 299\n",
      "train_loss- 1.93401  valid_loss- 2.29905  valid_score- 2.30018 in 0m 58s\n",
      "Epoch 232 / 299\n",
      "train_loss- 1.96633  valid_loss- 2.08284  valid_score- 2.08362 in 0m 58s\n",
      "Epoch 233 / 299\n",
      "train_loss- 1.89495  valid_loss- 2.04246  valid_score- 2.04370 in 0m 58s\n",
      "Epoch 234 / 299\n",
      "train_loss- 1.93394  valid_loss- 2.15687  valid_score- 2.15810 in 0m 58s\n",
      "Epoch 235 / 299\n",
      "train_loss- 1.91150  valid_loss- 2.34124  valid_score- 2.34252 in 0m 58s\n",
      "Epoch 236 / 299\n",
      "train_loss- 1.94163  valid_loss- 2.35684  valid_score- 2.35692 in 0m 58s\n",
      "Epoch 237 / 299\n",
      "train_loss- 1.92233  valid_loss- 2.23875  valid_score- 2.23950 in 0m 58s\n",
      "Epoch 238 / 299\n",
      "train_loss- 1.88478  valid_loss- 2.05112  valid_score- 2.05212 in 0m 58s\n",
      "Epoch 239 / 299\n",
      "train_loss- 1.86529  valid_loss- 2.04651  valid_score- 2.04736 in 0m 58s\n",
      "Epoch 240 / 299\n",
      "train_loss- 1.88838  valid_loss- 2.21345  valid_score- 2.21459 in 0m 58s\n",
      "Epoch 241 / 299\n",
      "train_loss- 1.92409  valid_loss- 2.12290  valid_score- 2.12367 in 0m 58s\n",
      "Epoch 242 / 299\n",
      ">> score improved..! \n",
      "train_loss- 1.92599  valid_loss- 1.94509  valid_score- 1.94592 in 0m 58s\n",
      "Epoch 243 / 299\n",
      "train_loss- 1.89006  valid_loss- 2.40759  valid_score- 2.40813 in 0m 58s\n",
      "Epoch 244 / 299\n",
      "train_loss- 1.90998  valid_loss- 2.22114  valid_score- 2.22197 in 0m 58s\n",
      "Epoch 245 / 299\n",
      "train_loss- 1.90839  valid_loss- 2.01993  valid_score- 2.02060 in 0m 58s\n",
      "Epoch 246 / 299\n",
      "train_loss- 1.89380  valid_loss- 2.04006  valid_score- 2.04053 in 0m 58s\n",
      "Epoch 247 / 299\n",
      "train_loss- 1.88373  valid_loss- 2.16986  valid_score- 2.17066 in 0m 58s\n",
      "Epoch 248 / 299\n",
      "train_loss- 1.90322  valid_loss- 2.17804  valid_score- 2.17909 in 0m 58s\n",
      "Epoch 249 / 299\n",
      "train_loss- 1.88160  valid_loss- 1.98402  valid_score- 1.98432 in 0m 58s\n",
      "Epoch 250 / 299\n",
      "train_loss- 1.91812  valid_loss- 2.18369  valid_score- 2.18455 in 0m 58s\n",
      "Epoch 251 / 299\n",
      "train_loss- 1.87433  valid_loss- 2.02745  valid_score- 2.02849 in 0m 58s\n",
      "Epoch 252 / 299\n",
      "train_loss- 1.86755  valid_loss- 2.13272  valid_score- 2.13334 in 0m 58s\n",
      "Epoch 253 / 299\n",
      "train_loss- 1.90651  valid_loss- 2.26861  valid_score- 2.26913 in 0m 58s\n",
      "Epoch 254 / 299\n",
      "train_loss- 1.86473  valid_loss- 2.00996  valid_score- 2.01078 in 0m 58s\n",
      "Epoch 255 / 299\n",
      "train_loss- 1.87385  valid_loss- 2.11705  valid_score- 2.11747 in 0m 58s\n",
      "Epoch 256 / 299\n",
      "train_loss- 1.87621  valid_loss- 2.02120  valid_score- 2.02250 in 0m 58s\n",
      "Epoch 257 / 299\n",
      "train_loss- 1.89729  valid_loss- 2.16906  valid_score- 2.16934 in 0m 58s\n",
      "Epoch 258 / 299\n",
      "train_loss- 1.83243  valid_loss- 2.08951  valid_score- 2.09021 in 0m 58s\n",
      "Epoch 259 / 299\n",
      "train_loss- 1.82744  valid_loss- 2.02509  valid_score- 2.02580 in 0m 58s\n",
      "Epoch 260 / 299\n",
      "train_loss- 1.85706  valid_loss- 2.39144  valid_score- 2.39148 in 0m 58s\n",
      "Epoch 261 / 299\n",
      "train_loss- 1.89252  valid_loss- 2.02839  valid_score- 2.02878 in 0m 58s\n",
      "Epoch 262 / 299\n",
      ">> score improved..! \n",
      "train_loss- 1.82706  valid_loss- 1.89462  valid_score- 1.89524 in 0m 58s\n",
      "Epoch 263 / 299\n",
      "train_loss- 1.88956  valid_loss- 2.06797  valid_score- 2.06852 in 0m 58s\n",
      "Epoch 264 / 299\n",
      ">> score improved..! \n",
      "train_loss- 1.92958  valid_loss- 1.88086  valid_score- 1.88210 in 0m 58s\n",
      "Epoch 265 / 299\n",
      "train_loss- 1.86482  valid_loss- 1.99745  valid_score- 1.99849 in 0m 58s\n",
      "Epoch 266 / 299\n",
      "train_loss- 1.81533  valid_loss- 2.16850  valid_score- 2.16922 in 0m 58s\n",
      "Epoch 267 / 299\n",
      "train_loss- 1.87192  valid_loss- 1.99089  valid_score- 1.99134 in 0m 58s\n",
      "Epoch 268 / 299\n",
      "train_loss- 1.80141  valid_loss- 1.90332  valid_score- 1.90402 in 0m 58s\n",
      "Epoch 269 / 299\n",
      "train_loss- 1.82937  valid_loss- 2.08540  valid_score- 2.08585 in 0m 58s\n",
      "Epoch 270 / 299\n",
      "train_loss- 1.83595  valid_loss- 2.05957  valid_score- 2.06052 in 0m 58s\n",
      "Epoch 271 / 299\n",
      "train_loss- 1.84343  valid_loss- 2.08391  valid_score- 2.08446 in 0m 58s\n",
      "Epoch 272 / 299\n",
      "train_loss- 1.82794  valid_loss- 2.35246  valid_score- 2.35301 in 0m 58s\n",
      "Epoch 273 / 299\n",
      "train_loss- 1.88359  valid_loss- 2.17652  valid_score- 2.17720 in 0m 58s\n",
      "Epoch 274 / 299\n",
      "train_loss- 1.82660  valid_loss- 2.15587  valid_score- 2.15686 in 0m 58s\n",
      "Epoch 275 / 299\n",
      "train_loss- 1.87725  valid_loss- 2.37192  valid_score- 2.37264 in 0m 58s\n",
      "Epoch 276 / 299\n",
      "train_loss- 1.81783  valid_loss- 2.20890  valid_score- 2.20957 in 0m 58s\n",
      "Epoch 277 / 299\n",
      "train_loss- 1.77696  valid_loss- 2.08940  valid_score- 2.09018 in 0m 58s\n",
      "Epoch 278 / 299\n",
      "train_loss- 1.81559  valid_loss- 2.09080  valid_score- 2.09169 in 0m 58s\n",
      "Epoch 279 / 299\n",
      "train_loss- 1.79640  valid_loss- 2.05506  valid_score- 2.05569 in 0m 58s\n",
      "Epoch 280 / 299\n",
      "train_loss- 1.78087  valid_loss- 2.15637  valid_score- 2.15675 in 0m 58s\n",
      "Epoch 281 / 299\n",
      "train_loss- 1.81332  valid_loss- 1.95561  valid_score- 1.95608 in 0m 58s\n",
      "Epoch 282 / 299\n",
      "train_loss- 1.83206  valid_loss- 1.97785  valid_score- 1.97863 in 0m 58s\n",
      "Epoch 283 / 299\n",
      "train_loss- 1.83050  valid_loss- 2.05075  valid_score- 2.05098 in 0m 58s\n",
      "Epoch 284 / 299\n",
      "train_loss- 1.82118  valid_loss- 2.11114  valid_score- 2.11214 in 0m 58s\n",
      "Epoch 285 / 299\n",
      "train_loss- 1.84658  valid_loss- 2.01149  valid_score- 2.01212 in 0m 58s\n",
      "Epoch 286 / 299\n",
      "train_loss- 1.82985  valid_loss- 2.31020  valid_score- 2.31113 in 0m 58s\n",
      "Epoch 287 / 299\n",
      "train_loss- 1.82015  valid_loss- 2.03802  valid_score- 2.03885 in 0m 58s\n",
      "Epoch 288 / 299\n",
      "train_loss- 1.79889  valid_loss- 2.04604  valid_score- 2.04691 in 0m 58s\n",
      "Epoch 289 / 299\n",
      "train_loss- 1.80580  valid_loss- 2.20707  valid_score- 2.20768 in 0m 58s\n",
      "Epoch 290 / 299\n",
      ">> score improved..! \n",
      "train_loss- 1.79708  valid_loss- 1.78831  valid_score- 1.78927 in 0m 58s\n",
      "Epoch 291 / 299\n",
      "train_loss- 1.79823  valid_loss- 2.02732  valid_score- 2.02824 in 0m 58s\n",
      "Epoch 292 / 299\n",
      "train_loss- 1.83175  valid_loss- 1.95636  valid_score- 1.95746 in 0m 58s\n",
      "Epoch 293 / 299\n",
      "train_loss- 1.81787  valid_loss- 2.05833  valid_score- 2.05867 in 0m 58s\n",
      "Epoch 294 / 299\n",
      "train_loss- 1.78324  valid_loss- 2.00650  valid_score- 2.00663 in 0m 58s\n",
      "Epoch 295 / 299\n",
      "train_loss- 1.79594  valid_loss- 2.15703  valid_score- 2.15729 in 0m 58s\n",
      "Epoch 296 / 299\n",
      "train_loss- 1.78467  valid_loss- 1.96931  valid_score- 1.96929 in 0m 58s\n",
      "Epoch 297 / 299\n",
      "train_loss- 1.79463  valid_loss- 1.96358  valid_score- 1.96410 in 0m 58s\n",
      "Epoch 298 / 299\n",
      "train_loss- 1.72995  valid_loss- 2.06949  valid_score- 2.07006 in 0m 58s\n",
      "Epoch 299 / 299\n",
      "train_loss- 1.78715  valid_loss- 1.96743  valid_score- 1.96865 in 0m 58s\n",
      "prediction value check:  [291.7906  159.31355 247.05898 207.34387]\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "n_splits = 5\n",
    "seed_everything(seed)\n",
    "splits = list(KFold(n_splits=n_splits, shuffle=True, random_state=seed).split(train_df))\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(\"===== Fold {} / {} Starts....\".format(fold+1, n_splits))\n",
    "    fold_num = str(fold+1)\n",
    "    trn_index, val_index = splits[fold]\n",
    "    x_tr, x_val = train_df.iloc[trn_index, :], train_df.iloc[val_index, :]\n",
    "    \n",
    "    # build model\n",
    "    \n",
    "    model = build_model(device, model_name='resnet18')\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    train_loader = build_dataloader(x_tr, batch_size, True)\n",
    "    valid_loader = build_dataloader(x_val, batch_size, False)\n",
    "    \n",
    "    best_loss = 99999999\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        since = time()\n",
    "\n",
    "        print('Epoch {} / {}'.format(epoch,  num_epochs - 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            if device:\n",
    "                data = data.to(device)\n",
    "                target = target.float().to(device)\n",
    "            else:\n",
    "                target = target.float()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        val_loss, val_score = validation(model, criterion, valid_loader, device)\n",
    "\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"F{}_resnet18_model.pt\".format(fold_num))\n",
    "            print(\">> score improved..! \")\n",
    "\n",
    "        time_elapsed = time() - since\n",
    "        print('train_loss- {:.5f}  valid_loss- {:.5f}  valid_score- {:.5f} in {:.0f}m {:.0f}s'.format(\n",
    "            train_loss, val_loss, val_score, time_elapsed // 60, time_elapsed % 60))\n",
    "        #print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    # inference #########\n",
    "    model.load_state_dict(torch.load(\"F{}_resnet18_model.pt\".format(fold_num)))\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(test_loader.dataset),4))\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            output = output.detach().cpu().numpy()\n",
    "    print('prediction value check: ', output[0])\n",
    "    np.savetxt('../wafer/resnet18_submission/resnet18_F{}.csv'.format(fold_num), output, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
